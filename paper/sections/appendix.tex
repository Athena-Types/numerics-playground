\section{Appendix: Metric Preservation Proof}
\begin{theorem}[Metric preservation]
  For any $\Delta \ | \ \Gamma \vdash e : \tau$ and substitutions $\sigma, \sigma'$ such that
  $\sigma \sim_{\gamma} \sigma' : \Delta \ | \ \Gamma$, then 
  $e~\sigma \sim_{\gamma \cdot (\Delta \ | \ \Gamma)} e~\sigma' : \tau$.
\end{theorem} \label{thm:metric-preservation}
\begin{proof}
  We induct over our typing derivation. The base cases (Var,
  \bnd{\textbf{bvar}}, Unit, Const) follow trivially. The subsumption and
  widening cases also follow trivially. We detail the remaining cases here. Note
  that the distance between $\bnd{\textbf{bnd}}$ vars in $\Delta$ is $\infty$ if
  they correspond to different bounds. Therefore for the following cases, we
  omit reptitively reasoning about $\Delta$ substitutions:
  \begin{description}
    \item[Case $\multimap$ I.] 
      We wish to show that for any $\Gamma \vdash \lambda x . e : \tau$ and
      subsitutions $\sigma \sim_{\gamma} \sigma' : \Gamma$ that $\lambda x .
      e~\sigma \sim_{\gamma \cdot \Gamma} \lambda x . e~\sigma'$. Unfolding, it
      suffices to show that both:
      \begin{enumerate}
        \item $\lambda x . e~\sigma, \lambda x . e~\sigma'$ are in
          $\mathcal{R}_{\tau_0 \multimap \tau}$
        \item $\mathcal{SD}_{\tau_0 \multimap \tau}(\lambda x . e~\sigma,
          \lambda x . e~\sigma') \leq \gamma \cdot \Gamma$
      \end{enumerate}
    
      Observe that we have by our inductive hypothesis that for any $\Delta, x:
      \tau' \vdash e : \tau$ and substitutions 
      $\delta[v_0/x] \sim_{\gamma'} \delta'[v_1/x] : \Gamma,~x : \tau'$ that 
      $e~\delta[v_0/x] \sim_{\gamma' \cdot \Gamma} e~\delta'[v_1/x]$ holds. 
      Let us now carry on with the proof:

      \begin{description}
        \item[\underline{Property 1.}] We need to show that $(\lambda x . e)~\sigma$ and
          $(\lambda x . e)~\sigma'$ are both in the relation $\mathcal{R}_{\tau_0 \multimap
          \tau}$. The cases are symmetric so we only show one case. Unfolding
          the definition of $\mathcal{R}_{\tau_0 \multimap \tau}$, let $w_0, w_1
          \in \mathcal{VR}_{\tau_0}$. 
          $(\lambda x . e) w_0 \mapsto e[w_0/x]$
          and
          $(\lambda x . e) w_1 \mapsto e[w_1/x]$
          and so it suffices to show that
          $e~\sigma[w_0/x], e~\sigma[w_1/x]$ 
          are closed expressions falling in $R_{\tau_0}$.
          This is true by our inductive hypothesis, instantiating with $\sigma =
          \delta = \delta'$, $v_0 = w_0$, and $v_1 = w_1$.
        \item[\underline{Property 2.}] Unfolding the definition of
          $\mathcal{SD}_{\tau_0 \multimap \tau}$, it suffices to show that:
          $$
          \mathcal{SD}_{\tau}
          ((\lambda x . e~w)~\sigma, (\lambda x . e~w)~\sigma') 
          \leq \gamma \cdot \Gamma
          $$
          Stepping, it suffices to show that
          $$
          \mathcal{SD}_{\tau}
          (e~\sigma[w/x], e~\sigma'[w/x]) 
          \leq \gamma \cdot \Gamma
          $$
          which holds by application of our inductive hypothesis when $\delta =
          \sigma$, $\delta' = \sigma'$, $v_0 = v_1 = w$, and $\gamma' = \gamma
          :: 0$.
      \end{description}
      So, by our inductive hypothesis and unfolding the definitions of
      $\mathcal{R}$ and $\mathcal{SD}$, we have properties (1) and (2)
      respectively. 
    \item[Case $\multimap$ E.] 
      By our inductive hypothesis, it suffices
      to prove this case for $e~f$.
      % By inversion, we know that $v = \lambda x . e$.
      We wish to show that for any $\Gamma + \Theta \vdash e~f : \tau$ and
      subsitutions 
      $\sigma \sim_{\alpha} \sigma': \Gamma + \Theta$ 
      that 
      $$e~f~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} e~f~\sigma'$$
      Using Lemma \ref{thm:sub-decomp} (substitution decomposition), we
      construct potentially overlapping substitutions
      $\sigma_0 \sim_{\alpha} \sigma_1 : \Gamma$
      and $\sigma_0' \sim_{\alpha} \sigma_1' : \Theta$
      where $FV(e) \subseteq DOM(\sigma_0) = DOM(\sigma_1)$ and
      $FV(f) \subseteq DOM(\sigma_0') = DOM(\sigma_1')$ and 
      $\sigma_0, \sigma_0' \subseteq \sigma$ and 
      $\sigma_1, \sigma_1' \subseteq \sigma'$ and $\gamma, \theta$ minimal.
      In other words, $\sigma_0, \sigma_1$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Gamma$.
      Similarly, $\sigma_0', \sigma_1'$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Theta$.
      Note that our substitutions \textit{must} overlap in the case where $FV(e)
      \cap FV(f)$ is non-empty.
      By our inductive hypothesis, we know that 
      $e~\sigma \mapsto^* \lambda x . M$ and 
      $e'~\sigma \mapsto^* \lambda x . M'$ for some $M, M'$.
      Since subsituting variables that are not free does not change the
      underlying term, we know that the following equations must hold where
      $\sigma^*, \sigma'^*$ are $\sigma$ and $\sigma'$ respectively with $x$
      removed:
      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . M)~[\sigma_0]~(w[\sigma_1]) &= 
            ((\lambda x . M[\sigma^*])~w)[\sigma] = 
            ((\lambda x . M)~w)[\sigma] \mapsto
            M[\sigma][w/x] \\
          (\lambda x . M')~[\sigma_0']~(w[\sigma_1']) &= 
            ((\lambda x . M'[\sigma'^*])~w)[\sigma'] = 
            ((\lambda x . M')~w)[\sigma'] \mapsto
            M'[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      and by stepping

      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . M)~[\sigma_0]~(w[\sigma_1]) &\mapsto M[\sigma_0][w[\sigma_1]/x] \\
          (\lambda x . M)~[\sigma_0']~(w[\sigma_1']) &\mapsto M'[\sigma_0'][w[\sigma_1']/x] \\
        \end{aligned}
      \end{equation}

      so by deterministic stepping
      \begin{equation}
        \begin{aligned}[c]
          M[\sigma_0][w[\sigma_1]/x] &=
            M[\sigma][w/x] \\
          M'[\sigma_0'][w[\sigma_1']/x] &=
            M'[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      By applying our inductive hypothesis and unfolding our definition of
      $R_{\tau_0 \multimap \tau}$, we know that $M, M'$ is 
      1-sensitive with respect to $w$ according to $\mathcal{SD}$. 
      This gives us:
      $$
      \mathcal{SD}_{\tau}(M~[w[\sigma]/x][\sigma], M~[w[\sigma']/x][\sigma]) 
      \leq 
      \mathcal{SD}_{\tau_0}(w[\sigma], w[\sigma']) 
      \leq 
      \theta \cdot \Theta
      $$
      and we also know that by our inductive hypothesis
      $$
      \mathcal{SD}_{\tau}(e~[\sigma]~f~[\sigma'], e~[\sigma']~f~[\sigma']) 
      =
      \mathcal{SD}_{\tau}(M~[w[\sigma']/x][\sigma], M'~[w[\sigma']/x][\sigma']) 
      \leq 
      \gamma \cdot \Gamma
      $$
      So by our triangle inequality of the syntactic distance between terms:
      $$
      M~[w[\sigma]/x]~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta}
      M'~[w[\sigma']/x]~\sigma'
      $$

      and therefore
      $$
      (\lambda x . M)~w~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta} (\lambda x . M')~w~\sigma'
      $$

      To complete the proof case, it suffices to prove 
      $$\alpha \cdot (\Gamma + \Theta) \geq \alpha \cdot \Gamma + \alpha \cdot \Theta$$

      which holds by construction (via our substitution decomposition lemma).
    \item[Case $M_q~e$ (let-bind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Like the $\multimap I$ case, we only need to prove one side due to
          symmetry. So we fix a substitution $\sigma$. By Lemma
          \ref{thm:ctx-stepping}, it suffices to prove this case for a value
          $(v_0, v_1)$. 
          We have by our inductive hypothesis that $(v_0, v_1) \in \mathcal{R}_{M_r
          \tau_0}$. So $\mathcal{SD}(v_0, v_1) \leq r$. By application of our
          inductive hypothesis, we know that (abusing notation) 
          $f[v_1/x], f[v_2/x] \in \mathcal{R}_{M_q}$ so
          $f[v_1/x] \mapsto^{*} (v_3, v_4), f[v_2/x] \mapsto^{*} (v_5, v_6)$ where 
          $(v_3, v_4), (v_5, v_6) \in \mathcal{R}_{M_q}$.
          We also know that 
          $$\sigma[v_0/x] \sim_{\vec{0}, r} \sigma[v_1/x] : \Gamma, x : \tau_0$$
          that
          $$f~\sigma[v_0/x] \sim_{(\vec{0},r) \cdot (\Gamma, x \mapsto s)} f~\sigma[v_1/x] : M_q \tau$$
          So therefore
          \begin{equation}
            \begin{aligned}[c]
              (v_3, v_4)~\sigma[v_1/x] 
                &\sim_{(\vec{0},r) \cdot (\Gamma, x \mapsto s)} 
              (v_5, v_6)~\sigma[v_2/x] : M_q \tau \\
              (v_3, v_4)~\sigma[v_1/x] 
                &\sim_{r \cdot s} 
              (v_5, v_6)~\sigma[v_2/x] : M_q \tau
            \end{aligned}
          \end{equation}
          So clearly $(v_3, v_6) \in \mathcal{R}_{M_{s \cdot r + q}}$ by
          application of triangle inequality: 
          $$
          r \cdot s + q \geq
          \mathcal{SD}_{tau}(v_3, v_5) + \mathcal{SD}_{\tau}(v_5, v_6) \geq
          \mathcal{SD}_{\tau}(v_3, v_6)
          $$
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : s \cdot \Gamma + \Theta$:
          \begin{equation}
            \label{eq:lb-prop2}
          \textbf{let-bind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (s
          \cdot \Gamma + \Theta)} \textbf{let-bind}~x = e \ \tin \ f \sigma' :
          M_{s \cdot r + q}
          \end{equation}
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : s \cdot \Gamma$
          and $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$ .

          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} : M_r \tau_0
          $$
          By our proof of \textbf{\underline{Property 1}}, we know that both
          sides of Equation~\ref{eq:lb-prop2} are in the relation
          $\mathcal{R}_{M_{r \cdot s + q} \tau}$ and we're not stuck. We want to
          show both sides have syntatic distance less than $\alpha \cdot (s
          \cdot \Gamma + \Theta)$. 
          We apply our inductive hypothesis once more and extend the
          $\theta$ substutitons to obtain:
          \begin{equation} \label{eq:lb-prop2.1}
            f~\sigma_{\theta}[e~\sigma_{\gamma}/x] 
            \sim_{\alpha \cdot \Theta + (\alpha \cdot s \cdot \Gamma)} 
            f~\sigma'_{\theta}[e~\sigma'_{\gamma}/x] : M_{r} \tau
          \end{equation}
          Since both sides are neighborhood monads, it suffices to bound
          distances in only the first components of each side which means that
          Equation~\ref{eq:lb-prop2.1} implies Equation~\ref{eq:lb-prop2}.
      \end{description}
    \item[Case (let-cobind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We again only need to prove one side due to symmetry.
          By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for a
          value $[v]~\sigma$ for a $\sigma$ compatible withi $\Gamma$. 
          We have by our inductive hypothesis that $[v]~\sigma \in \mathcal{R}_{!_s
          \tau_0}$. Unfolding, we have that $v~\sigma \in \mathcal{R}_{\tau_0}$.
          Stepping and applying our inductive hypothesis yields that
          $f~\sigma~[v/x] \in \mathcal{R}_{\tau}$.

        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : t \cdot \Gamma + \Theta$:
          $$\textbf{let-cobind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (t
          \cdot \Gamma + \Theta)} \textbf{let-cobind}~x = e \ \tin \ f \sigma' :
          \tau$$
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : \Gamma$
          and $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$.
          By application of the inductive hypothesis, we know that:
          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0
          $$
          where 
          $$
          e~\sigma_{\gamma} = [v_0]
          $$
          and
          $$
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0 = [v_1]
          $$
          for some $v_0, v_1$ so therefore by inspection of the relation:
          $$
          s \cdot \mathcal{SD}_{\tau_0}(v_0, v_1) =  
          \mathcal{SD}_{!_s~\tau_0}([v_0], [v_1])
          $$
          Let $\mathcal{SD}_{\tau_0}(v_0, v_1) = c$. Importantly, note that 
          \begin{equation} \label{eq:lcb-zero-sub}
            s \cdot c = \alpha \cdot \Gamma
          \end{equation}
          Then, we apply our inductive hypothesis once more and we extend the
          $\theta$ substutitons and to obtain:
          $$
          f~\sigma_{\theta}[\sigma_{\gamma}/x] 
          \sim_{\alpha \cdot \Theta + t \cdot s \cdot c}
          f~\sigma'_{\theta}[\sigma'_{\gamma}/x] : \tau
          $$
          Note that the distance $\sim$ is obtained via the inductive hypothesis
          and the fact that it is in $R_{M_{s \cdot r + q}}$ is obtained from
          our proof of \textbf{\underline{Property 1}}.
          Substituting by Equation~\ref{eq:lcb-zero-sub} we complete the case:
          $$
          f~\sigma_{\theta}[\sigma_{\gamma}/x] 
          \sim_{\alpha \cdot \Theta + t \cdot \alpha \cdot \Gamma}
          f~\sigma'_{\theta}[\sigma'_{\gamma}/x] : \tau
          $$
      \end{description}

    \item[Case (let).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. By symmetry, it suffices to
          only prove one side. Follows by application of our inductive
          hypothesis and some stepping.
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha} \sigma' :
          s \cdot \Gamma + \Theta$:
          $$
          \textbf{let} \ x = e \ \tin \ f \sigma 
            \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
          \textbf{let} \ x = e \ \tin \ f \sigma' :
          \tau
          $$
          By Lemma~\ref{thm:sub-decomp} we have substitutions $\sigma_{\gamma}
          \sim_\alpha \sigma'_{\gamma} : \Gamma$ and $\sigma_{\theta}
          \sim_\alpha \sigma'_{\gamma} : \Theta$. By applying our inductive
          hypothesis, we know that 
          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \tau_0
          $$
          So we can extend our pair of $\Theta$ subsitutions by these two terms
          respsectively and apply our inductive hypothesis yielding:
          $$
          \textbf{let} \ x = e \ \tin \ f \sigma 
            \sim_{(\alpha \cdot s \cdot \Gamma) + (\alpha \cdot \Theta)}
          \textbf{let} \ x = e \ \tin \ f \sigma' :
          \tau
          $$
          which is equivalent to what we wanted to show.

      \end{description}

    \item[Case factor.] 
      By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for 
      $\mathbf{factor}~((v_1, v_2), (v_3, v_4))$,  

      $\mathbf{factor}~((v'_1, v'_2), (v'_3, v'_4))$. 
      Unfolding our inductive hypothesis, we have that for substitutions
      $\sigma \sim_{\gamma} \sigma' : \Gamma$, that 
      $$
      \textbf{factor} \ ((v_1, v_2), (v_3, v_4))~\sigma 
      \sim_{\gamma \cdot \Gamma} 
      \textbf{factor} \ ((v'_1, v'_2), (v'_3, v'_4))~\sigma' 
      : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      Unfolding, we get:
      $$
      \textbf{factor} \ ((v_1~\sigma, v_2~\sigma), (v_3~\sigma, v_4~\sigma)) 
      \sim_{\gamma \cdot \Gamma} 
      \textbf{factor} \ ((v'_1~\sigma', v'_2~\sigma'), (v'_3~\sigma', v'_4~\sigma')) 
      : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      So by stepping (and reassociating) we can see that the following logical
      relation holds as distances between neighborhood monads are measured in
      terms of the first components:
      $$
      ((v_1~\sigma, v_3~\sigma), (v_2~\sigma, v_4~\sigma)) 
      \sim_{\gamma \cdot \Gamma} 
      ((v'_1~\sigma', v'_3~\sigma'), (v'_2~\sigma', v'_4~\sigma')) 
      : M_{max(r,q)}(\tau_0 \times \tau_1)
      $$

    \item[Case rnd.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. The case holds under the
          assumption the constant langauge parameter $u$ has the required
          properties (see Definition~\ref{def:numfuzz-interface}, property a)
          and application of our inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case ret.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. Holds trivially by application
          of the inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case op.] 
      Both properties hold by our language interface
      (Definition~\ref{def:numfuzz-interface}, property b) and application of
      the inductive hypothesis.

    \item[Case ! I.] Holds unfolding of our logical relations, and our inductive
      hypothesis.

    \item[Case $\times$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Holds by application of the inductive hypothesis and unfolding of the
          definition of $\mathcal{R}_{\tau_0 \times \tau_1}$.
        \item[\underline{Property 2.}]
          We have 
          $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$ 
          and wish to show that
          $$(e, f)~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (e, f)~\sigma'$$

          By Lemma~\ref{thm:sub-decomp}, we know that there exists substitutions
          $\sigma_{\Gamma} \sim_{\alpha} \sigma'_{\Gamma} : \Gamma$
          and
          $\sigma_{\Theta} \sim_{\alpha} \sigma'_{\Theta} : \Theta$.
          So by definition unfolding we have that:
          $$
          (v~\sigma_{\Gamma}, w~\sigma_{\Theta}) 
            \sim_{(\alpha \cdot \Gamma + \alpha \cdot \Theta)} 
          (v~\sigma'_{\Gamma}, w~\sigma'_{\Theta})
          : \tau_0 \times \tau_1
          $$
          so therefore by an analysis of free variables and subsitution
          $$
          (v, w)~\sigma 
            \sim_{(\alpha \cdot \Gamma + \alpha \cdot \Theta)} 
          (v, w)~\sigma'
          : \tau_0 \times \tau_1
          $$

          To complete the proof case, it suffices to prove:
          $$\alpha \cdot (\Gamma + \Theta) \geq \alpha \cdot \Gamma + \alpha \cdot \Theta$$

          which holds by construction (via our substitution decomposition lemma).
      \end{description}

    \item[Case $\times$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. The case holds by application
          of the inductive hypothesis and stepping once.
        \item[\underline{Property 2.}]
          It suffices to show that for all $\sigma \sim_{\gamma} \sigma' : \Gamma$
          if $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma' : \tau_1 \times \tau_2$
          then:
          $$
          \pi_i~e~\sigma \sim_{\gamma \cdot \Gamma} \pi_i~e~\sigma' : \tau_i
          $$
          which holds by stepping and some unfolding of the logical relation.
      \end{description}

    \item[Case $\otimes$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          The case holds by application of the inductive hypothesis.
        \item[\underline{Property 2.}]
          It suffices to show that for all 
          $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$:
          $$
          (e, f)~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (e, f)~\sigma' :
          \tau_0 \otimes \tau_1
          $$
          By Lemma~\ref{thm:sub-decomp}, we know that 
          $\sigma \sim_{\alpha} \sigma' : \Gamma$ and
          $\sigma \sim_{\alpha} \sigma' : \Theta$.
          Appying our inductive hypothesis, we get that
          $e~\sigma \sim_{\alpha \cdot \Gamma} e~\sigma' : \tau_0$ and
          $e~\sigma \sim_{\alpha \cdot \Theta} e~\sigma' : \tau_1$
          so by unfolding our logical relation we obtain
          $$
          (e, f)~\sigma \sim_{\alpha \cdot \Gamma + \alpha \cdot \Theta} (e, f)~\sigma' :
          \tau_0 \otimes \tau_1
          $$
          which is equivalent to what we wanted to show.
      \end{description}

    \item[Case $\otimes$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          We again only need to prove one side due to symmetry. Let our bound
          expression be some value $w$. By our inductive hypothesis $w = (v_0,
          v_1)$ for some $v_0$ and $v_1$. Stepping and applying our inductive
          hypothesis yields that $f~\sigma~[v_0/x][v_1/x] \in
          \mathcal{R}(\tau)$ for any $\sigma$ compatible with $\Theta$.
        \item[\underline{Property 2.}]
          This case mirrors the proof for \textbf{\underline{Property 2}} of the
          $\textbf{let}$ case.
      \end{description}

    \item[Case $+$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          Holds by application of the inductive hypothesis.
        \item[\underline{Property 2.}]
          It suffices to show that for all $\sigma \sim_{\gamma} \sigma' : \Gamma$
          if $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma' : \tau_0 + \tau_1$
          then:
          $$
          \tin_i~e~\sigma \sim_{\gamma \cdot \Gamma} \tin_i~e~\sigma' : \tau_0 +
          \tau_1
          $$
          which holds by stepping and some unfolding of the logical relation.
      \end{description}

    \item[Case $+$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. We again only need to prove one
          side due to symmetry. Let our bound expression be some value
          $\textbf{in}_i ~ v$. Stepping and applying our inductive hypothesis
          finishes this case.
        \item[\underline{Property 2.}]
          We wish to show that for all 
          $\sigma \sim_{\alpha} \sigma': s \cdot \Gamma + \Theta$ 
          that:
          $$
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2)~\sigma
          \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2)~\sigma'
          : \tau
          $$
          By Lemma~\ref{thm:sub-decomp} we have substitutions
          $$
          \sigma_{\Gamma} \sim_{\alpha} \sigma'_{\Gamma} : \Gamma
          $$
          $$
          \sigma_{\Theta} \sim_{\alpha} \sigma'_{\Theta} : \Theta
          $$
          that we can plug into our inductive hypothesis. Plugging in, we
          know that: $e~\sigma \mapsto \mathbf{in}_i~v$ 
          and $e~\sigma' \mapsto \mathbf{in}_j~v'$ where
          $\mathcal{SD}(\mathbf{in}_i~v, \mathbf{in}_j~v') 
          \leq \gamma \cdot \Gamma$.
          We now case over the following two scenarios:
          \begin{description}
            \item[Subcase $i = j$.] This subcase mirrors the proof for
              \textbf{\underline{Property 2}} of the $\textbf{let}$ case.
            \item[Subcase $i \not= j$.] In this case, we have:
              $\mathcal{SD}(\mathbf{in}_i~v, \mathbf{in}_j~v') = \infty \leq
              \alpha \cdot \Gamma$. Stepping and applying the inductive
              hypothesis, we obtain:
              $$
              f_i [v/x]\sigma 
              \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
              f_j [v'/x]\sigma'
              : \tau
              $$
              Since $s$ is non-zero, by applying our inductive hypothesis we
              know that $f_i$ and $f_j$ are non-zero sensitive in $x$ and since
              positive $s \cdot \infty = \infty$, we know that $\alpha \cdot (s
              \cdot \Gamma + \Theta) = \infty$. Therefore this subcase holds.
              Note that it is essential in this case that $s$ is non-zero; If
              $s$ is allowed to be zero, this would be a problem because $0
              \cdot \infty = 0$.
          \end{description}

    \end{description}

  \end{description}
  The remaining cases in this proof deal with bound polymorphism:
  \begin{description}
    \item[Case \textbf{bop}.] Holds by the spec on our language interface and
      from the fact that any differring $\Delta$-substitutions must be $\infty$
      apart.

    \item[Case $\forall$ I.] We need to show that for any substitutions:

      $$\sigma \sim_{\alpha} \sigma' : \Delta \ | \ \Gamma$$

      that 

      $\Lambda \epsilon . e~\sigma \sim_{\alpha \cdot (\Delta \ | \ \Gamma)}
      \Lambda \epsilon . e~\sigma : \forall \epsilon . \tau$. Similarly to the
      above cases, we observe that if the $\Delta$-substitutions in $\sigma$ and
      $\sigma'$ are different, the distance is $\infty$ and we are done. In the
      case that the $\Delta$-substitutions are the same, we unfold our
      definition of $\mathcal{R}_{\forall \epsilon . \tau}$ and observe that
      since $\epsilon \not\in FTV(\Gamma)$, we can apply our inductive
      hypothesis.

    \item[Case $\forall$ E.] We similarly observe that if the
      $\Delta$-substitutions in $\sigma$ and $\sigma'$ are different, the
      distance is $\infty$ and we are done. Unfolding our logical relation and
      applying our inductive hypothesis finishes the case.
  \end{description}
\end{proof}
