\section{Implementation and evaluation} \label{sec:eval}
We wish to have a practical technique for bounding floating-point round-off
error. To evaluate our approach, we implement the type inference algorithm
detailed in this paper in Rust.
We evaluate our implementation on speed and precision of the analysis against
competiting tools on benchmarks in the FPBench benchmark suite \cite{fpbench}
and select programs from the Satire benchmark suite \cite{satire}.
Since we are extending Numerical Fuzz and are not interested in duplicating
Numerical Fuzz's evaluation, we only use benchmark programs that are newly
supported, e.g. contain subtraction or range over negative numbers.
For our comparison, we use the \textsc{binary64} precision mode and set the
rounding mode to round towards zero.
Note that the add-assoc benchmark is not from FPBench and is instead taken from
our running example of pairwise summation on $w,x,y,z \in [-1, 1]$.
We run all of our benchmarks on an AWS c5.metal instance (96 vCPU, 192 GiB RAM).
Each timing benchmark result reported is the median of 6 benchmark runs.

% TODO: mention relative error

% TODO: orphaned
% One might wonder if using a paired representation, then running a bounds
% analysis to recover the error on the original unpaired representation hurts the
% precision of our analysis on programs. Interestingly, we do not lose any
% precision. We provide several examples showing that our bounds analysis is
% capable of reasoning that programs containing only-growing functions on
% non-negative numbers can only produce non-negative numbers. Therefore, on the
% programs supported by both Negative Fuzz and Numerical Fuzz, the maximum
% round-off error obtained by Negative Fuzz is simply the inferred grade of the
% monad and is no looser than the bound obtained by Numerical Fuzz. Negative Fuzz
% is therefore a strict improvement on Numerical Fuzz: our approach can analyze
% more programs with possibly tighter bounds on previously supported programs.

\paragraph{Supported operations.}
Compared to Numerical Fuzz, our instantiation supports subtraction,
multiplication, and addition. We additionally support negative numbers. However,
we do not support division or taking square roots; we leave this as a potential
future direction in future work. We assume that there is no under or overflow in
the program. Further, our error metric (relative precision) is only well-defined
when the floating-point computation and the exact computation are always
non-zero and the same sign.

\subsection{Data tables and plots}

\paragraph{Small benchmark programs.}
We compare the precision and performance of our approach on small benchmark
programs against two alternative approaches: FPTaylor \cite{fptaylor} and Gappa
\cite{gappa}. In all of the examples in Table~\ref{tab:small-precision-abs}, we
can see that NegFuzz's bounds are tighter when \textbf{factor} is used. We also
observe that NegFuzz (with factor) obtains competitive bounds within an order of
magnitude with Gappa and FPTaylor.
We also compare the performance of our NegFuzz implementation on small
benchmarks in Figure~\ref{tab:small-timing}. Negative Fuzz is frequently orders
of mangitude faster than Gappa and FPTaylor.

\input{../plots/small-benchmarks-tables.tex}

\paragraph{Large benchmark programs.}
To understand how Negative Fuzz scales, we run three paramterized benchmarks and
double the program size until a 450 second timeout is reached or over 64 GB of
RAM is used. We plot the results in Figure~\ref{fig:large-plots}. Failures due
to timeout or OOM are marked as a point on the uppermost edge of each plot. Both
the Horner and iterative summation schemes have a fixed evaluation order that is
not conducive to rewriting with \textbf{factor}, so we do not bother writing
benchmark variants with \textbf{factor}. 

On the Horner and serial summation benchmarks, Negative Fuzz has comparable
precision at faster performance than competing tools. On the matrix
multiplication benchmark, Negative Fuzz with \textbf{factor} outperforms Satire
on both precision and timing performance. 

\includegraphics[width=1\textwidth]{../plots/figures/large_benchmarks_combined.pdf}
\begin{figure}
\caption{
  Log-scale growth plots of absolute error and execution time of Negative Fuzz
  (with and without \textbf{factor}), Gappa, FPTaylor, and Satire on large
  benchmarks.
}
\label{fig:large-plots}
\end{figure}

\subsection{Evaluation takeaways}
From our evaluation tables and figures, we make four claims:

\paragraph{Our type-based approach is faster than competing approaches.} 
On both the small benchmarks taken from FPBench that our tool supports as well
as large benchmarks taken from Satire's artifact evaluation, our approach is
signficantly faster than competing approaches, often outperforming by an order
of magnitude. 

\paragraph{Our type-based approach yields useful a priori bounds that are
competitive with other competing approaches.} 
Our work is the first type-based approach that is able to reason about round-off
error in the presence of subtraction and negative numbers. Both the absolute
error bounds that we achieve are often comparable to state-of-the-art tools such
as Gappa and FPTaylor. 

\paragraph{A posteriori error bounds can be useful in obtaining tighter
round-off error analyses} 
We observe our approach on concrete program executions often produces
competitive error bounds. In some circumstances, our type-based approach can
produce a posteriori error bounds that are tighter than Gappa and FPTaylor. We
also observe that our a posteriori error bounds are cheap and practical to
compute given a concrete program output.

\paragraph{The \textbf{factor} primitive results in tighter error bounds and
minimal type-checking overhead.} 
We observe that using the \textbf{factor} in our programs always results in
tighter error bounds for the same underlying computation. Often times our bounds
are signifiantly smaller and occasionally they are an order of magnitude
smaller. We also observe that \textbf{factor} occasionally results in faster
type-checking, on some programs and rarely incurs a 50\% type-checking
performance overhead. 
