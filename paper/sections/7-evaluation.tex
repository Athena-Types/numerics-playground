\section{Implementation and evaluation} \label{sec:eval}
To evaluate our approach, we implement the type inference algorithm detailed in
this paper in Rust.
We evaluate the speed and precision of our implementation against competing
tools on the FPBench \cite{fpbench} and Satire \cite{satire} benchmark suite.
We only include FPBench programs that are newly supported, e.g. contain
subtraction or range over negative numbers.
Many of the programs in the FPBench benchmark suite are too small to draw
conclusions about the scalability of our approach. Therefore, we draw upon
large programs from the Satire benchmark suite \cite{satire}, which are taken
from real-world programs. Due to the sheer size of the programs in the Satire
suite, we only translated three program schemas into Negative Fuzz (with and
without \textbf{factor}), Gappa, and FPTaylor in our evaluation.

For each benchmarks, we set the precision to \textsc{binary64} and the rounding
mode to round towards infinity.
We convert all bounds to absolute error for an apples-to-apples comparison
against competing tools.
We run our benchmarks on an AWS c5.metal instance (96 vCPU, 192 GiB RAM).
Each timing benchmark result reported is the median of 6 benchmark runs.
Our approach has several assumptions and limitations. Like Numerical Fuzz, we
assume that there is no under or overflow in the program. We do not support
recursion, division, or square roots.

% TODO: mention relative error

% TODO: orphaned
% One might wonder if using a paired representation, then running a bounds
% analysis to recover the error on the original unpaired representation hurts the
% precision of our analysis on programs. Interestingly, we do not lose any
% precision. We provide several examples showing that our bounds analysis is
% capable of reasoning that programs containing only-growing functions on
% non-negative numbers can only produce non-negative numbers. Therefore, on the
% programs supported by both Negative Fuzz and Numerical Fuzz, the maximum
% round-off error obtained by Negative Fuzz is simply the inferred grade of the
% monad and is no looser than the bound obtained by Numerical Fuzz. Negative Fuzz
% is therefore a strict improvement on Numerical Fuzz: our approach can analyze
% more programs with possibly tighter bounds on previously supported programs.

\paragraph{Small benchmark programs.}
We compare the precision and performance of our approach on small benchmark
programs from FPBench against two alternative approaches: FPTaylor \cite{fptaylor} and Gappa
\cite{gappa}. 
\footnote{Note that the add-assoc benchmark is not from FPBench and is instead taken from
our running example of pairwise summation on $w,x,y,z \in [-1, 1]$.} 
We observe that Negative Fuzz (with \textbf{factor}) obtains competitive bounds
within an order of magnitude with Gappa and FPTaylor.
We also observe that our NegFuzz implementation is often orders of magnitude
faster than Gappa and FPTaylor (Table~\ref{tab:small-timing}).
FPTaylor and Gappa occupy a different part of the performance-precision design
space compared to Negative Fuzz: both FPTaylor and Gappa aim for precise, sound
floating-point analyses via global optimization or rewriting rather than speed
or scalability.

To understand how \textbf{factor} contributes the precision of our approach, we
perform a simple ablation by manually translating each benchmark from FPBench
into Negative Fuzz with and without the \textbf{factor} primitive.
In all of the examples in Table~\ref{tab:small-precision-abs}, we
can see that using the \textbf{factor} strictly improves the precision of our
analysis. We did not specifically optimize our implementation of \textbf{factor}
so it is difficult to draw any conclusions about \textbf{factor}'s performance
overhead.

\input{../plots/small-benchmarks-tables.tex}

\paragraph{Large benchmark programs.}
To understand how Negative Fuzz scales, we run three large parameterized
benchmarks, which we discuss below. Our large-scale comparisons are primarily
intended to compare against Satire, which is intended to be scalable by taking a
global optimization approach that allows users to trade-off precision for
analysis time. We include Gappa and FPTaylor to demonstrate that those two
tools do not scale well and often timeout.
We run each benchmark until a 450 second timeout is reached or over 64 GB of RAM
is used.
Both the Horner and iterative summation schemes have a fixed evaluation order
that makes rewriting with \textbf{factor} unhelpful, so we do not bother writing
benchmark variants with \textbf{factor}. 
We plot the results in Figure~\ref{fig:large-plots}. Each column represents a
set of paramterized benchmarks. The top row represents the absolute error bound
achieved by each tool. The bottom row represents the median execution time of
each program (across 6 runs). If any of the 6 runs failed due to timeout or OOM,
we mark the result as a failure on the uppermost edge of each plot. We discuss
the benchmarks below:
\begin{enumerate}
  \item We evaluate the polynomials of degree $2^2$ to $2^{11}$ using Horner's
    scheme and we observe that Negative Fuzz has comparable precision at
    significantly faster performance.
  \item We evaluate matrix multiplication on inputs from $2^2 \times 2^2 $ to
    $2^7 \times 2^7$. On large inputs, we observe that Negative Fuzz (without
    \textbf{factor}) has comparable precision at significantly faster performance.
    Rewriting the program with \textbf{factor} allows Negative Fuzz to achieve
    significantly better precision on large inputs than competing tools.
  \item We evaluate iterative summation (discussed in
    Section~\ref{sec:overview}) on sequences of addition from $2^2$ to $2^7$.
    Negative Fuzz has comparable precision at significantly faster performance.
\end{enumerate}

\begin{figure}
\includegraphics[width=1\textwidth]{../plots/figures/large_benchmarks_combined.pdf}
\caption{
  Log-scale growth plots of absolute error and execution time of Negative Fuzz
  (with and without \textbf{factor}), Gappa, FPTaylor, and Satire on large
  benchmarks. For all plots, smaller is better.
}
\label{fig:large-plots}
\end{figure}

\paragraph{Impact of factor.}
We observe that using \textbf{factor} in our evaluation only improves
our error analysis. For a program of type $M_q \ \textbf{num}_{\bnd{b}}$, our
analysis depends on two parts of our type: 
\begin{enumerate}
  \item The grade on the monad $q$. In our evaluation benchmarks
    (Section~\ref{sec:eval}), we observe that rewriting a program with
    \textbf{factor} only reduces the inferred monadic grade.
  \item The result of the bound analysis $\bnd{b}$, which is entirely orthogonal
    to the usage of \textbf{factor}. In fact, we observe that for each benchmark
    in our evaluation rewritten to use \textbf{factor}, the inferred bounds
    subscript on $\textbf{num}$ remains unchanged.
\end{enumerate}
So, each program rewritten with \textbf{factor} in our evaluation only
improved our grade $q$ while keeping the bound $b$ the same. Our error theorems
(Theorem~\ref{thm:paired-a-priori-abs} and
Theorem~\ref{thm:paired-a-priori-rel}) ensure that if a program is rewritten to
a lower monadic grade $q$, all else equal, the final error bound will improve.

\paragraph{Evaluation takeaways.}
From all our evaluation tables and figures, we conclude that when compared with
Gappa, FPTaylor, and Satire: (1) our type-based approach is faster, (2) yields
useful and competitive error bounds, and (3) using the \textbf{factor} primitive
often improves error bounds.
