\section{Implementation and evaluation} \label{sec:eval}
To evaluate our approach, we implement the type inference algorithm detailed in
this paper in Rust.
We evaluate the speed and precision of our implementation against competing
tools in the FPBench benchmark suite \cite{fpbench}.
Since we are extending Numerical Fuzz and are not interested in duplicating
Numerical Fuzz's evaluation, we only include FPBench programs that are newly
supported, e.g. contain subtraction or range over negative numbers.
Many of the programs in the FPBench benchmark suite are too small to draw
conclusions about the scability of our approach. So, we include select programs
from the Satire benchmark suite \cite{satire} that were easy to translate into
Negative Fuzz, Gappa, and FPTaylor. We found many of the programs from the
Satire benchmark suite to be difficult to translate due to the program size.
For all of our benchmarks, we use the \textsc{binary64} precision mode and set
the rounding mode to round towards infinity.
On some benchmarks, our tool is able to produce a relative precision bound.
However, as we are the only tool that produces a relative precision bound, we
convert all bounds to absolute error for an apples-to-apples comparsion against
competing tools.
We run all of our benchmarks on an AWS c5.metal instance (96 vCPU, 192 GiB RAM).
Each timing benchmark result reported is the median of 6 benchmark runs.

Our approach has several assumptions and limitations. We assume that there is no
under or overflow in the program. Further, the error metric (relative precision)
used in our type system is only well-defined when the floating-point computation
and the exact computation are always non-zero and the same sign. Therefore, even
though we report absolute error for an apples-to-apples comparison against
competing tools, our analysis additionally assumes that the floating-point
computation and exact computation are non-zero and the same sign. We do not
support division or taking square roots; we leave this as a potential future
direction.

% TODO: mention relative error

% TODO: orphaned
% One might wonder if using a paired representation, then running a bounds
% analysis to recover the error on the original unpaired representation hurts the
% precision of our analysis on programs. Interestingly, we do not lose any
% precision. We provide several examples showing that our bounds analysis is
% capable of reasoning that programs containing only-growing functions on
% non-negative numbers can only produce non-negative numbers. Therefore, on the
% programs supported by both Negative Fuzz and Numerical Fuzz, the maximum
% round-off error obtained by Negative Fuzz is simply the inferred grade of the
% monad and is no looser than the bound obtained by Numerical Fuzz. Negative Fuzz
% is therefore a strict improvement on Numerical Fuzz: our approach can analyze
% more programs with possibly tighter bounds on previously supported programs.

\subsection{Data tables and plots}

\paragraph{Small benchmark programs.}
We compare the precision and performance of our approach on small benchmark
programs from FPBench against two alternative approaches: FPTaylor \cite{fptaylor} and Gappa
\cite{gappa}. 
\footnote{Note that the add-assoc benchmark is not from FPBench and is instead taken from
our running example of pairwise summation on $w,x,y,z \in [-1, 1]$.}
FPTaylor and Gappa occupy a different part of the performance-precision design
space comapred to Numerical Fuzz and Negative Fuzz: both FPTaylor and Gappa aim
for precise, sound floating-point analyses via global optimization or rewriting
rather than speed or scalability.

We observe that Negative Fuzz (with \textbf{factor}) obtains competitive bounds
within an order of magnitude with Gappa and FPTaylor.
We also compare the performance of our NegFuzz implementation on small
benchmarks in Table~\ref{tab:small-timing}. 
Like Numerical Fuzz, Negative Fuzz is frequently orders of magnitude faster than
Gappa and FPTaylor.

To understand how \textbf{factor} contributes the precision of our approach, we
perform a simple ablation by manually translating each benchmark from FPBench
into Negative Fuzz with and without the \textbf{factor} primitive.
In all of the examples in Table~\ref{tab:small-precision-abs}, we
can see that using the \textbf{factor} strictly improves the precision of our
analysis. We did not specifically optimize our implementation of \textbf{factor}
so it is difficult to draw any conclusions about \textbf{factor}'s performance
overhead.

\input{../plots/small-benchmarks-tables.tex}

\paragraph{Large benchmark programs.}
To understand how Negative Fuzz scales, we run three parameterized benchmarks and
double the program size until a 450 second timeout is reached or over 64 GB of
RAM is used.
Both the Horner and iterative summation schemes have a fixed evaluation order
that makes rewriting with \textbf{factor} unhelpful, so we do not bother writing
benchmark variants with \textbf{factor}. 
We plot the results in Figure~\ref{fig:large-plots}. Each column represents a
set of paramterized benchmarks. The top row represents the absolute error bound
achieved by each tool. The bottom row repreesents the median execution time of
each program (across 6 runs). If any of the 6 runs failed due to timeout or OOM,
we marked the result as a point on the uppermost edge of each plot. We discuss
the benchmarks results below:
\begin{enumerate}
  \item We evaluate the Horner polynomial evaluation scheme from $2^2 (4)$ to
    $2^{11} (2048)$ and we observe that Negative Fuzz has comparable precision
    at significantly faster performance.
  \item We evaluate matrix multiplication from $2^2 (4)$ to $2^7 (128)$. On
    large inputs, we observe that Negative Fuzz (without \textbf{factor})
    comprable precision at significantly faster performance. Rewriting the
    program with \textbf{factor} allows Negative Fuzz to achieve significantly
    better precision on large inputs than competing tools.
  \item We evaluate the iterative summation algorithm (discussed in
    Section~\ref{sec:overview}) from $2^2 (4)$ to $2^7 (128)$. We observe that
    Negative Fuzz has comparable precision at significantly faster performance.
\end{enumerate}

\begin{figure}
\includegraphics[width=1\textwidth]{../plots/figures/large_benchmarks_combined.pdf}
\caption{
  Log-scale growth plots of absolute error and execution time of Negative Fuzz
  (with and without \textbf{factor}), Gappa, FPTaylor, and Satire on large
  benchmarks. For all plots, smaller is better.
}
\label{fig:large-plots}
\end{figure}

\paragraph{Evaluation takeaways.}
From our evaluation tables and figures, we conclude that when compared with
Gappa, FPTaylor, and Satire: (1) our type-based approach is faster, (2) yields
useful and competitive error bounds, and (3) the \textbf{factor} primitive often
results in improved error bounds.
Our work is also the first type-based approach that is able to reason about
round-off error in the presence of subtraction and negative numbers. 
Since Negative Fuzz uses a type-based approach to numerical analysis, our
approach also offers several qualitative advantages over competing tools. For
example, each function in a library of numerical analysis algorithms could be
typed once. Programs that call a library function would not need to type-check
the underlying library code and could instead rely on the function type as an
interface specfication. As opposed to a global optimization approach to
numerical analysis, type checking could also happen in parallel over the
dependency graph of the program. Type checking could also be performed
incrementally to eliminate rendundant computation.
