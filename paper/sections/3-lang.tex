\section{Language} \label{sec:lang}
Our language is an extension of the Numerical Fuzz
\cite{numfuzz} language, a call-by-value affine lambda calculus.
Building off Numerical Fuzz \cite{numfuzz}, we are interested in developing a
modular family of languages for analyzing numerical round-off error. Modularity
enables differing floating-point bit precisions (e.g. 32-bit floats, 64-bit
floats) and floating-point operations to be soundly instantiated. In
Section~\ref{sec:encoding}, we will instantiate our family of languages with a
paired representation which will help provide both a priori and a posteriori
bounds.

In this section, we detail the construction of our modular family of languages.
We first describe the terms (Section~\ref{sec:terms}), types and static
semantics (Section~\ref{sec:types}), operational semantics
(Section~\ref{sec:dynamic-semantics}), and a modular interface for soundly
instantiating the language. Finally, we define a logical relation and prove
soundness.

% and logical relation for the family of
% languages (Section~\ref{sec:logical-relation}). Finally, using our logical relation
% to precisely specify the required invariants for instantiating our language, we
% define a modular interface for instantiating the language as well as prove
% soundness for any interface-conforming instantiation of the language
% (Section~\ref{sec:interface}). 
% We extend the Numerical Fuzz family of languages to support two key features:
% \begin{enumerate}
%   \item Error and sensitivity sharing between terms, through the addition of a
%   \textbf{factor} primitive (Section~\ref{sec:error-sharing}). This enables more
%   programs to be typed with tighter error bounds.
%   % todo: add section ref
%   \item Interval analysis, through the incorporation of \textit{bound
%   polymorphism} (Section~\ref{sec:bound-poly}). In
%   (Section~\ref{sec:tightness}), we will prove that this will result in error
%   bounds no looser than prior type-based approaches for forwards analysis.
% \end{enumerate}
% Finally, we
% provide a formal specification for soundly instantiating a particular
% language and prove soundness via a logical relations argument.
%
% \subsection{Sharing Error} \label{sec:error-sharing}
%
% \subsection{Type-based Interval Analysis and Bound Polymorphism} \label{sec:bound-poly}

\subsection{Terms} \label{sec:terms}
% \subsection{Syntax} \label{sec:syntax}
% We present the full syntax for types, terms, and evaluation contexts in
% Figure~\ref{fig:syntax}. 

Like many lambda calculi for linear type systems, our language supports
variables, function abstraction and application, $\pi_i$ for projection, $in_i$
for injection, and two different types of products.
Our term syntax has explicit terms to represent our effectful computation
(rounding to the nearest float: $\textbf{rnd}~e$) and coeffects (scaling
sensitivity: $[e]$). We have several terms for sequencing and structuring
computation. We have $\textbf{let-bind}$ for sequencing monadic computation,
$\textbf{let-cobind}$ for sequencing comonadic computation, $\textbf{let-pair}$
for unpacking tensors, and \textbf{case \ldots of \ldots} for unpacking sum
types, and \textbf{let} for sequencing variable assignment.
Note that our term language is more expressive than Numerical Fuzz as we do not
restrict the bound term in our let expressions to be values.

Our term language extends Numerical Fuzz's term language. We introduce a
language for bounds (shown in Figure~\ref{fig:syntax}) as well as explicit
terms to represent polymorphic abstraction $\bnd{(\Lambda \epsilon. e)}$ and
instantiation $\bnd{(e \bnd{\{b\}})}$. This is useful for incorporating an bound
analysis into the type inference algorithm, which we detail in
Section~\ref{sec:inference}.

\begin{figure}[tbp]
  \begin{alignat*}{3}
    &\bnd{\text{Bounds } b, b_0, b_1} &::=~ &\bnd{\epsilon}
    \mid \bnd{c \in \mathbb{B}}
         \mid \bnd{\mathbf{bop_c(b_0, \ldots, b_c)}} \\
         &\text{Types } \tau, \tau_0, \tau_1 &::=~ &\mathbf{unit}
         \mid \num_{\bnd{b}}
         \mid \tau_0 \times \tau_1
         \mid \tau_0 \otimes \tau_1
         \mid \tau_0 + \tau_1
         \mid \tau_0 \multimap \tau_1
         \mid {\bang{s} \tau}
         \mid {M_u \tau}
         \mid \bnd{\textbf{bnd}}
         \mid \forall \epsilon . \tau
         \\
         &\text{Values } v, w \ &::=~ &\langle \rangle
         \mid k \in \textit{num}
         \mid \langle x,y \rangle 
         \mid (x, y)
         \mid \tin_i \ x
         \mid \lambda x.~e
         \mid \bnd{\Lambda \epsilon . e} \\
         % \mid \mathbf{op}(x) \\
         % & & & \mid [v]
         % \mid\rnd v
         % \mid {\ret v} 
         % \mid \factor v
         % \mid \letbind x = v \ \tin \ f 
         \\
         &\text{Terms } e, f, g &::=~ &v
         \mid x
         % \mid (b_0, b_1)
         \mid \mathbf{op} ~ e
         \mid e~f
         \mid \bnd{e~\{i\}}
         \mid {\pi}_i\ e
         \mid \langle e,f \rangle 
         \mid (e, f) \\
         & & & \mid \letpair \ (x,y) = e \ \tin \ f
         \mid \letassign x  = e \ \tin \ f \\
         & & & \mid \tin_i \ e
         \mid 
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2) \\
         & & &
         \mid [e]
         \mid \rnd e
         \mid {\ret e} 
         \mid \factor e \\
         & & & 
         \mid {\letbind x = e \ \tin \ f}
         \mid \letcobind x = e \ \tin \ f
         \\
         &\text{Evaluation contexts } C &::=~ &[.] 
         \mid \mathbf{op}(C) 
         \mid C~e 
         \mid v~C 
         \mid C~\bnd{\{b\}}
         \mid \pi_i~C \\
         & & &
         \mid \mathbf{in}_i~C
         \mid \langle C, e \rangle
         \mid \langle v, C \rangle
         \mid (C, e)
         \mid (v, C)
         \mid [ C ] \\
         & & &
         \mid \mathbf{rnd}~C 
         \mid \mathbf{ret}~C 
         \mid \mathbf{factor}~C \\ 
         & & &
         \mid \letassign x = C \ \tin \ f
         \mid \letpair x = C \ \tin \ f \\
         & & &
         \mid \letbind x = C \ \tin \ f
         \mid \letcobind x = C \ \tin \ f \\
  \end{alignat*}
  \caption{
    Types, values, and terms. $\mathbf{op} \in \mathcal{O}$. $i \in \{1, 2\}$.
    $\textbf{op}$ and $\textbf{bop}$ are syntactic metavars representing
    functions of arbitrary arity.
    $\textit{num}$ is a parameter in the programming language representing the
    set of numbers the language computes over.
  }
  \label{fig:syntax}
\end{figure}

\subsection{Types and static semantics}  \label{sec:static-semantics}
Following Numerical Fuzz, our type system has graded monad types $M_q \tau$ for
bounding the round-off error in $\tau$ by a real, non-negative grade $q$, scaled
metric types. We also have graded comonadic types $!_s \tau$ for bounding the
sensitivity of a type $\tau$ by a real, non-negative grade $s$.
Our language also supports linear function types $\tau_0 \multimap \tau$
corresponding to 1-sensitive functions.

Our typing judgements incorporate a graded effect and co-effect type system to
simulatenously track round-off error and function sensitivity. For example, the
following term with typing context and judgement indicates that the variable $x$
is $s$-sensitive in the expression $e$ and has worst-case round-off error
upper-bounded by $q$.
\begin{equation}
  x :_s \tau_0 \vdash e : M_q \tau_1
\end{equation}


Typing contexts have the following grammar: 
$\Gamma, \Delta ::= . | \Gamma, x :_s \tau$.
Semantically, we can treat contexts $\Gamma$ as a partial map from variables $x$
to pairs of sensitivities and types $(s, \tau)$. Similarly to Numerical Fuzz, we
can sum and scale typing contexts. We represent context scaling as $\Gamma +
\Delta$ and scaling by a real $s$ as $s \cdot \Gamma$. We define pair context
and scaling and then context summing and scaling below.

\begin{definition}[Pair summing]
  We can sum a pair of sensitivities and types $(s, \tau) + (s', \tau) = (s +
  s', \tau)$. Note that this is only well-defined if the types are syntacitcally
  equivalent.
\end{definition}

\begin{definition}[Context summing]
  A context $\Theta = \Gamma + \Gamma'$ if and only if $\forall x, \theta(x) =
  \Gamma(x) + \Gamma'(x)$ and is well defined.
\end{definition}

\begin{definition}[Pair scaling]
  We can scale a pair of sensitivities and types $s' \cdot (s, \tau) (s' \cdot s
  s', \tau)$. 
\end{definition}

\begin{definition}[Context scaling]
  A context $\Gamma' = s \cdot \Gamma$ if and only if $\forall x, \Gamma'(x) = s
  \cdot \Gamma'(x)$
\end{definition}

% describe rules, harp on linearity
The introduction and elimaination rules for $\otimes$, $\times$, and $\multimap$
mirror Numerical Fuzz and correspond to the standard introduction and
elimination rules in linear logic.
We are also able to scale typing contexts through the (! I) rule. This is useful
for instantiating functions with finitely scaled senstivity.
% todo: check that we can't be infinite and that this is up-to-date everywhere

The monadic grade upper-bounds the maximum possible round-off error possible.
Most of the rules for controlling the grade, (Ret), (Rnd), ($M_u$ E),
(Subsumption) are taken from Numerical Fuzz and are our core reasoning
principles for reasoning about round-off error. Intuitively, (Ret) introduces no
round-off error and (Rnd) introduces round-off error $u$ corresponding to the
unit round-off constant. Note that $M_u E$ is the characteristic rule for
\textbf{let-cobind} that incorporates senstivity and round-off information.
Our language also introduces a new (Factor) rule for reasoning about monadic
computations with round-off error, which allows for tighter error analyses. We
detail the (Factor) rule and walk through an example in
Section~\ref{sec:structure}.

In this paper, we use the color $\bnd{\textit{blue}}$ to denote bound
polymorphism. In the (\bnd{Widen}), (\bnd{Bvar}), (\bnd{Bop}), (\bnd{$\forall$
I}), and (\bnd{$\forall$ E}) rules, we further extend Numerical Fuzz's type
system with an abstract interpretation-style analysis by annotating
$\textbf{num}_{\bnd{i}}$ types with a subscript bound with the grammar $\bnd{b}$
shown in Figure~\ref{fig:syntax}. Our type system is parametric in
$\bnd{\mathbb{B}}$, which is a set of bounds equipped with a partial order
$\leq$.
In Section~\ref{sec:bexp}, we define the $\bnd{\llbracket} - \bnd{\rrbracket}$
relation interpreting bound expressions $\bnd{b}$ to a subset of
$\bnd{\mathbb{B}}$.
We additionally extend Numerical Fuzz by adding expressions in more places (e.g.
the application rule) and a $\textbf{factor}$ primitive to enable tighter error
bounds. To support operations that have polymorphic bounds, we additionally
modify our (Op) typing rule from Numerical Fuzz.
% For presentational purposes, we present a restricted form of the
% typing rule to make the modular interface easier to follow.

\begin{figure}
%% ROW1
\begin{center}
%% unit
\AXC{}
\RightLabel{(Unit)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash \langle \rangle : \mathbf{unit}$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% var
\AXC{$s \ge 1$}
\RightLabel{(Var)}
  \UIC{$\bnd{\Delta \ |} \ \Gamma, x:_s \tau, \Theta \vdash x : \tau$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% fun
\AXC{$\bnd{\Delta \ |} \ \Gamma, x:_1 \tau_0 \vdash e : \tau$}
\RightLabel{($\multimap$ I)}
\UnaryInfC{$\bnd{\Delta \ |} \ \Gamma \vdash \lambda x. e : \tau_0 \multimap \tau $}
\bottomAlignProof
\DisplayProof
\vskip 1em

%% app
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_0 \multimap \tau$}
\AXC{$\bnd{\Delta \ |} \ \Theta \vdash f : \tau_0 $}
\RightLabel{($\multimap$ E)}
\BinaryInfC{$\bnd{\Delta \ |} \ \Gamma + \Theta \vdash ef : \tau $}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% dep prod intro
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_0$}
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash f : \tau_1$}
\RightLabel{($\times$ I)}
\BinaryInfC{$\bnd{\Delta \ |} \ \Gamma \vdash \langle e, f \rangle: \tau_0 \times \tau_1$}
\bottomAlignProof
\DisplayProof
\vskip 1em
%%


%% ROW2
%% dep prod elim
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_1 \times \tau_2$}
\RightLabel{($\times$ E)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash {\pi}_i \ e : \tau_i$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% ind prod intro
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_0 $}
\AXC{$\bnd{\Delta \ |} \ \Theta \vdash f : \tau_1$}
\RightLabel{($\tensor$ I)}
\BIC{$\bnd{\Delta \ |} \ \Gamma + \Theta \vdash (e, f) : \tau_0 \tensor \tau_1$}
\bottomAlignProof
\DisplayProof
\vskip 1em

%% ind prod elim
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_0 \tensor \tau_1$ }
\AXC{$\bnd{\Delta \ |} \ \Theta,x:_s \tau_0,y:_s\tau_1 \vdash f: \tau $}
\RightLabel{($\tensor$ E)}
\BIC{$\bnd{\Delta \ |} \ s * \Gamma + \Theta \vdash \letpair (x,y) \ = \ e \ \tin \ f : \tau $}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% ind sum intro
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_0$ }
\RightLabel{($+$ $\text{I}_i$)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash \mathbf{in}_i \ e : \tau_0 + \tau_1$}
\bottomAlignProof
\DisplayProof
\vskip 1em

% sum elim
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau_0+\tau_1$}
\AXC{$\bnd{\Delta \ |} \ \Theta, x:_s \tau_0 \vdash f_1 : \tau$ \qquad
$\Delta \ | \ \Theta, x:_s \tau_1 \vdash f_2: \tau$}
\RightLabel{($+$ E)}
\AXC{$s > 0$}
\TIC{$\bnd{\Delta \ |} \ s * \Gamma + \Theta \vdash \mathbf{case} \ e \ \mathbf{of} \ (\mathbf{in}_1 x.f_1 \ | \ \mathbf{in}_2 x.f_2) : \tau$}
\bottomAlignProof
\DisplayProof
\vskip 1em

% box elim
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : {!_s \tau_0}$}
\AXC{$\bnd{\Delta \ |} \ \Theta, x:_{t*s} \tau_0 \vdash f : \tau$}
\RightLabel{($!$ E)}
\BIC{$\bnd{\Delta \ |} \ t * \Gamma + \Theta \vdash \letcobind x = e \ \tin \ f : \tau$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
% ops
\AXC{$\{ \mathbf{op} : \bnd{\forall \epsilon_0, \epsilon_1,} \tau_0 \multimap \tau_1 \} \in \Sigma$}
\RightLabel{(Op)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash \mathbf{op} : \bnd{\forall \epsilon_0, \epsilon_1,} \tau_0 \multimap \tau_1 $}
\bottomAlignProof
\DisplayProof
\vskip 1em
%%


%% ROW 5

%%% ROW 6

% let 
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e :  \tau_0$}
\AXC{$\bnd{\Delta \ |} \ \Theta, x:_{s} \tau \vdash f : \tau$}
\RightLabel{(Let)}
\BIC{$\bnd{\Delta \ |} \ s * \Gamma + \Theta \vdash \letassign x = e \ \tin \ f : \tau$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% const
% TODO: need to describe inj function to some degree here, I suppose
\AXC{$k \in \textit{num}$}
\RightLabel{(Const)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash k : \num_{\bnd{inj(k)}}$}
\bottomAlignProof
\DisplayProof
\vskip 1em

%% widen
\AXC{$\bnd{\llbracket b_0 \rrbracket} \leq \bnd{\llbracket b_1 \rrbracket}$}
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \num_{\bnd{b_0}}$}
\RightLabel{(\bnd{Widen})}
\BIC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \num_{\bnd{b_1}}$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
% box intro
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau$ }
\RightLabel{($!$ I)}
\UIC{$\bnd{\Delta \ |} \ s * \Gamma \vdash [e] : {!_s \tau}$}
\bottomAlignProof
\DisplayProof
\vskip 1em

%%% ROW 7

%% subsumption
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e :  M_q \tau$}
\AXC{$r \ge q$}
\RightLabel{(Subsumption)}
\BIC{$\bnd{\Delta \ |} \ \Gamma \vdash e :  M_{r} \tau$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
% factor
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : (M_q \tau_0) \times (M_r \tau_1)$}
\RightLabel{(Factor)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash \factor \ e : M_{max(q,r)} (\tau_0 \times \tau_1)$}
\bottomAlignProof
\DisplayProof
\vskip 1em
%% return
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \tau$}
\RightLabel{(Ret)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash \ret e : M_0 \tau$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
%% RND
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : \num_{\bnd{i}}$}
\RightLabel{(Rnd)}
\UIC{$\bnd{\Delta \ |} \ \Gamma \vdash \rnd \ e : M_u \ \num_{\bnd{i}}$}
\bottomAlignProof
\DisplayProof
\vskip 1em


%%% ROW 8


% let-bind
\AXC{$\bnd{\Delta \ |} \ \Gamma \vdash e : M_r \tau_0$}
\AXC{$\bnd{\Delta \ |} \ \Theta, x:_{s} \tau_0 \vdash f : M_{q} \tau$}
\RightLabel{($M_u$ E)}
\BIC{$\bnd{\Delta \ |} \ s * \Gamma + \Theta \vdash \letbind x = e \ \tin \ f : M_{s*r+q} \tau$}
\bottomAlignProof
\DisplayProof
\vskip 1em

% bvar
\AXC{}
\RightLabel{($\bnd{\text{Bvar}}$)}
\UIC{$\Delta, \epsilon : \textbf{bnd} \vdash \epsilon : \textbf{bnd}$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
\AXC{$\Delta \ \bdash \ b_0 : \textbf{bnd}$}
\AXC{$\ldots$}
\AXC{$\Delta \ \bdash \ b_n : \textbf{bnd}$}
\AXC{$\textbf{bop} \in \Sigma_{\textbf{bnd}}$}
\RightLabel{(\bnd{\text{Bop}})}
\QIC{$\Delta \bdash \bnd{\textbf{bop}}(b_0, \ldots, b_n) : \textbf{bnd}$}
\bottomAlignProof
\DisplayProof
\vskip 1em

\AXC{$\Delta, \epsilon : \textbf{bnd} \ | \ \Gamma \vdash e : \tau$}
\AXC{$\epsilon \not\in FTV(\Gamma)$}
\RightLabel{(\bnd{$\forall$-I})}
\BIC{$\Delta \ | \ \Gamma \vdash \Lambda \epsilon . e : \forall \epsilon . \tau$}
\bottomAlignProof
\DisplayProof
\hskip 0.5em
\AXC{$\Delta \ | \ \Gamma \vdash e : \forall \epsilon . \tau$}
\AXC{$\Delta \bdash i : \textbf{bnd}$}
\RightLabel{(\bnd{$\forall$-E})}
\BIC{$\Delta \ | \ \Gamma \vdash e~\{i\} : \tau[i/\epsilon]$}
\bottomAlignProof
\DisplayProof

\vskip 3em

\end{center}
    \caption{Typing rules for \Lang, with $s,t,q,r,u \in \NNR \cup \{\infty\}$
      and for $i \in \{ 1, 2 \}$ where $u$ is a fixed constant parameter (see
      Definition~\ref{def:numfuzz-interface} for details on picking an adequate
      constant). $FTV(\Gamma)$ refers to all the free type variables (e.g.
    $\epsilon_0, \epsilon_1$) in $\Gamma$.}
    \label{fig:typing_rules}
\end{figure}

\subsection{Operational Semantics} \label{sec:dynamic-semantics}
In Figure~\ref{fig:sub_eval_rules}, we define the operational semantics rewrite
relation $\mapsto$ to map from closed terms in \Lang to closed terms in \Lang.
Note that we only define our language over closed terms. 
Rather than stapling together and reasoning about two separate semantics, an
ideal and approximate semantics, we combine our reasoning under one rewrite
relation. This setup simplifies the structure of our logical relation and main
soundness theorem easier to follow.

Accordingly, the interesting stepping rules that differ from Numerical Fuzz are
the $\textbf{rnd}$ rule, which steps a value to an ideal and rounded value, the
$\textbf{ret}$ rule, which steps a value to the diagonal map, and the
$\textbf{let-bind}$ stepping rule, which composes rounded computation. 
For our new \textbf{factor} primitive, we can its operational semantics as a
reassociating of the underlying ideal and approximate terms.

We also extend the Numerical Fuzz language to allow expressions in more places.
We define evaluation contexts to separate the structural plumbing of the
operational semantics from the more interesting portions of the operational
semantics.
Evaluation contexts and the characteristic \textit{context rule} allow us to
simplify reasoning about the rewrite relation, which we factor out into a
separate lemma in our soundness proof. 

\begin{figure}
\begin{center}
\begin{equation*}
\begin{aligned}[c]
  \mathbf{op} \ \bnd{\{b_0\}} \ \bnd{\{b_1\}} \ v &\mapsto op \ \bnd{\{b_0\}} \
    \bnd{\{b_1\}} \ v \\
	\pi_i\langle v_1,v_2 \rangle &\mapsto v_i \\
	(\lambda x.e) \ v &\mapsto e[v/x] \\
  \rnd k &\mapsto (k, \rho(k))
\end{aligned}
\quad
\begin{aligned}[c]
	\letassign x = v \ \tin \ e &\mapsto e[v/x] \\
  \letpair (x, y) = (v, w) \ \tin \ e &\mapsto e[v/x][w/y] \\
  \letcobind x = [v] \ \tin \ e &\mapsto e[v/x] \\
  \ret v &\mapsto (v, v)
\end{aligned}
\end{equation*}
\vskip -1em
\begin{align*}
	\mathbf{case} \ (\mathbf{in}_i \ v) \ \mathbf{of} \ (\mathbf{in}_1 \ x.e_1 \ | \ \mathbf{in}_2 \ x.e_2 )  &\mapsto e_i[v/x]
   \\
  \factor ((v_1, v_2), (v_3, v_4)) &\mapsto ((v_1, v_3), (v_2, v_4)) \\
  \textbf{bop}(c_0, \ldots, c_n) &\mapsto \textit{bop}(c_0, \ldots, c_n)
  \\
  \Lambda \epsilon . e~{c} &\mapsto e[c /\epsilon] \\
\end{align*}
  \vskip -0.25em
  \RightLabel{(Context Rule)}
  \AXC{$e \mapsto e'$}
  \UIC{$C[e] \mapsto C[e']$}
	\DisplayProof
	
  \vskip 0.4em
  \AXC{$f[v_1/x] \mapsto^* (v_3, v_4)$}
  \AXC{$f[v_2/x] \mapsto^* (v_5, v_6)$}
  \BIC{$\letbind x = (v_1, v_2) \ \tin \ f \mapsto (v_3, v_6)$}
  \DisplayProof
\end{center}
    \caption{Substitution-style evaluation rules for \Lang. Parameterized for $i
    \in \{1, 2 \}$. $op$ is a higher-order metavar. When bolded $\mathbf{op}$
    refers to the syntax and when italicized $op$ refers to the corresponding
    function on syntactic values (it may be a constant function).}
    \label{fig:sub_eval_rules}
\end{figure}

% \subsection{Equivalence class of interval expressions.}
\subsection{Modular Interface} \label{sec:interface}
Numerical Fuzz is a modular family of programming langauges paramterized
by the appropriate $\rho$, constant parameter $u$, and the appropriate set of
numeric computations $\Sigma$.  
It is the proof obligation for any language designer instantiating the language
to demonstrate that these properties hold in order for our paramterized
soundness theorems to follow.

\begin{definition}[Interface for instantiating Negative Fuzz.]
  \label{def:numfuzz-interface}
  The interface for Negative Fuzz consists of $\textit{num}$,
  $(\bnd{\mathbb{B}}, \leq)$, $d_\textbf{num}$, $\rho$, $u$, $\Sigma$, and
  $\Sigma_{\bnd{bnd}}$ such that the following properties hold: 
\begin{description}
  \item[a) The \textit{num} and distance function $d_\textbf{num}$ form a
  metric space.]
  \item[b) Property of $\rho$ and constant parameter $u$.] We assume that the
    $\forall k \in \textit{num}, d_{\mathbf{num}}(\rho(k), k)
    \leq u$ where $u$ is the grade in the $\mathbf{rnd}$ typing rule.
  \item[c) Partial order of bounds.] The set of bounds and binary relation
    $(\bnd{\mathbb{B}}, \leq)$ form a partial order.
  \item[d) Property of $\mathbf{bop} \in \Sigma_{\bnd{bnd}}$: closure over bounds ($\bnd{\mathbb{B}}$).] 
    We further need to assume that for every operation $\mathbf{bop}(b_0,
    \ldots, b_n) : \Sigma_\textbf{num}$, we have that if $c_0, \ldots, c_n \in
    \mathbb{B}$ then the corresponding
    $\textit{bop}(c_0, \ldots, c_n) = c' : \textbf{bnd}$
    holds.
  \item[e) Property of $\mathbf{op} \in \Sigma$: metric preservation.] 
    \footnote{Note that we could make this portion of the interface more general
    by definining it in terms of the logical relation in
    Section~\ref{def:logical-relation}. We have opted to keep the interface
    minimal for presentational purposes.}

    We can view $\textit{op}$ as a (possibly constant) metalevel function.
    We also assume that for every operation has the following shape with no free
    type variables
    $\mathbf{op} : \forall \epsilon_0, \epsilon_1 . \tau_0 \multimap \tau_1 \in \ \Sigma$ 
    where for the corresponding $\textit{op} \ \bnd{\{b_0\}} \ \bnd{\{b_1\}} \
    v$ for all $\bnd{b_0}, \bnd{b_1} \in \mathbb{B}$ 
    and for all v mapping bounds and syntactic values in
    $(\tau_0 \to \tau_1)[b_0 / \epsilon_0, b_1 / \epsilon_1]$
    is 1-sensitive as measured by $\mathcal{SD}$, defined below.
\end{description}
\end{definition}

The remainder of this subsection is dedicated to defining distances between
terms, which is necessary to understand the metric preservation property our
interface demans on all operations.
We first define evaluation ($\bnd{\llbracket - \rrbracket}$) of our bound expressions to $\bnd{\mathbb{B}}$ which
we call our \textit{concrete bounds}.
Each bound operation $\bnd{\textbf{bop}}$ of arity $n$ has a corresponding
mathematical function $\bnd{\textit{bop}} : \bnd{\mathbb{B}}^n \to
\bnd{\mathbb{B}}$. 
We can now define closed \textit{bound expressions}, which we will use to define
the evaluation function used in the definition of our logical relations.

\begin{definition}[Closed bound expressions]
  A bound expression $b$ is a closed bound expression iff either:
  \begin{itemize}
    \item $b = c \in \bnd{\mathbb{B}}$. In other words, $b$ is a constant.
    \item $b = \bnd{\textbf{bop}}(b_0, \ldots, b_1)$ where $b_0, \ldots, b_1$
      are all closed bound expressions. $b$ is made up of constants.
  \end{itemize}
\end{definition}

\begin{definition}[Evaluation of closed bound expressions]
  \label{def:bound-exp-eval}
  For an bound expression $b$,
  \begin{equation}
    \begin{aligned}[c]
      \bnd{\llbracket} c \bnd{\rrbracket} &\triangleq c \\
      \bnd{\llbracket} \bnd{\textbf{bop}}(b_0, \ldots, b_n) \bnd{\rrbracket} &\triangleq \textit{bop}(b_0, \ldots, b_n)
    \end{aligned}
  \end{equation}
\end{definition}

% \begin{definition}[Bound expression equivalence class]
%   We inductively define our equivalence relation $\sim$ over closed bound
%   expressions $\bnd{b, b_0, b_1 \in \mathbb{B}_{\text{exp}}}$ based upon the
%   underlying equality of the sets $b_0 \sim b_1 \iff \bnd{\llbracket} b_0
%   \bnd{\rrbracket} = \bnd{\llbracket} b_0 \bnd{\rrbracket}$.
% \end{definition}
%
% \begin{lemma}
%   $\bnd{\mathbb{B}_{\text{exp}}}, \sim$ form an equivalence class
% \end{lemma}
% \begin{proof}
%   $\sim$ obeys symmetry, reflexivity, and transitivity by inspection of the
%   underlying equality.
% \end{proof}
%
% We are now ready to define a lattice structure over our the quotient space
% formed by our interval equivalence class $\bnd{\mathbb{B}_{\text{exp}}} / \sim$. We will later use this to
% define our logical relation over these interval bound equivalence classes.
%
% \begin{definition}[Bound expression partial order]
%   $b_0 \subseteq b_1 \iff \llbracket b_0 \rrbracket \subseteq \llbracket b_1
%   \rrbracket$.
%   The $\subseteq$ relation clearly respects our equivalence relation $\sim$ by
%   inspection of the underlying equality on sets used in the construction.
% \end{definition}

We proceed to define distances between bounds and expressions.
Our syntactic definition for distance, $d_\tau$ and the distance between
syntactic values $\mathcal{SD}_\tau$ (for $\mathcal{S}$yntactic
$\mathcal{D}$istance) and $\mathcal{SDV}_\tau$ (for $\mathcal{S}$yntactic
$\mathcal{D}$istance for $\mathcal{V}$alues) are closely related. Some care is
needed to ensure that the relation is well-founded. We define our distance over
syntactic values as follows:

\begin{definition}[Distance between closed syntactic terms]
  \begin{equation}
  \begin{aligned}[c]
    \mathcal{SD_{\tau}}(e_0, e_1) &\triangleq \mathcal{SDV}_{\tau}(v_0, v_1)
    &\text{ if } e_0 \mapsto^{*} v_0 \text{ and } e_1 \mapsto^{*} v_1 \\
    \mathcal{SD_{\tau}}(e_0, e_1) &\triangleq \infty &\text{ otherwise } \\
    \mathcal{SDV_{\mathbf{unit}}}(v, w) &\triangleq 0 &\text{ for } v, w =
      \langle \rangle \\
    \mathcal{SDV}_{\mathbf{num_{b}}}(c_0, c_1) &\triangleq 
      d_\mathbf{num} &\text{ for } c_0, c_1 \in \llbracket b \rrbracket \\
    \mathcal{SDV}_{\tau_0 \times \tau_1}((v_0, v_1), (w_0, w_1)) 
      &\triangleq max(\mathcal{SDV}_{\tau_0}(v_0, w_0),~\mathcal{SDV}_{\tau_0}(v_1, w_1))
    \\
    \mathcal{SDV}_{\tau_0 \otimes \tau_1}((v_0, v_1), (w_0, w_1)) 
      &\triangleq \mathcal{SDV}_{\tau_0}(v_0, w_0) + \mathcal{SDV}_{\tau_0}(v_1, w_1))
    \\
    \mathcal{SDV}_{\tau_0 + \tau_1}(\tin_i~v, \tin_i~w) 
      &\triangleq \mathcal{SDV}_{\tau_i}(v, w)
    \\
    \mathcal{SDV}_{\tau_0 \multimap \tau_1}(v_0, v_1) 
      &\triangleq \text{sup}_{w \in \mathcal{VR}_{\tau_0}} \mathcal{SD}_{\tau_1}(v_0~w,~v_1~w)
    \\
    % max: double check
    \mathcal{SDV}_{!_s \tau}([v], [w]) 
      &\triangleq s \cdot \mathcal{SDV}_{\tau}(v, w)
    \\
    \mathcal{SDV}_{M_q~\tau}((v_0, v_1), (w_0, w_1)) 
      &\triangleq \mathcal{SDV}_{\tau}(v_0, w_0)
    \\
    \mathcal{SDV}_{\textbf{bnd}}(\bnd{b_0}, \bnd{b_1}) 
      &\triangleq 0 &\text{ for } \bnd{\llbracket b_0 \rrbracket = \llbracket b_1 \rrbracket} 
    \\
    \mathcal{SDV}_{\textbf{bnd}}(\bnd{b_0}, \bnd{b_1}) 
      &\triangleq \infty & \text{otherwise}
    \\
    \mathcal{SDV}_{\forall \epsilon . \tau}(v_0, v_1) 
      &\triangleq \text{sup}_{c \in \mathbb{B}} 
      \mathcal{SD}_{\tau[w / \epsilon]}(v_0~\{c\},~v_1~\{c\})
    \\
    \mathcal{SDV_{\tau}}(v, w) &\triangleq \infty &\text{ otherwise } \\
  \end{aligned}
  \end{equation}
\end{definition}

\begin{definition}[Distance between expressions.]
For any two closed expressions $e_0, e_1$ falling in same type relation $e_0,
e_1 \in R_\tau$ (where $\tau$ is a closed type), we can write $e_0 \sim_r e_1 :
\tau$ where $\mathcal{SD}_{\tau}(e_0, e_1) \leq r$. 
\end{definition}

\subsection{Type Soundness} \label{sec:soundness}

In our language, we only reason about closed types and terms. Let $CV(\tau)$ be
the closed values of type $\tau$ and $CE(\tau)$ be the closed expressions of
type $\tau$. Then we can define a unary logical relation over types which
capture the core information needed to prove our error soundness theorem (Metric
Preservation, Theorem~\ref{thm:metric-preservation}).

\begin{definition}[Logical relation] \label{def:logical-relation}
  \begin{equation}
  \begin{aligned}[c]
    \mathcal{R_{\tau}} &\triangleq 
      \{ e \ | \ e \in CE(\tau) \text{ and } \exists v
        \in CV(\tau) \text{ s.t. } e \mapsto^{*} v \text{ and } v \in \mathcal{VR_{\tau}} 
      \} \\
    \mathcal{VR_{\mathbf{unit}}} &\triangleq \{ \langle \rangle \} \\
    \mathcal{VR}_{\mathbf{num}_{b}} &\triangleq \bnd{\llbracket b \rrbracket} \\
    % \mathcal{VR}_{\mathbf{num}_{(k_0, k_1)}} &\triangleq 
    %   \{ r \ | \ r \in \mathit{num} \text{ and } k_0 \leq r \leq k_1 \} \\
    \mathcal{VR_{\mathbf{\tau_0 \times \tau_1}}} &\triangleq 
      \{ \langle v, w \rangle \ | 
        \ v \in \mathcal{R}_{\tau_0} \text{ and } w \in \mathcal{R}_{\tau_1}
      \} \\
    \mathcal{VR_{\mathbf{\tau_0 \otimes \tau_1}}} &\triangleq 
      \{ ( v, w ) \ | 
        \ v \in \mathcal{R}_{\tau_0} \text{ and } w \in \mathcal{R}_{\tau_1}
      \} \\
    \mathcal{VR_{\mathbf{\tau_0 + \tau_1}}} &\triangleq 
      \{ \mathbf{inl}~v \ | \ v \in \mathcal{R}_{\tau_0} \} 
      \cup
      \{ \mathbf{inr}~v \ | \ v \in \mathcal{R}_{\tau_1} \} \\
    \mathcal{VR_{\mathbf{\tau_0 \multimap \tau_1}}} &\triangleq 
      \{ \lambda x . e \ | \ \forall w_0, w_1 \in \mathcal{VR}_{\tau_0}, \\ & \quad \quad \ (\lambda x.e)~w_0, (\lambda x . e)~w_1 \in
      \mathcal{R}_{\tau_1} \text{ and } \mathcal{SD}_{\tau_1}((\lambda x . e)~w_0, (\lambda x . e)~w_1) \leq
      \mathcal{SD}_{\tau_0}(w_0, w_1) \} \\
    \mathcal{VR_{\mathbf{!_s \tau}}} &\triangleq 
      \{ [~v~] \ | \ v \in \mathcal{R}_{\tau} \} \\
    % spicy hot new stuff
    \mathcal{VR_{\mathbf{M_q \tau}}} &\triangleq 
      \{ (v, w) \ | \ v, w \in \mathcal{R}_{\tau} \text{ and } \mathcal{SDV}_{\tau}(v, w)
      \leq q \} \\
    % \mathcal{VR}_{\textbf{bnd}} &\triangleq \{ (k_0, k_1) \ | \ k_0 \leq k_1 \ \forall k_0, k_1 \in
    % \textit{ num } \} \\
    \mathcal{VR_{\forall \epsilon. \tau}} &\triangleq 
    \{ \Lambda \epsilon . v \ | \ \forall \bnd{c} \in \bnd{\mathbb{B}}, \ v ~ 
      \{\bnd{c}\} \in
      \mathcal{R}_{\tau[\bnd{c} / \epsilon]}\} \\
    % \mathcal{VR_{\forall \epsilon. \tau}} &\triangleq 
    %   \{ (\llbracket b \rrbracket, v) \ | \ \forall (k_0, k_1) \in
    %   \mathcal{VR}_{\textbf{bnd}} \text{ s.t. } v ~ \{(k_0, k_1)\} \in
    %   \mathcal{R}_{\tau[(k_0, k_1) / \epsilon]}\} \\
  \end{aligned}
  \end{equation}
\end{definition}

\begin{lemma}[$\mathcal{SD}$ is a metric]
  $\mathcal{SD}$ forms a metric over our syntactic terms; in particular it
  satisfies:
  \begin{enumerate}
    \item Distance from any point to itself is zero: $\forall x,~\mathcal{SD}(x,
      x) = 0$.
    \item Positivity: $\forall x, y,~\mathcal{SD}(x, y) \geq 0$.
    \item Symmetry: $\forall x, y,~\mathcal{SD}(x, y) = \mathcal{SD}(y, x)$.
    \item Triangle inequality: $\forall x, y, z,~\mathcal{SD}(x, z) \leq
      \mathcal{SD}(x, y) + \mathcal{SD}(y, z)$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The properties holds for the base cases of $\mathcal{SDV}$ and follow for the
  remaining cases by our inductive hypothesis. Since our operational semntics is
  deterministic, the properties follow for $\mathcal{SD}$.
\end{proof}

\begin{lemma}[$\mathcal{SD}$ is preserved under stepping]
  If $e_0 \mapsto e'_0$, then for any $e_1$, $\mathcal{SD}(e_0, e_1) =
  \mathcal{SD}(e'_0, e_1)$.
\end{lemma}
\begin{proof}
  Holds by inspection of the definition of $\mathcal{SD}$.
\end{proof}
Note that by metric symmetry, $\mathcal{SD}$ is preserved under stepping on both
sides.

Following Fuzz, we prove the following syntanctic properties relating our type
system and operational semantics. These syntactic properties will be useful for
reasoning about the subtyping and type inference in Section~\ref{sec:inference}.

\begin{lemma}[Admissibility of weakening]\label{thm:weakening}
  If $\Delta \ | \ \Gamma \vdash e : \tau$, then 
  $\Delta + \Delta' \ | \ \Gamma + \Sigma \vdash e : \tau$.
\end{lemma}
\begin{proof}
  We induct over our typing devivation. 
  For each case, we can ignore arbitrary variables in our context (the type
  system is affine). 
  In particular, for the \textbf{!E}, \textbf{$\otimes$ E}, \textbf{+ E},
  \textbf{$M_u$~E}, and \textbf{$\otimes$ I} rules, the enviroment must be split
  into a $\Gamma$ (which can in some cases be scaled) and $\Delta$ (which does
  not ever get scaled); for each of these cases we choose to push unused
  variables into $\Delta$. 
\end{proof}

\begin{lemma}[Admissibility of weakening (on bound contexts)]\label{thm:weakening-bnd}
  If $\Delta \bdash i : \bnd{\textbf{bnd}}$ and $\Delta \subseteq \Delta'$, then
  $\Delta' \bdash i : \bnd{\textbf{bnd}}$. 
\end{lemma}
\begin{proof}
  $\Delta$ tracks the free type variables in $i$. 
  We induct over our typing devivation. The base cases hold. For the inductive
  cases, we induct over the size of $\Delta' - \Delta$.
  Our proof is complete.
\end{proof}

We now define vectors of expressions, and substitutions which will also be
useful later for stating and proving our main type soundness proof (Metric
Preservation, Theorem~\ref{thm:type-soundness}). We proceed to incrementally
build up to the definition of distance between substitutions below.

\begin{definition}[Distance between expression vectors.]
    We write vectors of expressions $\sigma$, $\sigma'$ are $\gamma$ apart for a
    given typing context $\Delta \ | \ \Gamma$ like so 
    $\sigma \sim_{\gamma} \sigma' : \Delta \ | \ \Gamma$
    if and only if
    $\gamma = r_0, r_1, \ldots$
    where
    $\sigma = \sigma_{\Delta}, e_0,~\ldots,~e_n$ 
    and 
    $\sigma' = \sigma_{\Delta}, e'_0,~\ldots,~e'_n$ 
    such that:
    $$
    e_0 \sim_{r_0} e'_0 : \tau_0~[\sigma_{\Delta}],
    ~\ldots,
    ~e_n \sim_{r_n} e'_n : \tau_n~[\sigma_{\Delta}]
    $$
    where $\sigma_{\Delta}$ is a bound vector with no free variables.
    Note that the distance between expression vectors are undefined when the
    bound vector $\sigma_{\Delta}$ differs between substitutions.
\end{definition}

\begin{definition}[Enviroment compatibility.]
We also say that a substitution vector 
$$\sigma_{\Delta}, [e_0/x_0, \ldots, e_n/x_n]$$
is \textit{compatible} with a typing context 
$x_0 : \tau_0, \ldots, x_n : \tau_n$
if each term 
$e_0 \in \mathcal{R}_{\tau_0~[\sigma_{\Delta}]}, 
\ldots,
e_n \in \mathcal{R}_{\tau_n~[\sigma_{\Delta}]}$ where all types are closed by
the bound vector $\sigma_{\Delta}$.
\end{definition}

\begin{definition}[Dot product of distance vectors.]
Our definition for the dot product of a distance vector $\gamma = r_0, \ldots,
r_n$ with respect to a context $\Gamma$ with sensitivities $s_0, \ldots, s_n$ is
the same as Fuzz \cite{Fuzz}: $\gamma \cdot \Gamma = \Sigma_{i = 1}^n r_i s_i$.
\end{definition}

In the remainder of the paper, we'll treat and represent our distance vector
$\gamma$ as a lookup function and assume that there is an implicit fixed
ordering on the variables.
\begin{definition}[Distance vector lookup.]
  We write, for a distance vector $\gamma$ and variable $x$, $\gamma(x)$ for the
  lookup of the distance of variable $x$ in $\gamma$. If the variable $x$ is not
  in the domain, $\gamma(x) = 0$ by default. 
\end{definition}

We have now finished defining all the components necessary to state our logical
relation. 
% TODO: check that we properly handle the not zero and intersection condition
% everywhere this is used
Now that we can talk about subsitutions, we follow Fuzz and prove a
$r$-sensitive substitution lemma.
\begin{lemma}[$r$-sensitive subsitution]\label{thm:substitution}
  Let $\Delta_0 \ | \ \Gamma \vdash e : \tau$ and $\Delta_1 \ | \ \Theta, x :_r
  \tau \vdash e' : \tau'$ and $r \not= 0$ and $\Delta_0 \cap \Delta_1 =
  \emptyset$, then $\Delta_0 + \Delta_1 \ | \ r \cdot \Gamma + \Theta \vdash
  e'[e/x] : \tau'$.
\end{lemma}
\begin{proof}
  We first induct over the height of the final typing derivation with
  substitution. Note that the $\bnd{Bvar}$ and $\bnd{Bop}$ rules are rules for a
  different typing judgement from our main judgement $\vdash$. The Unit, Const,
  and Op base cases are immediate. We detail the remaining cases here:
  \begin{description}
    \item[Var.] In the case that $s=1$, holds immediately. Otherwise, we apply
      weakening.
    \item[Inductive cases with possible variable capture $\multimap$ I, $\otimes E$,
      $+$ E, $!$ E, Let, $M_u$ E] 
      We case: if $x$ is immediately captured by the expression (e.g. $\lambda$ for
      $\multimap$ I or $x$ and $y$ for the \textbf{let-pair} expression, etc.), we can
      apply our inductive hypothesis.
      Otherwise, it holds immediately.
    \item[Inductive case: $\forall$ - I.]
      Holds by our inductive hyothesis and observing that the side condition
      necessary to apply the $\forall$ I rule  is satisifed by the  $\Delta_0
      \cap \Delta_1 = \emptyset$ condition.
    \item[Remaining inductive cases.] The remaining cases ($\multimap$ E,
      $\times$ I, $\times$ E, $\otimes$ I, $+$ $I_i$, Widen, ! I, Subsumption,
      Factor, Ret, Rnd, $\forall$ E) follow immediately by application of the
      inductive hypothesis.
  \end{description}
\end{proof}

% todo: put it in here

% Max: Theorem statement is not interesting when stepping is only over closed
% terms.
% \begin{lemma}[Subsitution is invariant under stepping]
%   \label{thm:sub-stepping}
%   For any substitution $\sigma$ and program $e$ such that $e~\sigma \mapsto
%   e_1$, if $e \mapsto e_2$ then $e_2~\sigma \mapsto^{*} v \iff e_1 \mapsto^{*} v$.
% \end{lemma}
% \begin{proof}
%   TODO
% \end{proof}
%
\begin{lemma}[Subsitution decomposition]
  \label{thm:sub-decomp}
  For substitutions 
  $\sigma \sim_{\alpha} \sigma' : \Delta_0 + \Delta_1 \ | \ \Gamma + \Theta$,
  there exists
  $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : \Delta_0 | \Gamma$
  and
  $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Delta_1 \ | \ \Theta$.
\end{lemma}
\begin{proof}
  Follows by induction over the length of the relation $\sigma \sim_{\alpha}
  \sigma'$.
\end{proof}

The below lemma is useful towards proving metric preservation:
\begin{lemma}[Metric preservation under context evaluation stepping]
  \label{thm:ctx-stepping}
  For a well-typed $\Delta \ | \ \Gamma \vdash C[e] : \tau$ and substitutions $\sigma,
  \sigma'$ such that $\sigma \sim_{\gamma} \sigma' : \Delta \ | \ \Gamma$ and
  $e~\sigma \mapsto^{*} v$ and
  $e~\sigma' \mapsto^{*} v'$.
  If $C[v] \sigma \sim_{\gamma \cdot (\Delta \ | \ \Gamma)} C[v'] \sigma' : \tau$ 
  then
  $C[e] \sigma \sim_{\gamma \cdot (\Delta \ | \ \Gamma)} C [e] \sigma' : \tau$.
\end{lemma}
\begin{proof}
  By inspection of our stepping relation, we can tell that stepping is
  deterministic. Further, since we only define our rewrite relation over closed
  terms, we know that $e~\sigma, e~\sigma'$ is closed and therefore constant
  under all substitutions. 
  So, by the definition of $\mathcal{R}$, $\mathcal{SD}$, and context stepping,
  we know that both 
  $$
  C[e~\sigma]~\sigma, C[e~\sigma']~\sigma' \in \mathcal{R}_{\tau}
  $$ 
  and that 
  $$
  \mathcal{SD}_{\tau}(C[e]\sigma, C[e]\sigma') = 
  \mathcal{SD}_{\tau}(C[e~\sigma]\sigma, C[e~\sigma']\sigma') = 
  \mathcal{SD}_{\tau}(C[v]\sigma, C[v']\sigma') = 
  \gamma \cdot (\Delta \ | \ \Gamma)
  $$
  respectively and therefore 
  $C[e] \sigma \sim_{\gamma \cdot (\Delta \ | \ \Gamma)} C[e] \sigma'$
  holds.
\end{proof}
Finally, we can prove our big error soundness theorem.
\begin{theorem}[Metric preservation]
  For any $\Delta \ | \ \Gamma \vdash e : \tau$ and substitutions $\sigma, \sigma'$ such that
  $\sigma \sim_{\gamma} \sigma' : \Delta \ | \ \Gamma$, then 
  $e~\sigma \sim_{\gamma \cdot (\Delta \ | \ \Gamma)} e~\sigma' : \tau$.
\end{theorem} \label{thm:metric-preservation}
\begin{proof}
  We induct over our typing derivation. The base cases (Var,
  \bnd{\textbf{bvar}}, Unit, Const) follow trivially. The subsumption and
  widening cases also follow trivially. We detail the remaining cases here. Note
  that the distance between $\bnd{\textbf{bnd}}$ vars in $\Delta$ is $\infty$ if
  they correspond to different bounds. Therefore for the following cases, we
  omit reptitively reasoning about $\Delta$ substitutions:
  \begin{description}
    \item[Case $\multimap$ I.] 
      We wish to show that for any $\Gamma \vdash \lambda x . e : \tau$ and
      subsitutions $\sigma \sim_{\gamma} \sigma' : \Gamma$ that $\lambda x .
      e~\sigma \sim_{\gamma \cdot \Gamma} \lambda x . e~\sigma'$. Unfolding, it
      suffices to show that both:
      \begin{enumerate}
        \item $\lambda x . e~\sigma, \lambda x . e~\sigma'$ are in
          $\mathcal{R}_{\tau_0 \multimap \tau}$
        \item $\mathcal{SD}_{\tau_0 \multimap \tau}(\lambda x . e~\sigma,
          \lambda x . e~\sigma') \leq \gamma \cdot \Gamma$
      \end{enumerate}
    
      Observe that we have by our inductive hypothesis that for any $\Delta, x:
      \tau' \vdash e : \tau$ and substitutions 
      $\delta[v_0/x] \sim_{\gamma'} \delta'[v_1/x] : \Gamma,~x : \tau'$ that 
      $e~\delta[v_0/x] \sim_{\gamma' \cdot \Gamma} e~\delta'[v_1/x]$ holds. 
      Let us now carry on with the proof:

      \begin{description}
        \item[\underline{Property 1.}] We need to show that $(\lambda x . e)~\sigma$ and
          $(\lambda x . e)~\sigma'$ are both in the relation $\mathcal{R}_{\tau_0 \multimap
          \tau}$. The cases are symmetric so we only show one case. Unfolding
          the definition of $\mathcal{R}_{\tau_0 \multimap \tau}$, let $w_0, w_1
          \in \mathcal{VR}_{\tau_0}$. 
          $(\lambda x . e) w_0 \mapsto e[w_0/x]$
          and
          $(\lambda x . e) w_1 \mapsto e[w_1/x]$
          and so it suffices to show that
          $e~\sigma[w_0/x], e~\sigma[w_1/x]$ 
          are closed expressions falling in $R_{\tau_0}$.
          This is true by our inductive hypothesis, instantiating with $\sigma =
          \delta = \delta'$, $v_0 = w_0$, and $v_1 = w_1$.
        \item[\underline{Property 2.}] Unfolding the definition of
          $\mathcal{SD}_{\tau_0 \multimap \tau}$, it suffices to show that:
          $$
          \mathcal{SD}_{\tau}
          ((\lambda x . e~w)~\sigma, (\lambda x . e~w)~\sigma') 
          \leq \gamma \cdot \Gamma
          $$
          Stepping, it suffices to show that
          $$
          \mathcal{SD}_{\tau}
          (e~\sigma[w/x], e~\sigma'[w/x]) 
          \leq \gamma \cdot \Gamma
          $$
          which holds by application of our inductive hypothesis when $\delta =
          \sigma$, $\delta' = \sigma'$, $v_0 = v_1 = w$, and $\gamma' = \gamma
          :: 0$.
      \end{description}
      So, by our inductive hypothesis and unfolding the definitions of
      $\mathcal{R}$ and $\mathcal{SD}$, we have properties (1) and (2)
      respectively. 
    \item[Case $\multimap$ E.] 
      By our inductive hypothesis, it suffices
      to prove this case for $e~f$.
      % By inversion, we know that $v = \lambda x . e$.
      We wish to show that for any $\Gamma + \Theta \vdash e~f : \tau$ and
      subsitutions 
      $\sigma \sim_{\alpha} \sigma': \Gamma + \Theta$ 
      that 
      $$e~f~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} e~f~\sigma'$$
      Using Lemma \ref{thm:sub-decomp} (substitution decomposition), we
      construct potentially overlapping substitutions
      $\sigma_0 \sim_{\alpha} \sigma_1 : \Gamma$
      and $\sigma_0' \sim_{\alpha} \sigma_1' : \Theta$
      where $FV(e) \subseteq DOM(\sigma_0) = DOM(\sigma_1)$ and
      $FV(f) \subseteq DOM(\sigma_0') = DOM(\sigma_1')$ and 
      $\sigma_0, \sigma_0' \subseteq \sigma$ and 
      $\sigma_1, \sigma_1' \subseteq \sigma'$ and $\gamma, \theta$ minimal.
      In other words, $\sigma_0, \sigma_1$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Gamma$.
      Similarly, $\sigma_0', \sigma_1'$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Theta$.
      Note that our substitutions \textit{must} overlap in the case where $FV(e)
      \cap FV(f)$ is non-empty.
      By our inductive hypothesis, we know that 
      $e~\sigma \mapsto^* \lambda x . M$ and 
      $e'~\sigma \mapsto^* \lambda x . M'$ for some $M, M'$.
      Since subsituting variables that are not free does not change the
      underlying term, we know that the following equations must hold where
      $\sigma^*, \sigma'^*$ are $\sigma$ and $\sigma'$ respectively with $x$
      removed:
      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . M)~[\sigma_0]~(w[\sigma_1]) &= 
            ((\lambda x . M[\sigma^*])~w)[\sigma] = 
            ((\lambda x . M)~w)[\sigma] \mapsto
            M[\sigma][w/x] \\
          (\lambda x . M')~[\sigma_0']~(w[\sigma_1']) &= 
            ((\lambda x . M'[\sigma'^*])~w)[\sigma'] = 
            ((\lambda x . M')~w)[\sigma'] \mapsto
            M'[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      and by stepping

      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . M)~[\sigma_0]~(w[\sigma_1]) &\mapsto M[\sigma_0][w[\sigma_1]/x] \\
          (\lambda x . M)~[\sigma_0']~(w[\sigma_1']) &\mapsto M'[\sigma_0'][w[\sigma_1']/x] \\
        \end{aligned}
      \end{equation}

      so by deterministic stepping
      \begin{equation}
        \begin{aligned}[c]
          M[\sigma_0][w[\sigma_1]/x] &=
            M[\sigma][w/x] \\
          M'[\sigma_0'][w[\sigma_1']/x] &=
            M'[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      By applying our inductive hypothesis and unfolding our definition of
      $R_{\tau_0 \multimap \tau}$, we know that $M, M'$ is 
      1-sensitive with respect to $w$ according to $\mathcal{SD}$. 
      This gives us:
      $$
      \mathcal{SD}_{\tau}(M~[w[\sigma]/x][\sigma], M~[w[\sigma']/x][\sigma]) 
      \leq 
      \mathcal{SD}_{\tau_0}(w[\sigma], w[\sigma']) 
      \leq 
      \theta \cdot \Theta
      $$
      and we also know that by our inductive hypothesis
      $$
      \mathcal{SD}_{\tau}(e~[\sigma]~f~[\sigma'], e~[\sigma']~f~[\sigma']) 
      =
      \mathcal{SD}_{\tau}(M~[w[\sigma']/x][\sigma], M'~[w[\sigma']/x][\sigma']) 
      \leq 
      \gamma \cdot \Gamma
      $$
      So by our triangle inequality of the syntactic distance between terms:
      $$
      M~[w[\sigma]/x]~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta}
      M'~[w[\sigma']/x]~\sigma'
      $$

      and therefore
      $$
      (\lambda x . M)~w~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta} (\lambda x . M')~w~\sigma'
      $$

      To complete the proof case, it suffices to prove 
      $$\alpha \cdot (\Gamma + \Theta) \geq \alpha \cdot \Gamma + \alpha \cdot \Theta$$

      which holds by construction (via our substitution decomposition lemma).
    \item[Case $M_q~e$ (let-bind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Like the $\multimap I$ case, we only need to prove one side due to
          symmetry. So we fix a substitution $\sigma$. By Lemma
          \ref{thm:ctx-stepping}, it suffices to prove this case for a value
          $(v_0, v_1)$. 
          We have by our inductive hypothesis that $(v_0, v_1) \in \mathcal{R}_{M_r
          \tau_0}$. So $\mathcal{SD}(v_0, v_1) \leq r$. By application of our
          inductive hypothesis, we know that (abusing notation) 
          $f[v_1/x], f[v_2/x] \in \mathcal{R}_{M_q}$ so
          $f[v_1/x] \mapsto^{*} (v_3, v_4), f[v_2/x] \mapsto^{*} (v_5, v_6)$ where 
          $(v_3, v_4), (v_5, v_6) \in \mathcal{R}_{M_q}$.
          We also know that 
          $$\sigma[v_0/x] \sim_{\vec{0}, r} \sigma[v_1/x] : \Gamma, x : \tau_0$$
          that
          $$f~\sigma[v_0/x] \sim_{(\vec{0},r) \cdot (\Gamma, x \mapsto s)} f~\sigma[v_1/x] : M_q \tau$$
          So therefore
          \begin{equation}
            \begin{aligned}[c]
              (v_3, v_4)~\sigma[v_1/x] 
                &\sim_{(\vec{0},r) \cdot (\Gamma, x \mapsto s)} 
              (v_5, v_6)~\sigma[v_2/x] : M_q \tau \\
              (v_3, v_4)~\sigma[v_1/x] 
                &\sim_{r \cdot s} 
              (v_5, v_6)~\sigma[v_2/x] : M_q \tau
            \end{aligned}
          \end{equation}
          So clearly $(v_3, v_6) \in \mathcal{R}_{M_{s \cdot r + q}}$ by
          application of triangle inequality: 
          $$
          r \cdot s + q \geq
          \mathcal{SD}_{tau}(v_3, v_5) + \mathcal{SD}_{\tau}(v_5, v_6) \geq
          \mathcal{SD}_{\tau}(v_3, v_6)
          $$
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : s \cdot \Gamma + \Theta$:
          \begin{equation}
            \label{eq:lb-prop2}
          \textbf{let-bind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (s
          \cdot \Gamma + \Theta)} \textbf{let-bind}~x = e \ \tin \ f \sigma' :
          M_{s \cdot r + q}
          \end{equation}
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : s \cdot \Gamma$
          and $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$ .

          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} : M_r \tau_0
          $$
          By our proof of \textbf{\underline{Property 1}}, we know that both
          sides of Equation~\ref{eq:lb-prop2} are in the relation
          $\mathcal{R}_{M_{r \cdot s + q} \tau}$ and we're not stuck. We want to
          show both sides have syntatic distance less than $\alpha \cdot (s
          \cdot \Gamma + \Theta)$. 
          We apply our inductive hypothesis once more and extend the
          $\theta$ substutitons to obtain:
          \begin{equation} \label{eq:lb-prop2.1}
            f~\sigma_{\theta}[e~\sigma_{\gamma}/x] 
            \sim_{\alpha \cdot \Theta + (\alpha \cdot s \cdot \Gamma)} 
            f~\sigma'_{\theta}[e~\sigma'_{\gamma}/x] : M_{r} \tau
          \end{equation}
          Since both sides are neighborhood monads, it suffices to bound
          distances in only the first components of each side which means that
          Equation~\ref{eq:lb-prop2.1} implies Equation~\ref{eq:lb-prop2}.
      \end{description}
    \item[Case (let-cobind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We again only need to prove one side due to symmetry.
          By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for a
          value $[v]~\sigma$ for a $\sigma$ compatible withi $\Gamma$. 
          We have by our inductive hypothesis that $[v]~\sigma \in \mathcal{R}_{!_s
          \tau_0}$. Unfolding, we have that $v~\sigma \in \mathcal{R}_{\tau_0}$.
          Stepping and applying our inductive hypothesis yields that
          $f~\sigma~[v/x] \in \mathcal{R}_{\tau}$.

        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : t \cdot \Gamma + \Theta$:
          $$\textbf{let-cobind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (t
          \cdot \Gamma + \Theta)} \textbf{let-cobind}~x = e \ \tin \ f \sigma' :
          \tau$$
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : \Gamma$
          and $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$.
          By application of the inductive hypothesis, we know that:
          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0
          $$
          where 
          $$
          e~\sigma_{\gamma} = [v_0]
          $$
          and
          $$
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0 = [v_1]
          $$
          for some $v_0, v_1$ so therefore by inspection of the relation:
          $$
          s \cdot \mathcal{SD}_{\tau_0}(v_0, v_1) =  
          \mathcal{SD}_{!_s~\tau_0}([v_0], [v_1])
          $$
          Let $\mathcal{SD}_{\tau_0}(v_0, v_1) = c$. Importantly, note that 
          \begin{equation} \label{eq:lcb-zero-sub}
            s \cdot c = \alpha \cdot \Gamma
          \end{equation}
          Then, we apply our inductive hypothesis once more and we extend the
          $\theta$ substutitons and to obtain:
          $$
          f~\sigma_{\theta}[\sigma_{\gamma}/x] 
          \sim_{\alpha \cdot \Theta + t \cdot s \cdot c}
          f~\sigma'_{\theta}[\sigma'_{\gamma}/x] : \tau
          $$
          Note that the distance $\sim$ is obtained via the inductive hypothesis
          and the fact that it is in $R_{M_{s \cdot r + q}}$ is obtained from
          our proof of \textbf{\underline{Property 1}}.
          Substituting by Equation~\ref{eq:lcb-zero-sub} we complete the case:
          $$
          f~\sigma_{\theta}[\sigma_{\gamma}/x] 
          \sim_{\alpha \cdot \Theta + t \cdot \alpha \cdot \Gamma}
          f~\sigma'_{\theta}[\sigma'_{\gamma}/x] : \tau
          $$
      \end{description}

    \item[Case (let).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. By symmetry, it suffices to
          only prove one side. Follows by application of our inductive
          hypothesis and some stepping.
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha} \sigma' :
          s \cdot \Gamma + \Theta$:
          $$
          \textbf{let} \ x = e \ \tin \ f \sigma 
            \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
          \textbf{let} \ x = e \ \tin \ f \sigma' :
          \tau
          $$
          By Lemma~\ref{thm:sub-decomp} we have substitutions $\sigma_{\gamma}
          \sim_\alpha \sigma'_{\gamma} : \Gamma$ and $\sigma_{\theta}
          \sim_\alpha \sigma'_{\gamma} : \Theta$. By applying our inductive
          hypothesis, we know that 
          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \tau_0
          $$
          So we can extend our pair of $\Theta$ subsitutions by these two terms
          respsectively and apply our inductive hypothesis yielding:
          $$
          \textbf{let} \ x = e \ \tin \ f \sigma 
            \sim_{(\alpha \cdot s \cdot \Gamma) + (\alpha \cdot \Theta)}
          \textbf{let} \ x = e \ \tin \ f \sigma' :
          \tau
          $$
          which is equivalent to what we wanted to show.

      \end{description}

    \item[Case factor.] 
      By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for 
      $\mathbf{factor}~((v_1, v_2), (v_3, v_4))$,  

      $\mathbf{factor}~((v'_1, v'_2), (v'_3, v'_4))$. 
      Unfolding our inductive hypothesis, we have that for substitutions
      $\sigma \sim_{\gamma} \sigma' : \Gamma$, that 
      $$
      \textbf{factor} \ ((v_1, v_2), (v_3, v_4))~\sigma 
      \sim_{\gamma \cdot \Gamma} 
      \textbf{factor} \ ((v'_1, v'_2), (v'_3, v'_4))~\sigma' 
      : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      Unfolding, we get:
      $$
      \textbf{factor} \ ((v_1~\sigma, v_2~\sigma), (v_3~\sigma, v_4~\sigma)) 
      \sim_{\gamma \cdot \Gamma} 
      \textbf{factor} \ ((v'_1~\sigma', v'_2~\sigma'), (v'_3~\sigma', v'_4~\sigma')) 
      : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      So by stepping (and reassociating) we can see that the following logical
      relation holds as distances between neighborhood monads are measured in
      terms of the first components:
      $$
      ((v_1~\sigma, v_3~\sigma), (v_2~\sigma, v_4~\sigma)) 
      \sim_{\gamma \cdot \Gamma} 
      ((v'_1~\sigma', v'_3~\sigma'), (v'_2~\sigma', v'_4~\sigma')) 
      : M_{max(r,q)}(\tau_0 \times \tau_1)
      $$

    \item[Case rnd.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. The case holds under the
          assumption the constant langauge parameter $u$ has the required
          properties (see Definition~\ref{def:numfuzz-interface}, property a)
          and application of our inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case ret.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. Holds trivially by application
          of the inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case op.] 
      Both properties hold by our language interface
      (Definition~\ref{def:numfuzz-interface}, property b) and application of
      the inductive hypothesis.

    \item[Case ! I.] Holds unfolding of our logical relations, and our inductive
      hypothesis.

    \item[Case $\times$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Holds by application of the inductive hypothesis and unfolding of the
          definition of $\mathcal{R}_{\tau_0 \times \tau_1}$.
        \item[\underline{Property 2.}]
          We have 
          $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$ 
          and wish to show that
          $$(e, f)~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (e, f)~\sigma'$$

          By Lemma~\ref{thm:sub-decomp}, we know that there exists substitutions
          $\sigma_{\Gamma} \sim_{\alpha} \sigma'_{\Gamma} : \Gamma$
          and
          $\sigma_{\Theta} \sim_{\alpha} \sigma'_{\Theta} : \Theta$.
          So by definition unfolding we have that:
          $$
          (v~\sigma_{\Gamma}, w~\sigma_{\Theta}) 
            \sim_{(\alpha \cdot \Gamma + \alpha \cdot \Theta)} 
          (v~\sigma'_{\Gamma}, w~\sigma'_{\Theta})
          : \tau_0 \times \tau_1
          $$
          so therefore by an analysis of free variables and subsitution
          $$
          (v, w)~\sigma 
            \sim_{(\alpha \cdot \Gamma + \alpha \cdot \Theta)} 
          (v, w)~\sigma'
          : \tau_0 \times \tau_1
          $$

          To complete the proof case, it suffices to prove:
          $$\alpha \cdot (\Gamma + \Theta) \geq \alpha \cdot \Gamma + \alpha \cdot \Theta$$

          which holds by construction (via our substitution decomposition lemma).
      \end{description}

    \item[Case $\times$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. The case holds by application
          of the inductive hypothesis and stepping once.
        \item[\underline{Property 2.}]
          It suffices to show that for all $\sigma \sim_{\gamma} \sigma' : \Gamma$
          if $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma' : \tau_1 \times \tau_2$
          then:
          $$
          \pi_i~e~\sigma \sim_{\gamma \cdot \Gamma} \pi_i~e~\sigma' : \tau_i
          $$
          which holds by stepping and some unfolding of the logical relation.
      \end{description}

    \item[Case $\otimes$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          The case holds by application of the inductive hypothesis.
        \item[\underline{Property 2.}]
          It suffices to show that for all 
          $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$:
          $$
          (e, f)~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (e, f)~\sigma' :
          \tau_0 \otimes \tau_1
          $$
          By Lemma~\ref{thm:sub-decomp}, we know that 
          $\sigma \sim_{\alpha} \sigma' : \Gamma$ and
          $\sigma \sim_{\alpha} \sigma' : \Theta$.
          Appying our inductive hypothesis, we get that
          $e~\sigma \sim_{\alpha \cdot \Gamma} e~\sigma' : \tau_0$ and
          $e~\sigma \sim_{\alpha \cdot \Theta} e~\sigma' : \tau_1$
          so by unfolding our logical relation we obtain
          $$
          (e, f)~\sigma \sim_{\alpha \cdot \Gamma + \alpha \cdot \Theta} (e, f)~\sigma' :
          \tau_0 \otimes \tau_1
          $$
          which is equivalent to what we wanted to show.
      \end{description}

    \item[Case $\otimes$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          We again only need to prove one side due to symmetry. Let our bound
          expression be some value $w$. By our inductive hypothesis $w = (v_0,
          v_1)$ for some $v_0$ and $v_1$. Stepping and applying our inductive
          hypothesis yields that $f~\sigma~[v_0/x][v_1/x] \in
          \mathcal{R}(\tau)$ for any $\sigma$ compatible with $\Theta$.
        \item[\underline{Property 2.}]
          This case mirrors the proof for \textbf{\underline{Property 2}} of the
          $\textbf{let}$ case.
      \end{description}

    \item[Case $+$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          Holds by application of the inductive hypothesis.
        \item[\underline{Property 2.}]
          It suffices to show that for all $\sigma \sim_{\gamma} \sigma' : \Gamma$
          if $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma' : \tau_0 + \tau_1$
          then:
          $$
          \tin_i~e~\sigma \sim_{\gamma \cdot \Gamma} \tin_i~e~\sigma' : \tau_0 +
          \tau_1
          $$
          which holds by stepping and some unfolding of the logical relation.
      \end{description}

    \item[Case $+$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. We again only need to prove one
          side due to symmetry. Let our bound expression be some value
          $\textbf{in}_i ~ v$. Stepping and applying our inductive hypothesis
          finishes this case.
        \item[\underline{Property 2.}]
          We wish to show that for all 
          $\sigma \sim_{\alpha} \sigma': s \cdot \Gamma + \Theta$ 
          that:
          $$
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2)~\sigma
          \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2)~\sigma'
          : \tau
          $$
          By Lemma~\ref{thm:sub-decomp} we have substitutions
          $$
          \sigma_{\Gamma} \sim_{\alpha} \sigma'_{\Gamma} : \Gamma
          $$
          $$
          \sigma_{\Theta} \sim_{\alpha} \sigma'_{\Theta} : \Theta
          $$
          that we can plug into our inductive hypothesis. Plugging in, we
          know that: $e~\sigma \mapsto \mathbf{in}_i~v$ 
          and $e~\sigma' \mapsto \mathbf{in}_j~v'$ where
          $\mathcal{SD}(\mathbf{in}_i~v, \mathbf{in}_j~v') 
          \leq \gamma \cdot \Gamma$.
          We now case over the following two scenarios:
          \begin{description}
            \item[Subcase $i = j$.] This subcase mirrors the proof for
              \textbf{\underline{Property 2}} of the $\textbf{let}$ case.
            \item[Subcase $i \not= j$.] In this case, we have:
              $\mathcal{SD}(\mathbf{in}_i~v, \mathbf{in}_j~v') = \infty \leq
              \alpha \cdot \Gamma$. Stepping and applying the inductive
              hypothesis, we obtain:
              $$
              f_i [v/x]\sigma 
              \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
              f_j [v'/x]\sigma'
              : \tau
              $$
              Since $s$ is non-zero, by applying our inductive hypothesis we
              know that $f_i$ and $f_j$ are non-zero sensitive in $x$ and since
              positive $s \cdot \infty = \infty$, we know that $\alpha \cdot (s
              \cdot \Gamma + \Theta) = \infty$. Therefore this subcase holds.
              Note that it is essential in this case that $s$ is non-zero; If
              $s$ is allowed to be zero, this would be a problem because $0
              \cdot \infty = 0$.
          \end{description}

    \end{description}

  \end{description}
  The remaining cases in this proof deal with bound polymorphism:
  \begin{description}
    \item[Case \textbf{bop}.] Holds by the spec on our language interface and
      from the fact that any differring $\Delta$-substitutions must be $\infty$
      apart.

    \item[Case $\forall$ I.] We need to show that for any substitutions:

      $$\sigma \sim_{\alpha} \sigma' : \Delta \ | \ \Gamma$$

      that 

      $\Lambda \epsilon . e~\sigma \sim_{\alpha \cdot (\Delta \ | \ \Gamma)}
      \Lambda \epsilon . e~\sigma : \forall \epsilon . \tau$. Similarly to the
      above cases, we observe that if the $\Delta$-substitutions in $\sigma$ and
      $\sigma'$ are different, the distance is $\infty$ and we are done. In the
      case that the $\Delta$-substitutions are the same, we unfold our
      definition of $\mathcal{R}_{\forall \epsilon . \tau}$ and observe that
      since $\epsilon \not\in FTV(\Gamma)$, we can apply our inductive
      hypothesis.

    \item[Case $\forall$ E.] We similarly observe that if the
      $\Delta$-substitutions in $\sigma$ and $\sigma'$ are different, the
      distance is $\infty$ and we are done. Unfolding our logical relation and
      applying our inductive hypothesis finishes the case.
  \end{description}
\end{proof}
