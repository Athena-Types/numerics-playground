\section{Introduction}
It is natural for ordinary programmers such as mathematicians and scientists to
wish their computers operate over the reals. 
However, this is impossible in many contexts. Instead, we settle for a finitary
floating-point number approximation.
As language designers, we wish to convince the ordinary programmer, who are
largely not experts in numerical analysis, that the floating-point semantics we
have provided is ``close enough" to what they desire.
Towards this end, we develop automated numerical analyses to bound the error
introduced by our floating-point approximations. We wish for our analyses to be
efficient, as automatic as possible, and precise.

A major challenge in the automated numerical analysis literature is scalability.
Many existing approaches rely on global optimization \cite{fptaylor}
\cite{satire}, rewrite saturation \cite{gappa}, or SMT-based methods
\cite{rosa}. However, as programs scale, these analysis approaches frequently
time-out. Recent work has applied typed-based analysis approaches to both
forwards and backwards error analysis, such as Numerical Fuzz \cite{numfuzz} and
Bean \cite{bean}. The advantage of type-based methods is that they are
inherently compositional and scalable: all of the information necessary to
perform an error analysis on a function is contained within the type.
There is no need to perform global optimization, run an algorithm to
convergence, saturate rewrites, or bit-blast.

% max: do I need to explain round-off error first?
\paragraph{Prior type-based work and limitations.} 
In this paper, we extend Numerical Fuzz, an affine call-by-value graded type
system for bounding round-off error. The core idea of Numerical Fuzz is to track
two key properties of each function: (1) the function sensitivity $s$ and (2)
the maximum error $q$ introduced by the function. Intuitively, $s$ upper-bounds
the amount by which perturbations in a function's input affect its output.
Formally, function sensitivity is defined below.
For the remainder of this paper, $\tilde{\phi}$ corresponds to the
floating-point approximation of $\phi$.

\begin{definition}[Lipschitz function sensitivity]
  For a given metric space $(X, d_X)$ and $(Y, d_Y)$ a function $f : X \to Y$ is
  $s$-sensitive if $\forall x, \tilde{x} \in X, d_Y(f(x), f(\tilde{x})) \leq s
  \cdot d_X(x, \tilde{x})$.
\end{definition}

Function sensitivity bounds the amount by which error in a function's inputs
will become magnified in its outputs.
% If a given function $f$ with senstivity $s$ and round-off error $q$ has at most
% $r$ error in its inputs $\tilde{x}$, we can apply triangle inequality to obtain
% that $\widetilde{f(x)}$ will exhibit at most $s \cdot r + q$ round-off error. 
% We can see the approach of combining function sensitivity with round-off error
% in Figure~\ref{fig:sen}.
% \begin{figure}[ht] 
% % https://q.uiver.app/#q=WzAsNyxbMCwwLCJ4Il0sWzMsMCwiXFx0aWxkZXt4fSJdLFswLDIsImYoeCkiXSxbOCwyLCJcXHdpZGV0aWxkZXtmKHgpfSJdLFswLDNdLFs4LDNdLFs0LDEsImYoXFx0aWxkZXt4fSkiXSxbMCwxLCJcXHRleHR7KHBlcnR1cmJhdGlvbiBpbiBmdW5jdGlvbiBpbnB1dHMpfSBcXFxcIHIiLDAseyJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzAsMiwiIiwxLHsiY29sb3VyIjpbMTIwLDYwLDMwXX1dLFsxLDMsIiIsMSx7ImNvbG91ciI6WzAsNjAsNTJdfV0sWzQsNSwicyBcXGNkb3QgciArIHEgXFxcXCBcXHRleHR7KHRyaWFuZ2xlIGluZXF1YWxpdHksIGZpbmFsIGJvdW5kKX0iLDIseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJhcnJvd2hlYWQifX19XSxbNiwzLCJxIFxcXFwgXFx0ZXh0eyhlcnJvciBpbiBmKX0iLDIseyJsYWJlbF9wb3NpdGlvbiI6NzAsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMSw2LCIiLDEseyJjb2xvdXIiOlsxMjAsNjAsMzBdfV0sWzIsNiwicyBcXGNkb3QgciBcXFxcIFxcdGV4dHsoYm91bmRlZCBieSBmdW5jdGlvbiBzZW5zaXRpdml0eSB9cykiLDIseyJsYWJlbF9wb3NpdGlvbiI6NjAsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=
% \[\begin{tikzcd}
% 	x &&& {\tilde{x}} \\
% 	&&&& {f(\tilde{x})} \\
% 	{f(x)} &&&&&&&& {\widetilde{f(x)}} \\
% 	{} &&&&&&&& {}
% 	\arrow["\begin{array}{c} \text{(perturbation in function inputs)} \\ r \end{array}", no head, from=1-1, to=1-4]
% 	\arrow[color={rgb,255:red,31;green,122;blue,31}, from=1-1, to=3-1]
% 	\arrow[color={rgb,255:red,31;green,122;blue,31}, from=1-4, to=2-5]
% 	\arrow[color={rgb,255:red,206;green,59;blue,59}, from=1-4, to=3-9]
% 	\arrow["\begin{array}{c} q \\ \text{(error in f)} \end{array}"'{pos=0.7}, no head, from=2-5, to=3-9]
% 	\arrow["\begin{array}{c} s \cdot r \\ \text{(bounded by function sensitivity }s) \end{array}"'{pos=0.6}, no head, from=3-1, to=2-5]
% 	\arrow["\begin{array}{c} s \cdot r + q \\ \text{(triangle inequality, final bound)} \end{array}"', tail reversed, from=4-1, to=4-9]
% \end{tikzcd}\]
% \caption{Diagram showing the use of function senstivity and triangle inequality
%   to bound round-off error.}
%   \label{fig:sen}
% \end{figure}
However, subtraction over the reals is infinitely-sensitive, which leads
Numerical Fuzz to conservatively conclude that any use of subtraction might lead
to infinite round-off error.
The core problem is that relatively small peturbations in the
input of subtraction can lead to hard-to-bound relatively large perturbations in
the output. As an illustrative toy example, suppose we have two large numbers
like 1,000,001 and 1,000,000 that we wish to subtract. If we perturb the input
by a small amount, for example, by 1 per-cent, we can observe quite large
\textit{relative} changes in the output. 

% As an illustrative toy example, suppose we
% have two large numbers like 1,000,001 and 1,000,000 with little relative error.
% Suppose that the relative error bounds for both large numbers is ``small" at 1
% per cent. Then the relative error for $1,000,001 - 1,000,000 = 1$ is quite
% large, upper-bounded at roughly 2,000,000 per cent.

% The problem is essentially:
% compositional type-based error bounds seem to require relative notions of error;
% however, relative notions of error are not well-behaved in the presence of
% subtraction and negative numbers. 
% This fundamental phenomenon is known as \textit{catastrophic cancellation}: two
% large nearby numbers with small relative error can be subtracted to produce a
% small number with arbitrarily high relative error.

% When catestrophic cancellation occurs, it is impossible to obtain tight error
% bounds. Importantly, we often know when catestrophic cancellation cannot not
% occur (\textit{a priori}) and can detect when it has occured (\textit{a
% posteori}). \textit{A posterori} error bounds are particularly promising as
% catestrophic cancellation only occurs when two large numbers are subtracted to
% produce a \textit{small} number with arbitrarily high error. Knowing that the
% final value is large allows for the ruling out of catestrophic cancellation,
% sidestepping the problem entirely.
% In both circumstances, our approach is able to reason about the presence of
% catestrophic cancellation (or rule it out) in order to produce useful error
% bounds.

\paragraph{Our approach: Negative Fuzz} 
The techniques detailed in this paper center around representing each possibly
negative number, $r$, in our semantics with a pair of non-negative $(a, b)$
which behave akin to ghost variables.
When our semantics adds $r_0 = (a_0, b_0)$ and $r_1 = (a_1, b_1)$ we produce
$r_0 + r_1 = (a_0 + a_1, b_0 + b_1)$. When our semantics subtracts two numbers
$r_0 - r_1 = (a_0 + b_1, b_0 + a_1)$, our paired type represenation only needs
to add components; subtraction is represented implictly. Distance between $(a_0,
b_0)$ and $(a_1, b_1)$ is encoded as the maximum of the pairwise distance:
$max(d(a_0, a_1), d(b_0, b_1))$. In this manner, we encode subtraction and
negative numbers in a finite-sensitive manner. 

We wish to turn our relative bounds on the paired components $a$ and $b$, into a
useful bound on $r$ (ideal) and $\tilde{r}$ (approximate). To produce a useful
bounds, we take advantage of the invariant that $r = a - b$ and $\tilde{r} =
\tilde{a} - \tilde{b}$. We detail two approaches towards convert relative bounds
on the paired components (e.g. $d(a, \tilde{a}), d(b, \tilde{b}) \leq q$) to
relative bounds on $r$ and $\tilde{r}$ (e.g. bounds on $d(r, \tilde{r})$):
\begin{description}
  \item[1) \textit{A priori.}] If we can obtain bounds on the paired ideal
    components $a$ and $b$, we would be able to statically produce bounds on the
    distance beteen $r$ and $\tilde{r}$.
  \item[2) \textit{A posterori.}] If we are able to obtain bounds on either $a$
    or $b$ \textit{and} know the value of $\tilde{r}$ from the actual execution
    of the floating-point program, we can also obtain after-the-fact bounds on
    $r$ and $\tilde{r}$.
\end{description}
Both approaches rely on having useful bounds for the ideal components. To
accomplish this, we incorporate an bounds analysis in the type system.
A challenge of doing a bounds analysis in the type system is how to assign a
precise bound to a function that has multiple call sites. A na{\"i}ve
approach might widen the bounds for each call site, drastically reducing the
precision of the bounds analysis. We address this challenge by introducing \textit{bound polymorphism}, allowing functions to be polymorphic in
the bounds assigned to the inputs. At each call site, we specialize the
polymorphic function to the appropriate bounds. 

To reduce the user type annotation burden, we provide a type inference algorithm
that can automatically generalize and instantiate bound-polymorphic functions.
We prove the soundness of our type inference algorithm and evaluate its utility
and performance. We additionally prove that our approach provides no looser
error bounds than prior type-based approaches while covering many more programs,
including those with subtraction and negative numbers. We defer the details of
our paired approach to Section~\ref{sec:encoding} and type inference to
Section~\ref{sec:inference}. 

% TODO: maybe refer to sec 5 (factor) above too?

Negative Fuzz addresses the limitations of prior work by providing a scalable,
type-based forwards error analysis that can handle broader classes of programs
and, at the same time, more accurately structure and sequence arithmetic while
tightening error bounds. 
Our approach is faster than competing approaches, mostly automatic with low type
annotation overhead, and competitive on precision with other state-of-the-art
techniques.
Concretely, our contributions are as follows:
\begin{itemize}
  \item We extend the Numerical Fuzz family of languages by incorporating an
    bounds analysis into the type system using \textit{bound polymorphism}. We
    further extend Numerical Fuzz to enable expressions in more places. We prove
    the soundness of our extensions to Numerical Fuzz in
    (Section~\ref{sec:lang}).

  \item We instantiate Numerical Fuzz to use a \textit{paired representation}
    that allows for subtraction and negative numbers to be soundly represented
    in a finitely-sensitive manner (Section~\ref{sec:instantiation}). 
    We instantiate our bounds domain to reason about our paired represenation.
    We provide error soundness theorems to allow the type to be understood in
    terms of floating-point error on the standard, unpaired program
    representation.
    % TODO: how do we show that this is non-trivial without talking too much
    % about type inference and bound polymorphism. does this do the job? feels a
    % bit understated here

    % TODO: some claim about novelty is needed to put this contribution in context
    % to develop the first type-based approach to forwards error analysis that
    % enables
    % Our instantiation relies on a \textit{paired numeric representation}. 
    % To our knowledge, our approach is the first automatic general-purpose
    % numerical analysis that can produce usable \textit{a posteori} error bounds.
    
    % We prove that error types can be used to provide both \textit{a priori} and
    % \textit{a posterori} compositional error bounds in the presence of
    % subtraction and negative numbers (Section~\ref{sec:application}). 

  \item We improve the expressiveness of Numerical Fuzz by allowing allowing
    error grades and sensitivities in rounded terms to be shared. This enables
    our error analysis to distinguish between reassociated floating-point
    computation with different error bounds, for example, the following two
    addition computations: \lstinline{((a + b) + c) + d = (a + b) + (c + d)}. We
    accomplish this through the addition of a previously-untypable primitive
    (Section~\ref{sec:structure}).

  % \item We enable the precise structuring and seqeuncing of arithmetic by
  %   allowing error and sensitivities in rounded terms to be shared. We
  %   accomplish this through the addition of a previously-untypable primitive
  %   inspired by the resource interpretation of linear logic
  %   (Section~\ref{sec:structure}). 

  \item We develop a type inference algorithm for our type system that can
    automatically infer bound polymorphism in many useful programs; the user can
    provide an program without bound annotations for inference. We prove the
    soundness of our type inference algorithm. 
    % We further prove that our approach leads to forwards error bounds no looser
    % than Numerical Fuzz for all programs that can be typed by Numerical Fuzz
    % (Section~\ref{sec:tightness}). 

  \item We implement our type-based approach for forwards error analysis. We
    evaluate our implementation against FPTaylor and Gappa and Satire on a suite
    of benchmarks translated into our core language and demonstrate that we
    obtain useful and competitive error bounds with faster and more scalable
    performance (Section~\ref{sec:eval}).
    % TODO: put in hard numbers
\end{itemize}
