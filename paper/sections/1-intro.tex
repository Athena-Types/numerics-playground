\section{Introduction}
It is natural for users such as mathematicians and scientists to desire
programs that operate over the reals. 
As language designers, we wish to show that the floating-point semantics we have
provided is ``close enough".
Towards this end, we develop automated numerical analyses to bound the error
introduced by our floating-point approximations. We wish for our analyses to be
efficient, as automatic as possible, and precise.

A major challenge in the automated numerical analysis literature is scalability.
Many existing approaches rely on global optimization \cite{fptaylor}
\cite{satire}, rewrite saturation \cite{gappa}, or SMT-based methods
\cite{rosa}. However, as programs scale, these analysis approaches frequently
time-out. Type-based approaches offer an alternative approach to automated
numerical analysis.
With type checking, there is no need to perform global optimization, run an
algorithm to convergence, saturate rewrites, or bit-blast.

In this paper, we extend Numerical Fuzz, an affine call-by-value graded type
system for bounding round-off error.
The advantage of a type-based method is that it is inherently compositional and
scalable: all of the information necessary to perform an error analysis on a
function is contained within the type. 
The core idea of Numerical Fuzz is to track two key properties of each function:
how a function magnifies error in its inputs (function sensitivity), and the
maximum error introduced by the function. This allows Numerical Fuzz to
compositionally bound the maximum accumulated error in a program via a graded
monad.

% \paragraph{Prior type-based work and limitations.} 
% For the remainder of this paper, $\tilde{\phi}$ corresponds to the
% floating-point approximation of $\phi$.
%
% \begin{definition}[Lipschitz function sensitivity]
%   For a given metric space $(X, d_X)$ and $(Y, d_Y)$ a function $f : X \to Y$ is
%   $s$-sensitive if $\forall x, \tilde{x} \in X, d_Y(f(x), f(\tilde{x})) \leq s
%   \cdot d_X(x, \tilde{x})$.
% \end{definition}

% Function sensitivity bounds the amount by which error in a function's inputs
% will become magnified in its outputs.
% If a given function $f$ with senstivity $s$ and round-off error $q$ has at most
% $r$ error in its inputs $\tilde{x}$, we can apply triangle inequality to obtain
% that $\widetilde{f(x)}$ will exhibit at most $s \cdot r + q$ round-off error. 
% We can see the approach of combining function sensitivity with round-off error
% in Figure~\ref{fig:sen}.
% \begin{figure}[ht] 
% % https://q.uiver.app/#q=WzAsNyxbMCwwLCJ4Il0sWzMsMCwiXFx0aWxkZXt4fSJdLFswLDIsImYoeCkiXSxbOCwyLCJcXHdpZGV0aWxkZXtmKHgpfSJdLFswLDNdLFs4LDNdLFs0LDEsImYoXFx0aWxkZXt4fSkiXSxbMCwxLCJcXHRleHR7KHBlcnR1cmJhdGlvbiBpbiBmdW5jdGlvbiBpbnB1dHMpfSBcXFxcIHIiLDAseyJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzAsMiwiIiwxLHsiY29sb3VyIjpbMTIwLDYwLDMwXX1dLFsxLDMsIiIsMSx7ImNvbG91ciI6WzAsNjAsNTJdfV0sWzQsNSwicyBcXGNkb3QgciArIHEgXFxcXCBcXHRleHR7KHRyaWFuZ2xlIGluZXF1YWxpdHksIGZpbmFsIGJvdW5kKX0iLDIseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJhcnJvd2hlYWQifX19XSxbNiwzLCJxIFxcXFwgXFx0ZXh0eyhlcnJvciBpbiBmKX0iLDIseyJsYWJlbF9wb3NpdGlvbiI6NzAsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMSw2LCIiLDEseyJjb2xvdXIiOlsxMjAsNjAsMzBdfV0sWzIsNiwicyBcXGNkb3QgciBcXFxcIFxcdGV4dHsoYm91bmRlZCBieSBmdW5jdGlvbiBzZW5zaXRpdml0eSB9cykiLDIseyJsYWJlbF9wb3NpdGlvbiI6NjAsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=
% \[\begin{tikzcd}
% 	x &&& {\tilde{x}} \\
% 	&&&& {f(\tilde{x})} \\
% 	{f(x)} &&&&&&&& {\widetilde{f(x)}} \\
% 	{} &&&&&&&& {}
% 	\arrow["\begin{array}{c} \text{(perturbation in function inputs)} \\ r \end{array}", no head, from=1-1, to=1-4]
% 	\arrow[color={rgb,255:red,31;green,122;blue,31}, from=1-1, to=3-1]
% 	\arrow[color={rgb,255:red,31;green,122;blue,31}, from=1-4, to=2-5]
% 	\arrow[color={rgb,255:red,206;green,59;blue,59}, from=1-4, to=3-9]
% 	\arrow["\begin{array}{c} q \\ \text{(error in f)} \end{array}"'{pos=0.7}, no head, from=2-5, to=3-9]
% 	\arrow["\begin{array}{c} s \cdot r \\ \text{(bounded by function sensitivity }s) \end{array}"'{pos=0.6}, no head, from=3-1, to=2-5]
% 	\arrow["\begin{array}{c} s \cdot r + q \\ \text{(triangle inequality, final bound)} \end{array}"', tail reversed, from=4-1, to=4-9]
% \end{tikzcd}\]
% \caption{Diagram showing the use of function senstivity and triangle inequality
%   to bound round-off error.}
%   \label{fig:sen}
% \end{figure}
There are two limitations of Numerical Fuzz that our work addresses:
\begin{enumerate}
  \item \textbf{Lack of support for negative numbers or subtraction:} Subtraction over the
   reals is infinitely-sensitive, which leads Numerical Fuzz to conservatively
   conclude that any use of subtraction might lead to infinite round-off error.
   Since many programs use subtraction or have negative numbers, this limitation
   severely restricts the practical utility of Numerical Fuzz in modeling
   real-world programs.
 \item \textbf{Conservative treatment of addition and subtraction:} For
   addition, floating-point error of addition is known to grow in the
   \textit{height} of a computation tree. In Numerical Fuzz, a sequence of
   additions grows in the \textit{number of nodes} in the corresponding
   computation tree. As many programs use addition or subtraction, this limits
   the precision of the Numerical Fuzz type system.
   % maybe tie into claim that Numerical Fuzz is blind to reassociating exprs
   % (when it really shouldn't be)
\end{enumerate}

\subsection*{Our approach: Negative Fuzz} 
% Negative Fuzz addresses the limitations of prior work by providing a scalable,
% type-based error analysis that can handle broader classes of programs and, at
% the same time, more accurately treat addition and subtraction with tighter error
% bounds. 
To support subtraction, we first introduce a \textit{paired
representation} that simulates the floating-point error of the standard,
unpaired program representation.
We demonstrate how to model subtraction as a finitely-sensitive operation.
Then, we incorporate an interval-style analysis in the type system to translate
the simulated error on the paired representation into floating-point error on
the standard representation.
We introduce a new primitive that can more precisely capture the
propagation of floating-point error under addition and subtraction.
Supporting subtraction and improving the precision of addition are orthogonal
contributions, so we describe them separately:
% TODO: signpost this approach:
% - sensitivity concern
%   - but wait, we broke it! (how to translate bounds?)
% - dealing with conservative treatment of addition and subtraction

\subsubsection*{Supporting subtraction and negative numbers.} 
% TODO: polish and org this entire para
To support subtraction and negative numbers, we represent each possibly negative
real $r$ in our semantics with a pair of non-negative $(a, b)$ such that $r = a
- b$.
% Then, we can translate the error over
% $a, b$ to be error over $r$.
We can model our operations, including subtraction, in this representation in a
finitely-sensitive manner.
To reason about the unpaired and paired representations side-by-side, we use a
triple $(r, a, b)$ where $r \in \mathbb{R}$ and $a, b \in \mathbb{R}^{+}$ and $r
= a - b$. For our paired approach to be useful in bounding the round-off
error on the unpaired $r$ representing real-world floating-point programs, we
need to be able to (1) ``simulate" floating-point round-off error on $r$ using
the paired representation, (2) measure error on the paired representation, and (3)
``translate" error bounds on paired representation to be back on the unpaired $r$
representing the real-world program.

To aid in the usability precision of our bounds analysis, we support analyzing
functions via \textit{bound polymorphism}, which allows functions to be
polymorphic in the bounds assigned to the inputs. To reduce the user type
annotation burden, we provide a type inference algorithm that can automatically
generalize and instantiate bound-polymorphic functions. We prove the soundness
of our type inference algorithm and evaluate its utility and performance. 

% We additionally show that our approach provides no looser error bounds than
% prior type-based approaches while covering many more programs, including those
% with subtraction and negative numbers. We defer the details of our paired
% approach to Section~\ref{sec:encoding} and type inference to
% Section~\ref{sec:inference}. 

\subsubsection*{More precise analysis of addition and subtraction.}
% TODO: fill in factor stuff, intuition and forwards ref
Floating-point addition and subtraction are not associative. In fact, different
associations of the same computation can result in drastically different error
terms. Unfortunately, Numerical Fuzz is not capable of distinguishing between
different associations. We address this limitation by introducing a primitive
which acts like an annotation. We prove that the primitive is type-sound and
show how to use the primitive in Section~\ref{sec:structure}.
% TODO: provide op sem interpretation (reassoc'ing error terms)?

\subsubsection*{Contributions summary.}
Concretely, our contributions are as follows:
\begin{itemize}
  \item We extend the Numerical Fuzz family of languages by incorporating an
    bounds analysis into the type system using \textit{bound polymorphism}. To
    make programs easier and more concise to write, we further extend Numerical
    Fuzz to enable expressions in more places. We prove the soundness of our
    extensions to Numerical Fuzz in (Section~\ref{sec:lang}).

  \item We instantiate Numerical Fuzz to use a \textit{paired representation}
    that allows for modeling subtraction and negative numbers in a
    finitely-sensitive manner (Section~\ref{sec:instantiation}). 
    We provide a polymorphic bounds analysis in the type system to reason about
    our paired representation.
    We provide error soundness theorems to allow the error grade and bounds
    analysis from program type to be translated into a floating-point error
    bound on the standard, unpaired program representation.
    % TODO: how do we show that this is non-trivial without talking too much
    % about type inference and bound polymorphism. does this do the job? feels a
    % bit understated here

    % TODO: some claim about novelty is needed to put this contribution in context
    % to develop the first type-based approach to forwards error analysis that
    % enables
    % Our instantiation relies on a \textit{paired numeric representation}. 
    % To our knowledge, our approach is the first automatic general-purpose
    % numerical analysis that can produce usable \textit{a posteori} error bounds.
    
    % We prove that error types can be used to provide both \textit{a priori} and
    % \textit{a posterori} compositional error bounds in the presence of
    % subtraction and negative numbers (Section~\ref{sec:application}). 

  \item We improve the precision of Numerical Fuzz by allowing the analysis to
    more efficiently reason about addition and subtraction. This lets us achieve
    a log-factor improvement on the error bounds of a program representing
    pairwise summation. We accomplish this by introducing a
    previously-untypable primitive that can more accurately capture different
    techniques for sequencing addition and subtraction operations
    (Section~\ref{sec:structure}).

  % \item We enable the precise structuring and seqeuncing of arithmetic by
  %   allowing error and sensitivities in rounded terms to be shared. We
  %   accomplish this through the addition of a previously-untypable primitive
  %   inspired by the resource interpretation of linear logic
  %   (Section~\ref{sec:structure}). 

  \item We develop a type inference algorithm for our type system that can
    automatically infer bound polymorphism in many useful programs; the user can
    provide an program without bound annotations for inference. We prove the
    soundness of our type inference algorithm. 
    We further show that our approach leads to forwards error bounds no looser
    than Numerical Fuzz for programs that can be typed by Numerical Fuzz
    (Section~\ref{sec:tightness}). 

  \item We implement our type-based approach for error analysis. 
    Our approach is faster than competing approaches, mostly automatic with low
    type annotation overhead, and competitive on precision with other
    state-of-the-art techniques.
    When compared against competing approaches (FPTaylor, Gappa and Satire) on a
    suite of benchmarks translated into our core language, we obtain useful and
    competitive error bounds with faster and more scalable performance
    (Section~\ref{sec:eval}).
    % TODO: put in hard numbers
\end{itemize}


