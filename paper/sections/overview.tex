\section{Technical Overview}
In this paper, we are primarily motivated to extend Numerical Fuzz to support
more floating-point operations and more rich ways of structuring and sequencing
arithmetic. We provide some background and briefly overview our extensions
below.

% In this section, we briefly introduce floating-point round-off error and outline
% the main technical features of our system. 
\subsection{Background: floating-point and error metrics}
The floating-point number system is a family of formats following the following
form:
\begin{equation}
  x = (-1)^s \cdot m \cdot \beta^{e - p + 1}
\end{equation}
where $s$ controls the sign, $m$ is the mantissa, $e$ is the exponent, and $p$
is the precision. A specific floating-point number system represents a subset of
the reals and generally fix a particular precision $p$, for example
$\textsc{binary32}$ and $\textsc{binary64}$ which fix $p = 32$ and $p = 64$
respectively. 
Operations over the reals can be represented in a floating-point system as the
exact operation followed by rounding by a function $\rho$ to a float. The error
introduced by the rounding function $\rho$ is known as \textit{round-off error}.

For this paper, we assume the absence of overflow and underflow. Then, the
standard rounding error model assumes $\rho$ will observe round-off error in at
most the last bit:
\begin{equation}
  \rho(x) = x (1 + \delta) \quad \quad |\delta| \leq u
\end{equation}
where $u$ is the \textit{unit round-off} value representing the maximum error
possible in the last bit for the floating point format (e.g. $2^{-24}$ and
$2^{-53}$ for 32 and 64-bit floating-point numbers respectively).

We are interested in bounding round-off error. 
To bound error, one must first be capable of measuring it. Following Numerical
Fuzz, we rely on Olver's \cite{olver} \textit{relative precision} metric for
measuring round-off error. 

\begin{equation}
  d_{\mathbb{R}}(r, \tilde{r}) = | ln(\frac{r}{\tilde{r}})|
\end{equation}

Relative precision forms a true metric and allows for compositional error
reasoning. 
In our case, we crucially rely on triangle inequality for the soundness of the
error grades for our charateristic monadic bind rule.
Importantly, triangle inequality does not hold for \textit{relative error}.

\subsection{Language Feature: Supporting Subtraction and Negative Numbers}
Many real-world programs use subtraction or negative numbers. Unfortuantely,
subtraction over the reals is infinitely sensitive. This poses a significant
challenge for reasoning about subtraction in prior type-based work. For this
langauge feature, consider the minimum program containing subtraction as a
running example: 
$\lambda x \ . \ \textbf{let} \ z \ = \ \textbf{sub} \ x \ \textbf{in} \ \textbf{rnd} \ z$ 
under the restriction that $x \in [1, 100]$ and $y \in [1000, 10000]$. 
We will then try to assign a type and demonstrate how to obtain a useful error
bound from the program type.

\paragraph{Paired representation.}
Our type system relies on bounding function senstivity. 
As subtraction over the reals is infinitely-sensitive, we first deal with the
problem of infinite sensitivity.
We develop a paired representation where subtraction and negative
numbers have finitely-bounded sensitivty. The trick is to semantically associate
for each real $r \in \mathbb{R}$ a triple $(r, a, b) \in \mathbb{P} = \mathbb{R}
\times \mathbb{R}^+ \times \mathbb{R}^+ = \mathit{num}$ such that $r = a - b$.
The paired components $a$ and $b$ are only used for error analysis and serve a
function similar to ghost variables in other program analyses.
Our error function $d_{\mathbb{P}}((r, a, b)(\tilde{r}, \tilde{a}, \tilde{b}))$
over $\mathbb{P}$ is measured as the 
$max(d_{\mathbb{R}}(a, \tilde{a}), d_{\mathbb{R}}(b, \tilde{b}))$.

Under the paired representation, we can implement and faitfully type addition,
subtraction, and multiplication in terms of only-growing and finitely-sensitive
operations over the paired components:
\begin{equation}
  \begin{aligned}[c]
    sub((r, a, b), (\tilde{r}, \tilde{a}, \tilde{b})) &\mapsto (r - \tilde{r}, a + \tilde{b}, \tilde{a} + b) \\
    add((r, a, b), (\tilde{r}, \tilde{a}, \tilde{b})) &\mapsto (r + \tilde{r}, a + \tilde{a}, b + \tilde{b}) \\
    mul((r, a, b), (\tilde{r}, \tilde{a}, \tilde{b})) 
      &\mapsto 
    (r * \tilde{r}, a * \tilde{a} + b * \tilde{b}, a * \tilde{b} + \tilde{a} * b)
  \end{aligned}
\end{equation}
In this manner, we can bound the sensitivity of each operation and soundly
assign a type. 
$\textbf{add}$ and $\textbf{mul}$ 
have the same type as in Numerical Fuzz. Suprisingly, subtraction has type as
addition: $\textbf{sub} : \textbf{num} \times \textbf{num} \multimap
\textbf{num}$. 
Returning to our running example, we can assign the type $\textbf{num} \times
\textbf{num} \multimap M_u \textbf{num}$ to the program 
$\lambda x . \ \textbf{let} \ z \ = \ \textbf{sub} \ x \ \textbf{in} \ \textbf{rnd} \ z$
where $u$ is the unit round-off constant.

\paragraph{Bound polymorphism.}
Unfortuantely, the error bounds obtained through the paired representation only
bound the maximum error on the \textit{paired} components. 
To be useful, we desire bounds on the actual program, e.g. $r$ the
\textit{unpaired} component. We can obtain bounds on $r$ by extending the type
system to be able to perform an interval-style analysis on the paired
components. In our running example, we have that $x \in [1, 100]$ and $y \in
[1000, 10000]$. For all inputs $x, y$ to
our input program, for an output value $(r, a, b), (\tilde{r}, \tilde{a},
\tilde{b})$, we can deduce from our type that $max(d(a, \tilde{a}) d(b, \tilde{b})) \leq u$.
Armed with more information, we can apply the results of the
interval analysis to the invariant that for a triple $(r, a, b)$ that $r = a -
b$.
Accordingly, we obtain the following \textit{a priori} static bound from
Theorem~\ref{thm:paired-a-priori}:
$d(r, \tilde{r}) 
= max(|ln(e^u + \frac{100}{900}(e^{-u}-e^u))|, |ln(e^-u + \frac{100}{900}(e^u-e^{-u}))|) 
= 4.635916 \times 10^{-8}$. 
% https://www.wolframalpha.com/input?i2d=true&i=max%5C%2840%29%7Cln%5C%2840%29Power%5Be%2C-%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D+%2B+Divide%5B10%2C900%5D%5C%2840%29Power%5Be%2C%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D-Power%5Be%2C-%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D%5C%2841%29%5C%2841%29%7C%5C%2844%29%7Cln%5C%2840%29Power%5Be%2C%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D+%2B+Divide%5B10%2C900%5D%5C%2840%29Power%5Be%2C-%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D-Power%5Be%2C%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D%5C%2841%29%5C%2841%29%7C%5C%2841%29
Another piece of information that tightens our error analysis is knowing how big
our output value is. For example, if we knew that $\tilde{r} = 1000$, then we
can apply Theorem~\ref{thm:paired-a-posteriori} and obtain the following tighter
\textit{a posterori} bound: TODO.

We now consider how to compositionally introduce a precise interval bounds
analysis to the type system. A naive approach to incorporating a bounds analysis
in the type system would be to annotate each $\textbf{num}_{\bnd{i}}$ with a
subscript interval $\bnd{i}$. However, many programs we wish to type call the
same function many times. To ensure that our interval analysis is scalable and
that functions types can be reused, we support \textit{bound polymorphism},
which allows us to specialize our function to have different concrete bounds
$\bnd{i}$ for each function call site. We write types $\tau$ polymorphic in
interval variable $\bnd{\epsilon}$ as $\bnd{\forall \epsilon.} \tau$.

For example, if we have a function such as the identity function over numbers
$id : \textbf{num}_{\bnd{i}} \multimap \textbf{num}_{\bnd{i}}$ called at two
different call sites with intervals $j$ and $k$, the inferred bound for $i$
would be $j \cup k$.
By adding \textit{bound polymorphism}, we allow functions to be
typed like so: $id : \bnd{\forall \epsilon.} ~ \textbf{num}_{\bnd{\epsilon}}
\multimap \textbf{num}_{\bnd{\epsilon}}$. We can introduce and eliminate
polymorphism via $\forall$ introduction and elimination rules.
% provided in Figure~\ref{fig:typing_rules}.

To reduce annotation overhead, we provide a type inference algorithm that can
automatically infer bound polymorphism abstraction and instantiation sites. We
prove our type inference algorithm sound in Section~\ref{sec:inference}. We also
prove that our low-annotation approach gives us bounds no looser than prior type
based work while also supporting subtraction and negative numbers.

\subsection{Language Feature: Precise Structuring and Sequencing of Arithmetic} 
\label{sec:factor}
Suppose, by way of example, that we wish to compute the sum
$w~\tilde{+}~x~\tilde{+}~y~\tilde{+}~z$, where $\tilde{+}$ represents addition
with round-off error. It is well known that round-off error grows linearly in
the height of a summation tree. So, a user might naturally want to sequence the
summation as a perfect binary tree: $(w~\tilde{+}~x)~\tilde{+}~(y~\tilde{+}~z)$.
In prior type-based work \cite{numfuzz}, the monadic computation sequencing rule
forces the error analysis to always compute the worst-case round-off error
associated with the pathological degenerate tree:
$((w~\tilde{+}~x)~\tilde{+}~y)~\tilde{+}~z$.
In other words, the error bound obtained by \cite{numfuzz} grows linearly in
size of the number of \textit{nodes}.

Our extension enables the user to associate the computation tree arbitrarily,
and to obtain tighter error bounds that grow linearly in size of the \textit{height} of
the tree.
We accomplish this by including a special primitive $\textbf{factor}$ in our
term language inspired by linear logic. Stucturing the example arithmetic into a
program below, we can see that using factor results in a 50\% smaller error
bound for a program representing the same association and order of operations.

\begin{figure}[ht] \label{fig:factor-cmp-overview}
\centering

\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
// (w + x) + (y + z) : M[3*u] num 
let-bind a = addfp <w, x> in
let-bind b = addfp <y, z> in
let-bind c = addfp <a, b> in
addfp c 
\end{lstlisting}
\caption{Without factor: error bound is $3u$.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
// (w + x) + (y + z) : M[2*u] num 
let-bind a = 
  factor <addfp <w, x>, addfp <y, z>> in
  addfp a
\end{lstlisting}
\caption{With factor: error bound is $2u$.}
\end{subfigure}

\caption{Side-by-side comparison with and without the \textbf{factor} primitive.}
\label{fig:factor-side-by-side}
\end{figure}

We defer a detailed explaination of \textbf{factor} and how it allows for
structuring and sequencing arithmetic towards a more precise error analysis in
Section~\ref{sec:structure}.
