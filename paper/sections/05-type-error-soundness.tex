\section{Type Soundness}
Following Fuzz, we prove the following syntanctic properties relating our type
systema and operational semantics.

\begin{lemma}[Weakening]\label{thm:weakening}
  If $\Gamma \vdash e : \tau$, then $\Gamma + \Sigma \vdash e : \tau$.
\end{lemma}
\begin{proof}
  We induct over our typing devivation. 
  For each case, we can ignore arbitrary variables in our context (the type system is
  affine). 
  In particular, for the \textbf{!E}, \textbf{$\otimes$ E}, \textbf{+ E},
  \textbf{$M_u$~E}, and \textbf{$\otimes$ I} rules, the enviroment must be split
  into a $\Gamma$ (which can in some cases be scaled) and $\Delta$ (which does
  not ever get scaled); for each of these cases we choose to push unused
  variables into $\Delta$. 
\end{proof}

Following Fuzz, we prove a $r$-sensitive substitution lemma.
\begin{lemma}[$r$-sensitive subsitution]\label{thm:substitution}
  Let $\Gamma \vdash e : \tau$ and $\Theta, x :_r \tau \vdash e' : \tau'$, then 
  $r \cdot \Gamma + \Theta \vdash e'[e/x] : \tau'$.
\end{lemma}
\begin{proof}
  Follows by induction over the height of our typing derivation and applying the
  inductive hypothesis to each applicable premise.
\end{proof}

The following notation mirrors the look of the Fuzz metric perservation theorem
statement but contains differences in the construction. In particular, logical our
relation is neither coinductive nor step-indexed. It is also unary, using mutual
(well-founded) recursion between the definition of a syntactic term falling
within the logical relation and the definition of the syntactic distance between
terms.

For any two closed expressions $e_0, e_1$ falling in same type relation $e_0,
e_1 \in R_\tau$, we can write $e_0 \sim_r e_1$ where $\mathcal{SD}_{\tau}(e_0,
e_1) \leq r$. Similarly, we can write lists of expressions (e.g. for
subsitutions) $\sigma_0, \sigma_1$ for a given typing context $\Gamma$ such
that: $\sigma_0 \sim_{\gamma} \sigma_1 : \Gamma$ for a \textit{distance vector}
$\gamma$
where
$\sigma_0 = e^0_0,~e^0_1,~\ldots e^0_n$ 
and $\sigma_1 = e^1_0,~e^1_1,~\ldots e^1_n$ 
and $\gamma = r_0, r_1, \ldots r_n$ such that:
$$e^0_0 \sim_{r_0} e^1_0 :
\tau_0,~e^0_0 \sim_{r_0} e^1_0 : \tau_0,~\ldots e^0_n \sim_{r_1} e^1_n :
\tau_n$$

Our definition for the dot product of a distance vector is the same as Fuzz.
We also write, for a distance vector $\gamma$ and variable $x$, $v(x)$ for the
lookup of the distance of variable $x$ in $v$. If the variable $x$ is not in the
domain, $v(x) = 0$ by default.
% todo: put it in here

% Max: Theorem statement is not interesting when stepping is only over closed
% terms.
% \begin{lemma}[Subsitution is invariant under stepping]
%   \label{thm:sub-stepping}
%   For any substitution $\sigma$ and program $e$ such that $e~\sigma \mapsto
%   e_1$, if $e \mapsto e_2$ then $e_2~\sigma \mapsto^{*} v \iff e_1 \mapsto^{*} v$.
% \end{lemma}
% \begin{proof}
%   TODO
% \end{proof}

Note that while we use variable names in this paper, this is only for notational
convenience. We assume that the underlying proof uses de Bruijin indicies or
similar. Then, the below lemma is useful towards proving metric preservation:
\begin{lemma}[Metric preservation under context evaluation stepping]
  \label{thm:ctx-stepping}
  For a well-typed $\Gamma \vdash C[e] : \tau$ and substitutions $\sigma,
  \sigma'$ such that $\sigma \sim_{\gamma} \sigma' : \Gamma$ and
  $e \mapsto^{*} v$.
  If $C[v] \sigma \sim_{\gamma \cdot \Gamma} C[v] \sigma' : \tau$ then
  $C[e] \sigma \sim_{\gamma \cdot \Gamma} C [e] \sigma' : \tau$.
\end{lemma}
\begin{proof}
  By inspection of our stepping relation, we can tell that stepping is
  deterministic. Further, since we only define our rewrite relation over closed
  terms, we know that $e$ is closed and therefore constant under all
  substitutions. 
  So, by the definition of $\mathcal{R}$, $\mathcal{SD}$, 
  context stepping, we know that both 
  $$C[e] \sigma, C[e] \sigma' \in \mathcal{R}_{\tau}$$ 
  and that 
  $$\mathcal{SD}_{\tau}(C[e]\sigma, C[e]\sigma') = \mathcal{SD}_{\tau}(C[e']\sigma, C[e']\sigma') = \gamma \cdot \Gamma$$
  respectively and therefore $C[e] \sigma \sim_{\gamma \cdot \Gamma} C[e]
  \sigma'$ holds.
\end{proof}

\begin{lemma}[Substitution decomposition]
  \label{thm:sub-decomp}
  For substitutions $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$, there
  exists decompositions $\sigma_{\gamma} \sim_{\gamma} \sigma'_{\gamma} :
  \Gamma$ and $\sigma_{\theta} \sim_{\theta} \sigma'_{\theta} : \Theta$ where
  $\gamma$ and $\theta$ are minimal, or equivalently, $\alpha(i \in DOM(\Gamma
  + \Theta)) = \gamma(i) + \theta(i)$.
\end{lemma}
\begin{proof}
  We induct over the distance vector $\alpha$ which also happens to fix a
  variable ordering of $\Gamma + \Sigma$. Base case is trivial and the inductive
  case follows by unfolding.
\end{proof}

% TODO: Say something about how one can assume that things are nice values.
% \begin{corollary}
% \end{corollary}

\begin{theorem}[Metric preservation]
  For any $\Gamma \vdash e : \tau$ and substitutions $\sigma, \sigma'$ such that
  $\sigma \sim_{\gamma} \sigma' : \Gamma$, then 
  $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma'$.
\end{theorem}
\begin{proof}
  We induct over our typing derivation. The base cases (Var, Unit, Const) follow
  trivially. The subsumption case also follows trivially. We detail the
  remaining cases here:
  \begin{description}
    \item[Case $\multimap$ I.] 
      We wish to show that for any $\Gamma \vdash \lambda x . e : \tau$ and
      subsitutions $\sigma \sim_{\gamma} \sigma' : \Gamma$ that $\lambda x .
      e~\sigma \sim_{\gamma \cdot \Gamma} \lambda x . e~\sigma'$. Unfolding, it
      suffices to show that both:
      \begin{enumerate}
        \item $\lambda x . e~\sigma, \lambda x . e~\sigma'$ are in
          $\mathcal{R}_{\tau_0 \multimap \tau}$
        \item $\mathcal{SD}_{\tau_0 \multimap \tau}(\lambda x . e~\sigma,
          \lambda x . e~\sigma') \leq \gamma \cdot \Gamma$
      \end{enumerate}
    
      Observe that we have by our inductive hypothesis that for any $\Delta, x:
      \tau' \vdash e : \tau$ and subsitutions 
      $\delta[v_0/x] \sim_{\gamma'} \delta'[v_1/x] : \Gamma,~x : \tau'$ that 
      $e~\delta[v_0/x] \sim_{\gamma' \cdot \Gamma} e~\delta'[v_1/x]$ holds. 
      Let us now carry on with the proof:

      \begin{description}
        \item[\underline{Property 1.}] We need to show that $\sigma$ and
          $\sigma'$ are both in the relation $\mathcal{R}_{\tau_0 \multimap
          \tau}$. The cases are symmetric so we only show one case. Unfolding
          the definition of $\mathcal{R}_{\tau_0 \multimap \tau}$, let $w_0, w_1
          \in \mathcal{VR}_{\tau_0}$. 
          $(\lambda x . e) w_0 \mapsto e[w_0/x]$
          and
          $(\lambda x . e) w_1 \mapsto e[w_1/x]$
          and so it suffices to show that
          $e~\sigma[w_0/x], e~\sigma[w_1/x]$ 
          are closed expressions falling in $R_{\tau_0}$.
          This is true by our inductive hypothesis, instantiating with $\sigma =
          \delta = \delta'$, $v_0 = w_0$, and $v_1 = w_1$.
        \item[\underline{Property 2.}] Unfolding the definition of
          $\mathcal{SD}_{\tau_0 \multimap \tau}$, it suffices to show that:
          $$
          \mathcal{SD}_{\tau}
          ((\lambda x . e~w)~\sigma, (\lambda x . e~w)~\sigma') 
          \leq \gamma \cdot \Gamma
          $$
          Stepping, it suffices to show that
          $$
          \mathcal{SD}_{\tau}
          (e~\sigma[w/x], e~\sigma'[w/x]) 
          \leq \gamma \cdot \Gamma
          $$
          which holds by application of our inductive hypothesis when $\delta =
          \sigma$, $\delta' = \sigma'$, $v_0 = v_1 = w$, and $\gamma' = \gamma
          :: 0$.
      \end{description}
      So, by our inductive hypothesis and unfolding the definitions of
      $\mathcal{R}$ and $\mathcal{SD}$, we have properties (1) and (2)
      respectively. 
    \item[Case $\multimap$ E.] 
      By Lemma \ref{thm:ctx-stepping} and our inductive hypothesis, it suffices
      to prove this case for $v~w$.
      By inversion, we know that $v = \lambda x . e$.
      We wish to show that for any $\Gamma + \Theta \vdash (\lambda x . e) w : \tau$ and
      subsitutions 
      $\sigma \sim_{\alpha} \sigma': \Gamma + \Theta$ 
      that 
      $$(\lambda x . e)~w~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (\lambda x . e)~w~\sigma'$$
      Using Lemma \ref{thm:sub-decomp} (substitution decomposition), we
      construct potentially overlapping substitutions 
      $\sigma_0 \sim_{\gamma} \sigma_1 : \Gamma$ 
      and $\sigma_0' \sim_{\theta} \sigma_1' : \Theta$
      where $FV(\lambda x . e) \subseteq DOM(\sigma_0) = DOM(\sigma_1)$ and  
      $FV(w) \subseteq DOM(\sigma_0') = DOM(\sigma_1')$ and 
      $\sigma_0, \sigma_0' \subseteq \sigma$ and 
      $\sigma_1, \sigma_1' \subseteq \sigma'$ and $\gamma, \theta$ minimal.
      In other words, $\sigma_0, \sigma_1$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Gamma$.
      Similarly, $\sigma_0', \sigma_1'$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Theta$.
      Note that our substitutions \textit{must} overlap in the case where $FV(\lambda x . e)
      \cap FV(w)$ is non-empty.
      Since subsituting variables that are not free does not change the
      underlying term, we know that the following equations must hold where
      $\sigma^*, \sigma'^*$ are $\sigma$ and $\sigma'$ respectively with $x$
      removed:
      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . e)~[\sigma_0]~(w[\sigma_1]) &= 
            ((\lambda x . e[\sigma^*])~w)[\sigma] = 
            ((\lambda x . e)~w)[\sigma] = 
            e[\sigma][w/x] \\
          (\lambda x . e)~[\sigma_0']~(w[\sigma_1']) &= 
            ((\lambda x . e[\sigma'^*])~w)[\sigma'] = 
            ((\lambda x . e)~w)[\sigma'] = 
            e[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      and by stepping

      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . e)~[\sigma_0]~(w[\sigma_1]) &= e[\sigma_0][w[\sigma_1]/x] \\
          (\lambda x . e)~[\sigma_0']~(w[\sigma_1']) &= e[\sigma_0'][w[\sigma_1']/x] \\
        \end{aligned}
      \end{equation}

      so by transitivity
      \begin{equation}
        \begin{aligned}[c]
          e[\sigma_0][w[\sigma_1]/x] &=
            e[\sigma][w/x] \\
          e[\sigma_0'][w[\sigma_1']/x] &=
            e[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      By applying our inductive hypothesis and unfolding our definition of
      $R_{\tau_0 \multimap \tau}$, we know that $e$ is 
      1-sensitive with respect to $w$ according to $\mathcal{SD}$. 
      Then, we know that by application of triangle inequality of the syntactic
      distance between terms:
      $$v~w~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta} v~w~\sigma'$$

      To complete the proof case, it suffices to prove 
      $$\alpha \cdot (\Gamma + \Theta) \geq \gamma \cdot \Gamma + \theta \cdot \Theta$$

      which holds by construction (via our substitution decomposition lemma).
    \item[Case $M_q~e$ (let-bind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Like the $\multimap I$ case, we only need to prove one side due to
          symmetry. By Lemma \ref{thm:ctx-stepping}, it suffices to prove this
          case for a bound value $(v_0, v_1)$. 
          We have by our inductive hypothesis that $(v_0, v_1) \in \mathcal{R}_{M_r
          \tau_0}$. So $\mathcal{SD}(v_0, v_1) \leq r$. By application of our
          inductive hypothesis, we know that (abusing notation) 
          $f[v_1/x] \mapsto^{*} (v_3, v_4), f[v_2/x] \mapsto^{*} (v_5, v_6) \in \mathcal{R}_{M_q}$.
          We also know that for all $\sigma$ where
          $$\sigma[v_1/x] \sim_{\vec{0}, r} \sigma[v_2/x] : \Gamma, x : \tau_0$$
          that
          $$f~\sigma[v_1/x] \sim_{(\vec{0},r) \cdot (\Gamma, s)} f~\sigma[v_2/x] : M_q \tau$$
          So therefore
          \begin{equation}
            \begin{aligned}[c]
              (v_3, v_4)~\sigma[v_1/x] &\sim_{(\vec{0},r) \cdot (\Gamma, s)} (v_5, v_6)~\sigma[v_2/x] : M_q \tau \\
              (v_3, v_4)~\sigma[v_1/x] &\sim_{r \cdot s} (v_5, v_6)~\sigma[v_2/x] : M_q \tau
            \end{aligned}
          \end{equation}
          So clearly $(v_3, v_6) \in \mathcal{R}_{M_{s \cdot r + q}}$.
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : s \cdot \Gamma + \Theta$:
          $$\textbf{let-bind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (s
          \cdot \Gamma + \Theta)} \textbf{let-bind}~x = e \ \tin \ f \sigma' :
          M_{s \cdot r + q}$$
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\gamma} \sigma'_{\gamma} : s \cdot \Gamma$
          and $\sigma_{\theta} \sim_{\theta} \sigma'_{\theta} : \Theta$ for
          $\gamma$ and $\theta$ minimal. 
          We also know that 
          $\sigma_{\gamma} \sim_{\frac{1}{s} \cdot \gamma} \sigma'_{\gamma} : \Gamma$ by
          inspection of the definition of type context scaling.
          By application of the inductive hypothesis, we know that:
          $$
          e~\sigma_{\gamma} \sim_{\frac{1}{s} \cdot \gamma \cdot \Gamma}
          e~\sigma'_{\gamma} : M_r \tau_0
          $$
          To apply our inductive hypothesis once more, we extend the $\theta$
          substutitons and obtain (where the $\frac{1}{s}$ is cancelled out by
          $s$):
          $$\sigma_{\theta}[e~\sigma_{\gamma}/x] \sim_{\theta \cdot \Theta + (\gamma \cdot \Gamma)}\sigma'_{\theta}[e~\sigma'_{\gamma}/x] : M_{s \cdot r + q} \tau$$
          Note that the distance $\sim$ is obtained via the inductive hypothesis
          and the fact that it is in $R_{M_{s \cdot r + q}}$ is obtained from
          our proof of \textbf{\underline{Property 1}}.
          Finally, we prove that:
          $$
          \alpha \cdot (s \cdot \Gamma + \Theta) = 
          (\alpha \cdot s \cdot \Gamma) + (\alpha \cdot \Theta)
          \geq
          \gamma \cdot \Gamma + \theta \cdot \Theta
          $$
          by construction (via our substitution decomposition lemma).
      \end{description}
    \item[Case (let-cobind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We again only need to prove one side due to symmetry.
          By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for a
          bound value $[v]$. 
          We have by our inductive hypothesis that $[v] \in \mathcal{R}_{!_s \tau_0}$.
          Unfolding, we have that $v \in \mathcal{R}_{\tau_0}$.
          Stepping and applying our inductive hypothesis yields that $f[v/x] \in
          \mathcal{R}_{\tau}$.
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : t \cdot \Gamma + \Theta$:
          $$\textbf{let-cobind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (t
          \cdot \Gamma + \Theta)} \textbf{let-cobind}~x = e \ \tin \ f \sigma' :
          \tau$$
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\gamma} \sigma'_{\gamma} : t \cdot \Gamma$
          and $\sigma_{\theta} \sim_{\theta} \sigma'_{\theta} : \Theta$ for
          $\gamma$ and $\theta$ minimal. 
          We also know that 
          $\sigma_{\gamma} \sim_{\frac{1}{t} \cdot \gamma} \sigma'_{\gamma} : \Gamma$ by
          inspection of the definition of type context scaling.

          By application of the inductive hypothesis, we know that:
          $$
          e~\sigma_{\gamma} \sim_{\frac{1}{t} \cdot \gamma \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0
          $$
          and therefore by inspection of the relation:
          $$
          e~\sigma_{\gamma} \sim_{\frac{1}{t \cdot s} \cdot \gamma \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \> \tau_0
          $$
          To apply our inductive hypothesis once more, we extend the $\theta$
          substutitons and obtain (where the $\frac{1}{t \cdot s}$ is is
          cancelled out from multiplying byt $t \cdot s$ ):
          $$\sigma_{\theta}[e~\sigma_{\gamma}/x] \sim_{\theta \cdot \Theta + (\gamma \cdot s \cdot \Gamma)}\sigma'_{\theta}[e~\sigma'_{\gamma}/x] : \tau$$
          Note that the distance $\sim$ is obtained via the inductive hypothesis
          and the fact that it is in $R_{M_{s \cdot r + q}}$ is obtained from
          our proof of \textbf{\underline{Property 1}}.
          Finally, we prove that:
          $$
          \alpha \cdot (t \cdot \Gamma + \Theta) = 
          (\alpha \cdot t \cdot \Gamma) + (\alpha \cdot \Theta)
          \geq
          \gamma \cdot \Gamma + \theta \cdot \Theta
          $$
          by construction (via our substitution decomposition lemma).
      \end{description}

    \item[Case factor.] 
      By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for 
      $\mathbf{factor}~((v_1, v_2), (v_3, v_4))$. 
      Unfolding our inductive hypothesis, we have that for substitutions
      $\sigma \sim_{\gamma} \sigma' : \Gamma$, that 
      $((v_1, v_2), (v_3, v_4))~\sigma \sim_{\gamma \cdot \Gamma} ((v_1, v_2),
      (v_3, v_4))~\sigma' : (M_q~\tau_0) \times (M_r~\tau_1)$.
      Unfolding, we get:
      $$
      ((v_1~\sigma, v_2~\sigma), (v_3~\sigma, v_4~\sigma)) \sim_{\gamma \cdot \Gamma} ((v_1~\sigma', v_2~\sigma'),
      (v_3~\sigma', v_4~\sigma')) : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      So by reassociating we obtain:
      $$
      ((v_1~\sigma, v_3~\sigma), (v_2~\sigma, v_4~\sigma)) \sim_{\gamma \cdot \Gamma} ((v_1~\sigma', v_3~\sigma'),
      (v_2~\sigma', v_4~\sigma')) : M_{max(r,q)}(\tau_0 \times \tau_1)
      $$

    \item[Case rnd.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Holds under the assumption the constant langauge parameter $u$ has the
          required properties (see Definition~\ref{def:numfuzz-interface},
          property a) and application of our inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case ret.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Holds trivially by applicatin of the inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case op.] 
      Both properties hold by our language interface
      (Definition~\ref{def:numfuzz-interface}, property b) and application of
      the inductive hypothesis.
  \end{description}
\end{proof}

% \begin{lemma}[Metric preservation]
% \end{lemma}
%
% \begin{lemma}[Type error soundness]
% \end{lemma}
