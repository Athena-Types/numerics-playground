\section{Type Soundness}
Following Fuzz, we prove the following syntanctic properties relating our type
systema and operational semantics.

\begin{lemma}[Weakening]\label{thm:weakening}
  If $\Gamma \vdash e : \tau$, then $\Gamma + \Sigma \vdash e : \tau$.
\end{lemma}
\begin{proof}
  We induct over our typing devivation. 
  For each case, we can ignore arbitrary variables in our context (the type system is
  affine). 
  In particular, for the \textbf{!E}, \textbf{$\otimes$ E}, \textbf{+ E},
  \textbf{$M_u$~E}, and \textbf{$\otimes$ I} rules, the enviroment must be split
  into a $\Gamma$ (which can in some cases be scaled) and $\Delta$ (which does
  not ever get scaled); for each of these cases we choose to push unused
  variables into $\Delta$. 
\end{proof}

Following Fuzz, we prove a $r$-sensitive substitution lemma.
\begin{lemma}[$r$-sensitive subsitution]\label{thm:substitution}
  Let $\Gamma \vdash e : \tau$ and $\Theta, x :_r \tau \vdash e' : \tau'$, then 
  $r \cdot \Gamma + \Theta \vdash e'[e/x] : \tau'$.
\end{lemma}
\begin{proof}
  We first induct over the height of the second typing derivation. 
  \begin{description}
    \item[Base case.] Follows by copying-and-pasting the first typing derivation
      if / where a variable is used.
    \item[Inductive case.] We case into each possible typing rule. 
      For each premise in the typing rule, we have a way / a function to
      subsitute $e$ for $x$ with sensitivity $s$ and obtain a valid typing tree. 
      We apply this function to each premise (where $x$ is not captured by a
      binder). 
      Now we want to construct a valid typing tree for $r \cdot \Gamma + \Theta
      \vdash e'[e/x] : \tau'$. We observe how $x$ gets scaled (in the $!_s \tau$
      rule) or combined in the typing derivation (whereever $\Gamma + \Theta$
      occurs) and observe that our invariant is maintained.
  \end{description}
\end{proof}

The following notation mirrors the look of the Fuzz metric perservation theorem
statement but contains differences in the construction. In particular, our
logical relation is neither coinductive nor step-indexed. It is also unary,
using mutual (well-founded) recursion between the definition of a syntactic term
falling within the logical relation and the definition of the syntactic distance
between terms.

For any two closed expressions $e_0, e_1$ falling in same type relation $e_0,
e_1 \in R_\tau$, we can write $e_0 \sim_r e_1$ where $\mathcal{SD}_{\tau}(e_0,
e_1) \leq r$. Similarly, we can write lists of expressions (e.g. for
subsitutions) $\sigma_0, \sigma_1$ for a given typing context $\Gamma$ such
that: $\sigma_0 \sim_{\gamma} \sigma_1 : \Gamma$ for a \textit{distance vector}
$\gamma$
where
$\sigma_0 = e_0,~e_1,~\ldots e_{n-1}$ 
and $\sigma_1 = e'_0,~e'_1,~\ldots e'_{n-1}$ 
and $\gamma = r_0, r_1, \ldots r_{n-1}$ such that:
$$e_0 \sim_{r_0} e'_0 : \tau_0,~e_0 \sim_{r_0} e'_0 : \tau_0,~\ldots e_{n-1}
\sim_{r_1} e'_{n-1} : \tau_n$$
We also say that a substitution $[e_1/x_1, e_2/x_2 \ldots]$ is
\textit{compatible} with a typing context $x_1 : \tau_1, x_2 : \tau_2, \ldots$
if each term $e_1 \in \mathcal{R}_{\tau_1}, e_2 \in \mathcal{R}_{\tau_2},
\ldots$

Our definition for the dot product of a distance vector is the same as Fuzz. We
also write, for a distance vector $\gamma$ and variable $x$, $\gamma(x)$ for the
lookup of the distance of variable $x$ in $\gamma$. If the variable $x$ is not
in the domain, $\gamma(x) = 0$ by default. From here on in the paper, we'll
treat and represent our distance vector $\gamma$ as a lookup function and assume
that there is an implicit fixed ordering on the variables.
% todo: put it in here

% Max: Theorem statement is not interesting when stepping is only over closed
% terms.
% \begin{lemma}[Subsitution is invariant under stepping]
%   \label{thm:sub-stepping}
%   For any substitution $\sigma$ and program $e$ such that $e~\sigma \mapsto
%   e_1$, if $e \mapsto e_2$ then $e_2~\sigma \mapsto^{*} v \iff e_1 \mapsto^{*} v$.
% \end{lemma}
% \begin{proof}
%   TODO
% \end{proof}
%
\begin{lemma}[Subsitution decomposition]
  \label{thm:sub-decomp}
  For substitutions $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$,
  exists
  $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : \Gamma$
  and
  $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$.
\end{lemma}
\begin{proof}
  Fix an ordering of the variables in $\Gamma + \Theta$. Follows by induction.
\end{proof}

The below lemma is useful towards proving metric preservation:
\begin{lemma}[Metric preservation under context evaluation stepping]
  \label{thm:ctx-stepping}
  For a well-typed $\Gamma \vdash C[e] : \tau$ and substitutions $\sigma,
  \sigma'$ such that $\sigma \sim_{\gamma} \sigma' : \Gamma$ and
  $e~\sigma \mapsto^{*} v$ and
  $e~\sigma' \mapsto^{*} v'$.
  If $C[v] \sigma \sim_{\gamma \cdot \Gamma} C[v'] \sigma' : \tau$ then
  $C[e] \sigma \sim_{\gamma \cdot \Gamma} C [e] \sigma' : \tau$.
\end{lemma}
\begin{proof}
  By inspection of our stepping relation, we can tell that stepping is
  deterministic. Further, since we only define our rewrite relation over closed
  terms, we know that $e~\sigma, e~\sigma'$ is closed and therefore constant
  under all substitutions. 
  So, by the definition of $\mathcal{R}$, $\mathcal{SD}$, 
  context stepping, we know that both 
  $$
  C[e~\sigma]~\sigma, C[e~\sigma']~\sigma' \in \mathcal{R}_{\tau}
  $$ 
  and that 
  $$
  \mathcal{SD}_{\tau}(C[e]\sigma, C[e]\sigma') = 
  \mathcal{SD}_{\tau}(C[e~\sigma]\sigma, C[e~\sigma']\sigma') = 
  \mathcal{SD}_{\tau}(C[v]\sigma, C[v']\sigma') = 
  \gamma \cdot \Gamma
  $$
  respectively and therefore 
  $C[e] \sigma \sim_{\gamma \cdot \Gamma} C[e] \sigma'$
  holds.
\end{proof}

\begin{theorem}[Metric preservation]
  For any $\Gamma \vdash e : \tau$ and substitutions $\sigma, \sigma'$ such that
  $\sigma \sim_{\gamma} \sigma' : \Gamma$, then 
  $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma'$.
\end{theorem}
\begin{proof}
  We induct over our typing derivation. The base cases (Var, Unit, Const) follow
  trivially. The subsumption case also follows trivially. We detail the
  remaining cases here:
  \begin{description}
    \item[Case $\multimap$ I.] 
      We wish to show that for any $\Gamma \vdash \lambda x . e : \tau$ and
      subsitutions $\sigma \sim_{\gamma} \sigma' : \Gamma$ that $\lambda x .
      e~\sigma \sim_{\gamma \cdot \Gamma} \lambda x . e~\sigma'$. Unfolding, it
      suffices to show that both:
      \begin{enumerate}
        \item $\lambda x . e~\sigma, \lambda x . e~\sigma'$ are in
          $\mathcal{R}_{\tau_0 \multimap \tau}$
        \item $\mathcal{SD}_{\tau_0 \multimap \tau}(\lambda x . e~\sigma,
          \lambda x . e~\sigma') \leq \gamma \cdot \Gamma$
      \end{enumerate}
    
      Observe that we have by our inductive hypothesis that for any $\Delta, x:
      \tau' \vdash e : \tau$ and substitutions 
      $\delta[v_0/x] \sim_{\gamma'} \delta'[v_1/x] : \Gamma,~x : \tau'$ that 
      $e~\delta[v_0/x] \sim_{\gamma' \cdot \Gamma} e~\delta'[v_1/x]$ holds. 
      Let us now carry on with the proof:

      \begin{description}
        \item[\underline{Property 1.}] We need to show that $(\lambda x . e)~\sigma$ and
          $(\lambda x . e)~\sigma'$ are both in the relation $\mathcal{R}_{\tau_0 \multimap
          \tau}$. The cases are symmetric so we only show one case. Unfolding
          the definition of $\mathcal{R}_{\tau_0 \multimap \tau}$, let $w_0, w_1
          \in \mathcal{VR}_{\tau_0}$. 
          $(\lambda x . e) w_0 \mapsto e[w_0/x]$
          and
          $(\lambda x . e) w_1 \mapsto e[w_1/x]$
          and so it suffices to show that
          $e~\sigma[w_0/x], e~\sigma[w_1/x]$ 
          are closed expressions falling in $R_{\tau_0}$.
          This is true by our inductive hypothesis, instantiating with $\sigma =
          \delta = \delta'$, $v_0 = w_0$, and $v_1 = w_1$.
        \item[\underline{Property 2.}] Unfolding the definition of
          $\mathcal{SD}_{\tau_0 \multimap \tau}$, it suffices to show that:
          $$
          \mathcal{SD}_{\tau}
          ((\lambda x . e~w)~\sigma, (\lambda x . e~w)~\sigma') 
          \leq \gamma \cdot \Gamma
          $$
          Stepping, it suffices to show that
          $$
          \mathcal{SD}_{\tau}
          (e~\sigma[w/x], e~\sigma'[w/x]) 
          \leq \gamma \cdot \Gamma
          $$
          which holds by application of our inductive hypothesis when $\delta =
          \sigma$, $\delta' = \sigma'$, $v_0 = v_1 = w$, and $\gamma' = \gamma
          :: 0$.
      \end{description}
      So, by our inductive hypothesis and unfolding the definitions of
      $\mathcal{R}$ and $\mathcal{SD}$, we have properties (1) and (2)
      respectively. 
    \item[Case $\multimap$ E.] 
      By our inductive hypothesis, it suffices
      to prove this case for $e~f$.
      % By inversion, we know that $v = \lambda x . e$.
      We wish to show that for any $\Gamma + \Theta \vdash e~f : \tau$ and
      subsitutions 
      $\sigma \sim_{\alpha} \sigma': \Gamma + \Theta$ 
      that 
      $$e~f~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} e~f~\sigma'$$
      Using Lemma \ref{thm:sub-decomp} (substitution decomposition), we
      construct potentially overlapping substitutions
      $\sigma_0 \sim_{\alpha} \sigma_1 : \Gamma$
      and $\sigma_0' \sim_{\alpha} \sigma_1' : \Theta$
      where $FV(e) \subseteq DOM(\sigma_0) = DOM(\sigma_1)$ and
      $FV(f) \subseteq DOM(\sigma_0') = DOM(\sigma_1')$ and 
      $\sigma_0, \sigma_0' \subseteq \sigma$ and 
      $\sigma_1, \sigma_1' \subseteq \sigma'$ and $\gamma, \theta$ minimal.
      In other words, $\sigma_0, \sigma_1$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Gamma$.
      Similarly, $\sigma_0', \sigma_1'$ correspond to the parts of $\sigma$ and $\sigma'$ 
      respectively that are represented by $\Theta$.
      Note that our substitutions \textit{must} overlap in the case where $FV(e)
      \cap FV(f)$ is non-empty.
      By our inductive hypothesis, we know that 
      $e~\sigma \mapsto^* \lambda x . M$ and 
      $e'~\sigma \mapsto^* \lambda x . M'$ for some $M, M'$.
      Since subsituting variables that are not free does not change the
      underlying term, we know that the following equations must hold where
      $\sigma^*, \sigma'^*$ are $\sigma$ and $\sigma'$ respectively with $x$
      removed:
      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . M)~[\sigma_0]~(w[\sigma_1]) &= 
            ((\lambda x . M[\sigma^*])~w)[\sigma] = 
            ((\lambda x . M)~w)[\sigma] \mapsto
            M[\sigma][w/x] \\
          (\lambda x . M')~[\sigma_0']~(w[\sigma_1']) &= 
            ((\lambda x . M'[\sigma'^*])~w)[\sigma'] = 
            ((\lambda x . M')~w)[\sigma'] \mapsto
            M'[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      and by stepping

      \begin{equation}
        \begin{aligned}[c]
          (\lambda x . M)~[\sigma_0]~(w[\sigma_1]) &\mapsto M[\sigma_0][w[\sigma_1]/x] \\
          (\lambda x . M)~[\sigma_0']~(w[\sigma_1']) &\mapsto M'[\sigma_0'][w[\sigma_1']/x] \\
        \end{aligned}
      \end{equation}

      so by deterministic stepping
      \begin{equation}
        \begin{aligned}[c]
          M[\sigma_0][w[\sigma_1]/x] &=
            M[\sigma][w/x] \\
          M'[\sigma_0'][w[\sigma_1']/x] &=
            M'[\sigma'][w/x] \\
        \end{aligned}
      \end{equation}

      By applying our inductive hypothesis and unfolding our definition of
      $R_{\tau_0 \multimap \tau}$, we know that $M, M'$ is 
      1-sensitive with respect to $w$ according to $\mathcal{SD}$. 
      This gives us:
      $$
      \mathcal{SD}_{\tau}(M~[w[\sigma]/x][\sigma], M~[w[\sigma']/x][\sigma]) 
      \leq 
      \mathcal{SD}_{\tau_0}(w[\sigma], w[\sigma']) 
      \leq 
      \theta \cdot \Theta
      $$
      and we also know that by our inductive hypothesis
      $$
      \mathcal{SD}_{\tau}(e~[\sigma]~f~[\sigma'], e~[\sigma']~f~[\sigma']) 
      =
      \mathcal{SD}_{\tau}(M~[w[\sigma']/x][\sigma], M'~[w[\sigma']/x][\sigma']) 
      \leq 
      \gamma \cdot \Gamma
      $$
      So by our triangle inequality of the syntactic distance between terms:
      $$
      M~[w[\sigma]/x]~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta}
      M'~[w[\sigma']/x]~\sigma'
      $$

      and therefore
      $$
      (\lambda x . M)~w~\sigma \sim_{\gamma \cdot \Gamma + \theta \cdot \Theta} (\lambda x . M')~w~\sigma'
      $$

      To complete the proof case, it suffices to prove 
      $$\alpha \cdot (\Gamma + \Theta) \geq \alpha \cdot \Gamma + \alpha \cdot \Theta$$

      which holds by construction (via our substitution decomposition lemma).
    \item[Case $M_q~e$ (let-bind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Like the $\multimap I$ case, we only need to prove one side due to
          symmetry. So we fix a substitution $\sigma$. By Lemma
          \ref{thm:ctx-stepping}, it suffices to prove this case for a value
          $(v_0, v_1)$. 
          We have by our inductive hypothesis that $(v_0, v_1) \in \mathcal{R}_{M_r
          \tau_0}$. So $\mathcal{SD}(v_0, v_1) \leq r$. By application of our
          inductive hypothesis, we know that (abusing notation) 
          $f[v_1/x], f[v_2/x] \in \mathcal{R}_{M_q}$ so
          $f[v_1/x] \mapsto^{*} (v_3, v_4), f[v_2/x] \mapsto^{*} (v_5, v_6)$ where 
          $(v_3, v_4), (v_5, v_6) \in \mathcal{R}_{M_q}$.
          We also know that 
          $$\sigma[v_0/x] \sim_{\vec{0}, r} \sigma[v_1/x] : \Gamma, x : \tau_0$$
          that
          $$f~\sigma[v_0/x] \sim_{(\vec{0},r) \cdot (\Gamma, x \mapsto s)} f~\sigma[v_1/x] : M_q \tau$$
          So therefore
          \begin{equation}
            \begin{aligned}[c]
              (v_3, v_4)~\sigma[v_1/x] 
                &\sim_{(\vec{0},r) \cdot (\Gamma, x \mapsto s)} 
              (v_5, v_6)~\sigma[v_2/x] : M_q \tau \\
              (v_3, v_4)~\sigma[v_1/x] 
                &\sim_{r \cdot s} 
              (v_5, v_6)~\sigma[v_2/x] : M_q \tau
            \end{aligned}
          \end{equation}
          So clearly $(v_3, v_6) \in \mathcal{R}_{M_{s \cdot r + q}}$ by
          application of triangle inequality: 
          $$
          r \cdot s + q \geq
          \mathcal{SD}_{tau}(v_3, v_5) + \mathcal{SD}_{\tau}(v_5, v_6) \geq
          \mathcal{SD}_{\tau}(v_3, v_6)
          $$
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : s \cdot \Gamma + \Theta$:
          \begin{equation}
            \label{eq:lb-prop2}
          \textbf{let-bind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (s
          \cdot \Gamma + \Theta)} \textbf{let-bind}~x = e \ \tin \ f \sigma' :
          M_{s \cdot r + q}
          \end{equation}
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : s \cdot \Gamma$
          and $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$ .

          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} : M_r \tau_0
          $$
          By our proof of \textbf{\underline{Property 1}}, we know that both
          sides of Equation~\ref{eq:lb-prop2} are in the relation
          $\mathcal{R}_{M_{r \cdot s + q} \tau}$ and we're not stuck. We want to
          show both sides have syntatic distance less than $\alpha \cdot (s
          \cdot \Gamma + \Theta)$. 
          We apply our inductive hypothesis once more and extend the
          $\theta$ substutitons to obtain:
          \begin{equation} \label{eq:lb-prop2.1}
            f~\sigma_{\theta}[e~\sigma_{\gamma}/x] 
            \sim_{\alpha \cdot \Theta + (\alpha \cdot s \cdot \Gamma)} 
            f~\sigma'_{\theta}[e~\sigma'_{\gamma}/x] : M_{r} \tau
          \end{equation}
          Since both sides are neighborhood monads, it suffices to bound
          distances in only the first components of each side which means that
          Equation~\ref{eq:lb-prop2.1} implies Equation~\ref{eq:lb-prop2}.
      \end{description}
    \item[Case (let-cobind).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We again only need to prove one side due to symmetry.
          By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for a
          value $[v]~\sigma$ for a $\sigma$ compatible withi $\Gamma$. 
          We have by our inductive hypothesis that $[v]~\sigma \in \mathcal{R}_{!_s
          \tau_0}$. Unfolding, we have that $v~\sigma \in \mathcal{R}_{\tau_0}$.
          Stepping and applying our inductive hypothesis yields that
          $f~\sigma~[v/x] \in \mathcal{R}_{\tau}$.

        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha}
          \sigma' : t \cdot \Gamma + \Theta$:
          $$\textbf{let-cobind}~x = e \ \tin \ f \sigma \sim_{\alpha \cdot (t
          \cdot \Gamma + \Theta)} \textbf{let-cobind}~x = e \ \tin \ f \sigma' :
          \tau$$
          By Lemma $\ref{thm:sub-decomp}$ (substitution decomposition) we have
          substitutions 
          $\sigma_{\gamma} \sim_{\alpha} \sigma'_{\gamma} : \Gamma$
          and $\sigma_{\theta} \sim_{\alpha} \sigma'_{\theta} : \Theta$.
          By application of the inductive hypothesis, we know that:
          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0
          $$
          where 
          $$
          e~\sigma_{\gamma} = [v_0]
          $$
          and
          $$
          e~\sigma'_{\gamma} \ : \> !_{s}~\tau_0 = [v_1]
          $$
          for some $v_0, v_1$ so therefore by inspection of the relation:
          $$
          s \cdot \mathcal{SD}_{\tau_0}(v_0, v_1) =  
          \mathcal{SD}_{!_s~\tau_0}([v_0], [v_1])
          $$
          Let $\mathcal{SD}_{\tau_0}(v_0, v_1) = c$. Importantly, note that 
          \begin{equation} \label{eq:lcb-zero-sub}
            s \cdot c = \alpha \cdot \Gamma
          \end{equation}
          Then, we apply our inductive hypothesis once more and we extend the
          $\theta$ substutitons and to obtain:
          $$
          f~\sigma_{\theta}[\sigma_{\gamma}/x] 
          \sim_{\alpha \cdot \Theta + t \cdot s \cdot c}
          f~\sigma'_{\theta}[\sigma'_{\gamma}/x] : \tau
          $$
          Note that the distance $\sim$ is obtained via the inductive hypothesis
          and the fact that it is in $R_{M_{s \cdot r + q}}$ is obtained from
          our proof of \textbf{\underline{Property 1}}.
          Substituting by Equation~\ref{eq:lcb-zero-sub} we complete the case:
          $$
          f~\sigma_{\theta}[\sigma_{\gamma}/x] 
          \sim_{\alpha \cdot \Theta + t \cdot \alpha \cdot \Gamma}
          f~\sigma'_{\theta}[\sigma'_{\gamma}/x] : \tau
          $$
      \end{description}

    \item[Case (let).] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. By symmetry, it suffices to
          only prove one side. Follows by application of our inductive
          hypothesis and some stepping.
        \item[\underline{Property 2.}]
          We need to show that for all substitutions $\sigma \sim_{\alpha} \sigma' :
          s \cdot \Gamma + \Theta$:
          $$
          \textbf{let} \ x = e \ \tin \ f \sigma 
            \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
          \textbf{let} \ x = e \ \tin \ f \sigma' :
          \tau
          $$
          By Lemma~\ref{thm:sub-decomp} we have substitutions $\sigma_{\gamma}
          \sim_\alpha \sigma'_{\gamma} : \Gamma$ and $\sigma_{\theta}
          \sim_\alpha \sigma'_{\gamma} : \Theta$. By applying our inductive
          hypothesis, we know that 
          $$
          e~\sigma_{\gamma} \sim_{\alpha \cdot \Gamma}
          e~\sigma'_{\gamma} \ : \tau_0
          $$
          So we can extend our pair of $\Theta$ subsitutions by these two terms
          respsectively and apply our inductive hypothesis yielding:
          $$
          \textbf{let} \ x = e \ \tin \ f \sigma 
            \sim_{(\alpha \cdot s \cdot \Gamma) + (\alpha \cdot \Theta)}
          \textbf{let} \ x = e \ \tin \ f \sigma' :
          \tau
          $$
          which is equivalent to what we wanted to show.

      \end{description}

    \item[Case factor.] 
      By Lemma \ref{thm:ctx-stepping}, it suffices to prove this case for 
      $\mathbf{factor}~((v_1, v_2), (v_3, v_4))$, 
      $\mathbf{factor}~((v'_1, v'_2), (v'_3, v'_4))$. 
      Unfolding our inductive hypothesis, we have that for substitutions
      $\sigma \sim_{\gamma} \sigma' : \Gamma$, that 
      $$
      \textbf{factor} \ ((v_1, v_2), (v_3, v_4))~\sigma 
      \sim_{\gamma \cdot \Gamma} 
      \textbf{factor} \ ((v'_1, v'_2), (v'_3, v'_4))~\sigma' 
      : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      Unfolding, we get:
      $$
      \textbf{factor} \ ((v_1~\sigma, v_2~\sigma), (v_3~\sigma, v_4~\sigma)) 
      \sim_{\gamma \cdot \Gamma} 
      \textbf{factor} \ ((v'_1~\sigma', v'_2~\sigma'), (v'_3~\sigma', v'_4~\sigma')) 
      : (M_q~\tau_0) \times (M_r~\tau_1)
      $$
      So by stepping (and reassociating) we can see that the following logical
      relation holds as distances between neighborhood monads are measured in
      terms of the first components:
      $$
      ((v_1~\sigma, v_3~\sigma), (v_2~\sigma, v_4~\sigma)) 
      \sim_{\gamma \cdot \Gamma} 
      ((v'_1~\sigma', v'_3~\sigma'), (v'_2~\sigma', v'_4~\sigma')) 
      : M_{max(r,q)}(\tau_0 \times \tau_1)
      $$

    \item[Case rnd.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. The case holds under the
          assumption the constant langauge parameter $u$ has the required
          properties (see Definition~\ref{def:numfuzz-interface}, property a)
          and application of our inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case ret.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. Holds trivially by application
          of the inductive hypothesis.
        \item[\underline{Property 2.}]
          Since distance is measured for the neighborhood monad on the first
          component, which in our case is the ideal (not rounded) component (see
          stepping rule), this holds trivially by application of the inductive
          hypothesis.
      \end{description}

    \item[Case op.] 
      Both properties hold by our language interface
      (Definition~\ref{def:numfuzz-interface}, property b) and application of
      the inductive hypothesis.

    \item[Case ! I.] Holds unfolding of our logical relations, and our inductive
      hypothesis.

    \item[Case $\times$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          Holds by application of the inductive hypothesis and unfolding of the
          definition of $\mathcal{R}_{\tau_0 \times \tau_1}$.
        \item[\underline{Property 2.}]
          We have 
          $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$ 
          and wish to show that
          $$(e, f)~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (e, f)~\sigma'$$

          By Lemma~\ref{thm:sub-decomp}, we know that there exists substitutions
          $\sigma_{\Gamma} \sim_{\alpha} \sigma'_{\Gamma} : \Gamma$
          and
          $\sigma_{\Theta} \sim_{\alpha} \sigma'_{\Theta} : \Theta$.
          So by definition unfolding we have that:
          $$
          (v~\sigma_{\Gamma}, w~\sigma_{\Theta}) 
            \sim_{(\alpha \cdot \Gamma + \alpha \cdot \Theta)} 
          (v~\sigma'_{\Gamma}, w~\sigma'_{\Theta})
          : \tau_0 \times \tau_1
          $$
          so therefore by an analysis of free variables and subsitution
          $$
          (v, w)~\sigma 
            \sim_{(\alpha \cdot \Gamma + \alpha \cdot \Theta)} 
          (v, w)~\sigma'
          : \tau_0 \times \tau_1
          $$

          To complete the proof case, it suffices to prove:
          $$\alpha \cdot (\Gamma + \Theta) \geq \alpha \cdot \Gamma + \alpha \cdot \Theta$$

          which holds by construction (via our substitution decomposition lemma).
      \end{description}

    \item[Case $\times$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. The case holds by application
          of the inductive hypothesis and stepping once.
        \item[\underline{Property 2.}]
          It suffices to show that for all $\sigma \sim_{\gamma} \sigma' : \Gamma$
          if $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma' : \tau_1 \times \tau_2$
          then:
          $$
          \pi_i~e~\sigma \sim_{\gamma \cdot \Gamma} \pi_i~e~\sigma' : \tau_i
          $$
          which holds by stepping and some unfolding of the logical relation.
      \end{description}

    \item[Case $\otimes$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          The case holds by application of the inductive hypothesis.
        \item[\underline{Property 2.}]
          It suffices to show that for all 
          $\sigma \sim_{\alpha} \sigma' : \Gamma + \Theta$:
          $$
          (e, f)~\sigma \sim_{\alpha \cdot (\Gamma + \Theta)} (e, f)~\sigma' :
          \tau_0 \otimes \tau_1
          $$
          By Lemma~\ref{thm:sub-decomp}, we know that 
          $\sigma \sim_{\alpha} \sigma' : \Gamma$ and
          $\sigma \sim_{\alpha} \sigma' : \Theta$.
          Appying our inductive hypothesis, we get that
          $e~\sigma \sim_{\alpha \cdot \Gamma} e~\sigma' : \tau_0$ and
          $e~\sigma \sim_{\alpha \cdot \Theta} e~\sigma' : \tau_1$
          so by unfolding our logical relation we obtain
          $$
          (e, f)~\sigma \sim_{\alpha \cdot \Gamma + \alpha \cdot \Theta} (e, f)~\sigma' :
          \tau_0 \otimes \tau_1
          $$
          which is equivalent to what we wanted to show.
      \end{description}

    \item[Case $\otimes$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          We again only need to prove one side due to symmetry. Let our bound
          expression be some value $w$. By our inductive hypothesis $w = (v_0,
          v_1)$ for some $v_0$ and $v_1$. Stepping and applying our inductive
          hypothesis yields that $f~\sigma~[v_0/x][v_1/x] \in
          \mathcal{R}(\tau)$ for any $\sigma$ compatible with $\Theta$.
        \item[\underline{Property 2.}]
          This case mirrors the proof for \textbf{\underline{Property 2}} of the
          $\textbf{let}$ case.
      \end{description}

    \item[Case $+$ I.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}.
          Holds by application of the inductive hypothesis.
        \item[\underline{Property 2.}]
          It suffices to show that for all $\sigma \sim_{\gamma} \sigma' : \Gamma$
          if $e~\sigma \sim_{\gamma \cdot \Gamma} e~\sigma' : \tau_0 + \tau_1$
          then:
          $$
          \tin_i~e~\sigma \sim_{\gamma \cdot \Gamma} \tin_i~e~\sigma' : \tau_0 +
          \tau_1
          $$
          which holds by stepping and some unfolding of the logical relation.
      \end{description}

    \item[Case $+$ E.] 
      We prove each property separately.
      \begin{description}
        \item[\underline{Property 1.}]
          We apply Lemma~\ref{thm:ctx-stepping}. We again only need to prove one
          side due to symmetry. Let our bound expression be some value
          $\textbf{in}_i ~ v$. Stepping and applying our inductive hypothesis
          finishes this case.
        \item[\underline{Property 2.}]
          We wish to show that for all 
          $\sigma \sim_{\alpha} \sigma': s \cdot \Gamma + \Theta$ 
          that:
          $$
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2)~\sigma
          \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
          \mathbf{case} \ e \ \mathbf{of} \ (\tin_1 \ x.f_1 \ | \ \tin_2 \ x.f_2)~\sigma'
          : \tau
          $$
          By Lemma~\ref{thm:sub-decomp} we have substitutions
          $$
          \sigma_{\Gamma} \sim_{\alpha} \sigma'_{\Gamma} : \Gamma
          $$
          $$
          \sigma_{\Theta} \sim_{\alpha} \sigma'_{\Theta} : \Theta
          $$
          that we can plug into our inductive hypothesis. Plugging in, we
          know that: $e~\sigma \mapsto \mathbf{in}_i~v$ 
          and $e~\sigma' \mapsto \mathbf{in}_j~v'$ where
          $\mathcal{SD}(\mathbf{in}_i~v, \mathbf{in}_j~v') 
          \leq \gamma \cdot \Gamma$.
          We now case over the following two scenarios:
          \begin{description}
            \item[Subcase $i = j$.] This subcase mirrors the proof for
              \textbf{\underline{Property 2}} of the $\textbf{let}$ case.
            \item[Subcase $i \not= j$.] In this case, we have:
              $\mathcal{SD}(\mathbf{in}_i~v, \mathbf{in}_j~v') = \infty \leq
              \alpha \cdot \Gamma$. Stepping and applying the inductive
              hypothesis, we obtain:
              $$
              f_i [v/x]\sigma 
              \sim_{\alpha \cdot (s \cdot \Gamma + \Theta)}
              f_j [v'/x]\sigma'
              : \tau
              $$
              Since $s$ is non-zero, by applying our inductive hypothesis we
              know that $f_i$ and $f_j$ are non-zero sensitive in $x$ and since
              positive $s \cdot \infty = \infty$, we know that $\alpha \cdot (s
              \cdot \Gamma + \Theta) = \infty$. Therefore this subcase holds.
              Note that it is essential in this case that $s$ is non-zero; If
              $s$ is allowed to be zero, this would be a problem because $0
              \cdot \infty = 0$.
          \end{description}


      \end{description}

  \end{description}
\end{proof}

% \begin{lemma}[Metric preservation]
% \end{lemma}
%
% \begin{lemma}[Type error soundness]
% \end{lemma}
