\section{Introduction}
It is natural for ordinary programmers such as mathematicians and scientists to
wish their computers operate over the reals. 
But this is impossible and programming languages typically compute over the
finitary floating-point number approximation.
As language designers, we wish to convince the ordinary programmer, who are
largely not experts in numerical analysis, that the floating-point semantics we
have provided is ``close enough" to what they desire.
Towards this end, we develop automated numerical analyses to bound the error
introduced by our floating-point approximations. We wish for our analyses to be
efficient, as automatic as possible, and precise.

One major challenge in the automated numerical analysis literature is
scalability. Many existing approaches rely on global optimization
\cite{fptaylor} \cite{satire}, rewrite saturation \cite{gappa}, or SMT-based
methods \cite{rosa}. However, as programs scale, these analysis approaches
frequently time-out. Recent work has applied typed-based analysis approaches to
both forwards and backwards error analysis, such as Numerical Fuzz
\cite{numfuzz} and Bean \cite{bean}. The advantage of type-based methods is that
they are inherently compositional and scalable: all of the information necessary
to perform an error analysis on a function is contained within the function
type. There is no need to perform global optimization, run an algorithm to
convergence, saturate rewrites, or bit-blast.

\paragraph{Limitations of prior type-based work} 
The core idea of prior type-based work is to track two key properties of each
function: the function sensitivity $s$ and the maximum round-off error $q$
introduced by the function. Intuitively, $s$ upper-bounds the amount by which
perturbations in a function's input affect its output. 
As explained visually in Figure~\ref{fig:sen}, it is the
combination of function sensitivity along with round-off error that allows
results in a compositional error bound.
For given function $f$ with senstivity $s$ and round-off error $q$ with at most
$r$ error in its inputs $\tilde{x}$, we can apply triangle inequality to reason
that $\widetilde{f(x)}$ will exhibit at most $s \cdot r + q$ round-off error. 

\begin{figure}[ht] \label{fig:sen}
% https://q.uiver.app/#q=WzAsNyxbMCwwLCJ4Il0sWzMsMCwiXFx0aWxkZXt4fSJdLFswLDIsImYoeCkiXSxbOCwyLCJcXHdpZGV0aWxkZXtmKHgpfSJdLFswLDNdLFs4LDNdLFs0LDEsImYoXFx0aWxkZXt4fSkiXSxbMCwxLCJcXHRleHR7KHBlcnR1cmJhdGlvbiBpbiBmdW5jdGlvbiBpbnB1dHMpfSBcXFxcIHIiLDAseyJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzAsMiwiIiwxLHsiY29sb3VyIjpbMTIwLDYwLDMwXX1dLFsxLDMsIiIsMSx7ImNvbG91ciI6WzAsNjAsNTJdfV0sWzQsNSwicyBcXGNkb3QgciArIHEgXFxcXCBcXHRleHR7KHRyaWFuZ2xlIGluZXF1YWxpdHksIGZpbmFsIGJvdW5kKX0iLDIseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJhcnJvd2hlYWQifX19XSxbNiwzLCJxIFxcXFwgXFx0ZXh0eyhlcnJvciBpbiBmKX0iLDIseyJsYWJlbF9wb3NpdGlvbiI6NzAsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMSw2LCIiLDEseyJjb2xvdXIiOlsxMjAsNjAsMzBdfV0sWzIsNiwicyBcXGNkb3QgciBcXFxcIFxcdGV4dHsoYm91bmRlZCBieSBmdW5jdGlvbiBzZW5zaXRpdml0eSB9cykiLDIseyJsYWJlbF9wb3NpdGlvbiI6NjAsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=
\[\begin{tikzcd}
	x &&& {\tilde{x}} \\
	&&&& {f(\tilde{x})} \\
	{f(x)} &&&&&&&& {\widetilde{f(x)}} \\
	{} &&&&&&&& {}
	\arrow["\begin{array}{c} \text{(perturbation in function inputs)} \\ r \end{array}", no head, from=1-1, to=1-4]
	\arrow[color={rgb,255:red,31;green,122;blue,31}, from=1-1, to=3-1]
	\arrow[color={rgb,255:red,31;green,122;blue,31}, from=1-4, to=2-5]
	\arrow[color={rgb,255:red,206;green,59;blue,59}, from=1-4, to=3-9]
	\arrow["\begin{array}{c} q \\ \text{(error in f)} \end{array}"'{pos=0.7}, no head, from=2-5, to=3-9]
	\arrow["\begin{array}{c} s \cdot r \\ \text{(bounded by function sensitivity }s) \end{array}"'{pos=0.6}, no head, from=3-1, to=2-5]
	\arrow["\begin{array}{c} s \cdot r + q \\ \text{(triangle inequality, final bound)} \end{array}"', tail reversed, from=4-1, to=4-9]
\end{tikzcd}\]
\end{figure}

However, subtraction (and adding by a negative number) is infinitely-sensitive,
which poses a challenge for a compositional analysis that relies on functions
with finite senstivity bounds. The fundamental problem with adding negative
numbers and subtraction is that those operations are infinitely-sensitive;
relatively small peturbations in the input can lead to hard-to-bound relatively
large perturbations in the output. As an illustrative toy example, suppose we
have two large numbers like 1,000,001 and 1,000,000 that we wish to subtract. If
we perturb the input by a small amount, for example, by 1 per-cent, we can
observe quite large \textit{relative} changes in the output. 

% As an illustrative toy example, suppose we
% have two large numbers like 1,000,001 and 1,000,000 with little relative error.
% Suppose that the relative error bounds for both large numbers is ``small" at 1
% per cent. Then the relative error for $1,000,001 - 1,000,000 = 1$ is quite
% large, upper-bounded at roughly 2,000,000 per cent.

% The problem is essentially:
% compositional type-based error bounds seem to require relative notions of error;
% however, relative notions of error are not well-behaved in the presence of
% subtraction and negative numbers. 
% This fundamental phenomenon is known as \textit{catastrophic cancellation}: two
% large nearby numbers with small relative error can be subtracted to produce a
% small number with arbitrarily high relative error.

% When catestrophic cancellation occurs, it is impossible to obtain tight error
% bounds. Importantly, we often know when catestrophic cancellation cannot not
% occur (\textit{a priori}) and can detect when it has occured (\textit{a
% posteori}). \textit{A posterori} error bounds are particularly promising as
% catestrophic cancellation only occurs when two large numbers are subtracted to
% produce a \textit{small} number with arbitrarily high error. Knowing that the
% final value is large allows for the ruling out of catestrophic cancellation,
% sidestepping the problem entirely.
% In both circumstances, our approach is able to reason about the presence of
% catestrophic cancellation (or rule it out) in order to produce useful error
% bounds.

\paragraph{Our approach} 
The techniques detailed in this paper centers around representing each possibly
negative number, $r$, in our type semantics with a pair of non-negative $(a,
b)$. When our semantics adds $r = (a, b)$ and $r' = (a', b')$ we produce $r + r'
= (a + a', b + b')$. When our semantics subtracts two numbers $r - r' = (a + b',
b + a')$, our paired type represenation only needs to add components;
subtraction is represented implictly. Distance between $(a, b)$ and $(a', b')$
is encoded as the maximum of the pairwise distance: $max(d(a, a'), d(b, b'))$.
In this manner, we encode subtraction and negative numbers in a finite-sensitive
manner. We combine this representation with a polymorphic intervals bounds
analysis in the type system to track the pairs and provide both $\textit{a
priori}$ and $\textit{a posteori}$ error bounds. Importantly, the operational
semantics of the programming language remains standard and unpaired and the
interval analysis can be automatically inferred via type inference. We
additionally prove that our approach provides no looser error bounds than prior
type-based approaches while covering many more programs, including those with
subtraction and negative numbers. We defer the details of our paired approach to
Section~\ref{sec:encoding} and type inference to Section~\ref{sec:inference}. 
% max: maybe refer to sec 5 (factor) above too?

We address the limitations of prior work by providing a scalable, type-based
forwards error analysis that can handle broader classes of programs and, at the
same time, more accurately structure and sequence arithemetic while tightening
error bounds. Our approach is faster than competing approaches, mostly automatic
with low type annotation overhead, and competitive on precision with other
state-of-the-art techniques. Concretely, our contributions are as follows:
\begin{itemize}
  \item We extend the Numerical Fuzz family of languages by incorporating an
    interval-style analysis into the type system using interval \textit{bound
    polymorphism}. We further extend Numerical Fuzz to enable expressions in
    more places. We prove the soundness of our extensions to Numerical Fuzz in
    (Section~\ref{sec:lang}).

  \item We encode Numerical Fuzz to use a \textit{paired representation} that
    allows for subtraction and negative numbers to be soundly represented in a
    finitely-sensitive manner (Section~\ref{sec:instantiation}). We then prove
    both \textit{a priori} and \textit{a posterori} compositional error bounds
    in the presence of subtraction and negative numbers
    (Section~\ref{sec:application}). 

    % todo: some claim about novelty is needed to put this contribution in context
    % to develop the first type-based approach to forwards error analysis that
    % enables
    % Our instantiation relies on a \textit{paired numeric representation}. 
    % To our knowledge, our approach is the first automatic general-purpose
    % numerical analysis that can produce usable \textit{a posteori} error bounds.

  \item We enable the structuring and seqeuncing of arithmetic for tighter error
    bounds by allowing error and sensitivities in rounded terms to be shared. We
    accomplish this through the addition of a previously-untypable primitive
    inspired by the resource interpretation of linear logic
    (Section~\ref{sec:structure})

  \item We develop a type inference algorithm for our type system that can
    automatically infer bound polymorphism in many useful programs; the user can
    provide an entirely unannotated program for inference. We prove the
    soundness of our type inference algorithm. We further prove that our
    approach leads to forwards error bounds no looser than prior type-based work
    for forwards analysis (Section~\ref{sec:tightness}). 

  \item We implement our type-based approach for forwards error analysis. We
    evaluate our implementation against FPTaylor and Gappa on a suite of
    benchmarks translated into our core language and demonstrate that we obtain
    competitive error bounds with often faster performance
    (Section~\ref{sec:eval}).
    % todo: put in hard numbers
\end{itemize}
