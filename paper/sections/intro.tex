\section{Introduction}
It is natural for ordinary programmers such as mathematicians and scientists to
wish their computers operate over the reals. 
But this is impossible and programming languages typically compute over the
finitary floating-point number approximation.
As language designers, we wish to convince the ordinary programmer, who are
largely not experts in numerical analysis, that the floating-point semantics we
have provided is ``close enough" to what they desire.
Towards this end, we develop automated numerical analyses to bound the error
introduced by our floating-point approximations. We wish for our analyses to be
efficient, as automatic as possible, and precise.

One major challenge in the automated numerical analysis literature is
scalability. Many existing approaches rely on global optimization
\cite{fptaylor} \cite{satire}, rewrite saturation \cite{gappa}, or SMT-based
methods \cite{rosa}. However, as programs scale, these analysis approaches
frequently time-out. Recent work has applied typed-based analysis approaches to
both forwards and backwards error analysis, such as Numerical Fuzz
\cite{numfuzz} and Bean \cite{bean}. The advantage of type-based methods is that
they are inherently compositional and scalable: all of the information necessary
to perform an error analysis on a function is contained within the function
type. There is no need to perform global optimization, run an algorithm to
convergence, saturate rewrites, or bit-blast.

However, prior type-based work for forwards error analysis is not capable of
handling negative numbers and subtraction. The problem is essentially:
compositional type-based error bounds seem to require relative notions of error;
however, relative notions of error are not well-behaved in the presence of
subtraction and negative numbers. As an illustrative toy example, suppose we
have two large numbers like 1,000,001 and 1,000,000 with possible error. Suppose
that the relative error bounds for both large numbers is at 1 per cent. Then the
relative error for $1,000,001 - 1,000,000 = 1$ is quite large, upper-bounded at
roughly 2,000,000 per cent.
This fundamental phenomenon is known as \textit{catastrophic cancellation}: two
large nearby numbers with small relative error can be subtracted to produce a
small number with arbitrarily high relative error. 

When catestrophic cancellation occurs, it is impossible to obtain tight error
bounds. Importantly, we often know when catestrophic cancellation cannot not
occur (\textit{a priori}) and can detect when it has occured (\textit{a
posteori}). \textit{A posterori} error bounds are particularly promising as
catestrophic cancellation only occurs when two large numbers are subtracted to
produce a \textit{small} number with arbitrarily high error. Knowing that the
final value is large allows for the ruling out of catestrophic cancellation,
sidestepping the problem entirely.
In both circumstances, our approach is able to reason about the presence of
catestrophic cancellation (or rule it out) in order to produce useful error
bounds.

Our approach centers around representing each possibly negative number, $r$, in
our type semantics with a pair of non-negative $(a, b)$. When our semantics adds
$r = (a, b)$ and $r' = (a', b')$ to produce $r + r' = (a + a', b + b')$. When
our semantics subtracts two numbers $r - r' = (a + b', b + a')$, our paired type
represenation only needs to add components; subtraction is represented
implictly. We use this representation combined with an intervals bounds analysis
using type polymorphism to track the pairs and provide both $\textit{a priori}$
and $\textit{a posteori}$ error bounds. Importantly, the operational semantics
of the programming language remains standard and unpaired. We additionally prove
that our approach provides no looser error bounds than prior type-based
approaches while covering many more programs, including those with subtraction
and negative numbers.
We defer the details of this approach to Section ?. 
% todo: fill in section with ref

We also address the above limitations by providing a scalable, type-based
forwards error analysis that can handle broader classes of programs and, at the
same time, tighten error bounds. Our approach is faster than competing
approaches, mostly automatic with low type annotation overhead, and competitive
on precision with other state-of-the-art techniques. Concretely, our
contributions are as follows:
\begin{itemize}
  \item We extend Numerical Fuzz by introducing a previously-untypable primitive
    that enables the error and sensitivities in rounded terms to be shared,
    allowing for tighter error bounds. We also extend Numerical Fuzz to enable
    expressions in more places. We prove the soundness of these extensions to
    Numerical Fuzz.

  \item We develop a technique to incorporate interval analysis into the type
    system using interval \textit{bound polymorphism}. We provide a type
    inference and checking algorithm for bound polymorphism and prove soundness.

  \item We develop the first type-based approach to forwards error analysis that
    enables both \textit{a priori} and \textit{a posterori} compositional error
    bounds in the presence of subtraction and negative numbers. We prove that
    our approach, which can handle strictly more programs, leads to forwards
    error bounds no looser than prior type-based work for forwards analysis.

  \item We implement our type-based approach for forwards error analysis. We
    evaluate our implementation against FPTaylor and Gappa on a suite of
    benchmarks translated into our core language and demonstrate that we obtain
    competitive error bounds with often faster performance.
    % todo: put in hard numbers
\end{itemize}
