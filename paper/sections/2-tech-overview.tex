\section{Technical Overview} \label{sec:overview}
In this paper, we are primarily motivated to extend Numerical Fuzz to support
more floating-point operations and richer ways of structuring and sequencing
arithmetic. We provide some background and briefly overview our extensions
below.

% In this section, we briefly introduce floating-point round-off error and
% outline the main technical features of our system. 
\subsection{Background: floating-point and error metrics}
The floating-point number system is a family of formats with the following
setup:
\begin{equation}
  x = (-1)^s \cdot m \cdot \beta^{e - p + 1}
\end{equation}
where $s$ controls the sign, $m$ is the mantissa, $e$ is the exponent, and $p$
is the precision. A specific floating-point number system fixes a particular
precision $p$ and represents a subset of the reals, for example
$\textsc{binary32}$ and $\textsc{binary64}$ which fix $p = 32$ and $p = 64$
respectively. 
Operations over the reals can be represented in a floating-point system as the
exact operation followed by rounding by a function $\rho$ to a float. The error
introduced by the rounding function $\rho$ is known as \textit{round-off error}.

For this paper, we assume the absence of overflow and underflow. Then, the
standard rounding error model assumes $\rho$ will observe round-off error in at
most the last bit:
\begin{equation}
  \rho(x) = x (1 + \delta) \quad \quad |\delta| \leq u
\end{equation}
where $u$ is the \textit{unit round-off} value representing the maximum error
possible in the last bit for the floating point format (e.g. $2^{-24}$ and
$2^{-53}$ for 32 and 64-bit floating-point numbers respectively).

We are interested in bounding round-off error. 
To bound error, one must first have a suitable definition for measuring error.
Following Numerical Fuzz, we rely on Olver's \cite{olver} \textit{relative
precision} metric for measuring round-off error. 

\begin{equation}
  d_{\mathbb{R}}(r, \tilde{r}) = | ln(\frac{r}{\tilde{r}})|
\end{equation}

Our approach relies on triangle inequality for the soundness of the error grades
for our charateristic monadic bind rule.
As relative precision forms a true metric and satisfies triangle equality, we
are able to relative precision to compositionally reason about repeated
applications of the monadic bind rule.
Importantly, triangle inequality does not hold for \textit{relative error}.

\subsection{Language Feature: Supporting Subtraction and Negative Numbers}
% TODO: mention that this is forwards error above maybe? in contrast with bean?
% maybe explain fowards vs backwards
We wish to support a forwards error analysis on programs containing subtraction
or negative numbers. 
Unforutantely, Numerical Fuzz cannot assign a type to infinitely sensitive
functions and subtraction over the reals is infinitely sensitive. 
Stated in terms of numerical analysis, subtraction between two possibly close
numbers is ill-conditioned. 
The core problem is that relatively small peturbations in the
input of subtraction can lead to hard-to-bound relatively large perturbations in
the output. As an illustrative toy example, suppose we have two large numbers
like 1,000,001 and 1,000,000 that we wish to subtract. If we perturb the input
by a small amount, for example, by 1 per-cent, we can observe quite large
\textit{relative} changes in the output. 
In other words, subtracting two large nearby numbers could obtain a small number
with arbitrarily high relative error.
This feature of subtraction is sometimes referred to as \textit{catestrophic
cancelation} and forms the primary technical obstacle that this paper aims to
overcome.

Let us examine the problem of subtraction in greater detail from the perspective
of a type system designer. Consider the minimum program containing subtraction
as a running example:
$\lambda x \ . \ \lambda y \ . \ \textbf{let-bind} \ z \ = \ \textbf{rnd} \ x \ \textbf{in} \ \textbf{sub} \ \langle z, y \rangle$ 
under the restriction that $x \in [1, 100]$ and $y \in [1000, 10000]$. 
% TODO: more intuition
We first demonstrate the core of our technical solution and assign a type in
Negative Fuzz to a simplified toy example. Then, we will demonstrate how to
obtain a useful error bound from the program type from the example.

\paragraph{Standard representation.}
Our type system relies on bounding function senstivity. 
We first attempt to assign a type to a hypothetical subtraction primitive in
Numerical Fuzz:
$\textbf{sub} : !_{\infty} (\textbf{num} \times \textbf{num}) \multimap \textbf{num}$.
Then, the type of our running example is 
$!_{\infty} (\textbf{num} \times \textbf{num}) \multimap M_{\infty}~\textbf{num}$.
Our type comes from two facts.
Firstly, due to $\textbf{rnd}$, we know that there can be up to $u$ error
introduced in the input $z$ to $sub$.
Secondly, function sensitivity bounds the round-off error of the whole program
at $\infty \cdot u = \infty$ where $u$ is our unit-roundoff constant.
Therefore, we know that our example program is both infinitely-sensitive and
has infintely-bounded error.
This is not a useful error bound.

\paragraph{Paired representation.}
As subtraction over the reals is infinitely-sensitive, we first deal with the
problem of infinite sensitivity.
We develop a paired representation where subtraction and negative
numbers have finitely-bounded sensitivty. The trick is to associate for each
real $r \in \mathbb{R}$ a triple $(r, a, b) \in \mathbb{P} = \mathbb{R} \times
\mathbb{R}^+ \times \mathbb{R}^+ = \mathit{num}$ such that $r = a - b$.
The paired components $a$ and $b$ are only used for error analysis and serve a
function similar to ghost variables in other program analyses.
Our error function $d_{\mathbb{P}}((r, a, b)(\tilde{r}, \tilde{a}, \tilde{b}))$
over $\mathbb{P}$ is defined below:
\begin{definition}[Distance over pairs] \label{def:distance-p}
$d_{\mathbb{P}}((r, a, b)(\tilde{r}, \tilde{a}, \tilde{b})) = max(d_{\mathbb{R}}(a, \tilde{a}), d_{\mathbb{R}}(b, \tilde{b}))$.
\end{definition}

Importantly, we can ``simulate" error on the paired representation by defining a
paired version of our rounding function $\rho$:
\begin{definition}[Rounding over pairs] \label{def:rounding-p}
$\rho_{\mathbb{P}}((r, a, b)) = 
\begin{cases}
  (0, a, b)  & r = 0\\
(\rho_{\mathbb{R}}(r), a \cdot \frac{\rho_{\mathbb{R}}(r)}{r}  b \cdot
  \frac{\rho_{\mathbb{R}}(r)}{r}), & r \not=0 \\
\end{cases}
$
\end{definition}
which satisfies the constraint that $r = a - b$ at all times.
This constraint is useful for letting us relate error over the unpaired
components to error on the paired component.

Under the paired representation, we can implement and faitfully type addition,
subtraction, and multiplication in terms of finitely-sensitive operations over
the paired components:
\begin{equation}
  \begin{aligned}[c]
    sub((r, a, b), (r', a', b')) &\mapsto (r - r', a + b', a' + b) \\
    add((r, a, b), (r', a', b')) &\mapsto (r + r', a + a', b + b') \\
    mul((r, a, b), (r', a', b')) 
      &\mapsto 
    (r * r', a * a' + b * b', a * b' + a' * b)
  \end{aligned}
\end{equation}
In this manner, we can bound the sensitivity of each operation and soundly
assign a type. 
$\textbf{add}$ and $\textbf{mul}$ 
have the same type as in Numerical Fuzz. Subtraction has type as
addition: $\textbf{sub} : \textbf{num} \times \textbf{num} \multimap
\textbf{num}$. 
Returning to our running example, we can assign the type 
$\textbf{num} \times \textbf{num} \multimap M_u \textbf{num}$ to the program 
$\lambda x . \ \textbf{let} \ z \ = \ \textbf{sub} \ x \ \textbf{in} \ \textbf{rnd} \ z$
where $u$ is the unit round-off constant.

\paragraph{Bound polymorphism.}
Now that we have a way to bound error on the paired components, we need to
translate these bounds to state something useful over the unpaired component.
% Unfortuantely, the error bounds obtained through the paired representation only
% bound the maximum error on the \textit{paired} components. 
% To be useful, we wish to relate error on the paired components $a$, $b$ with the
% error on the \textit{unpaired} component $r$.
We can obtain bounds on $r$ by recalling the invariant that for a triple $(r, a,
b)$ that $r = a - b$.
If we have bounds on $a$ and $b$, we can take advantage of our invariant.
Simplifying for presentational purposes, we can obtain interval bounds on $r$ by
extending the type system to be able to perform an interval analysis on the
paired components. In our running example, we have that $x \in [1, 100]$ and $y
\in [1000, 10000]$. 
The bounds analysis in our type inference algorithm will assign the type $M_u
\textbf{num}_{\bnd{(900, 9999)}}$ to the program, where the subscript denotes
the interval the output will belong to.
Further, we can duduce from the inferred grade of our monad that for all inputs
$x, y$ to our input program, for an output value $(r, a, b), (\tilde{r},
\tilde{a}, \tilde{b})$, that $max(d(a, \tilde{a}) d(b, \tilde{b})) \leq u$.
Armed with the output of type inference, we can apply our main error theorem,
Theorem~\ref{thm:paired-a-priori}, for 32-bit floats:
% TODO: check this calculation
$d(r, \tilde{r}) 
= max(|ln(e^u + \frac{100}{900}(e^{-u}-e^u))|, |ln(e^{-u} + \frac{100}{900}(e^u-e^{-u}))|) 
= 4.635916 \times 10^{-8}$. 
% https://www.wolframalpha.com/input?i2d=true&i=max%5C%2840%29%7Cln%5C%2840%29Power%5Be%2C-%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D+%2B+Divide%5B10%2C900%5D%5C%2840%29Power%5Be%2C%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D-Power%5Be%2C-%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D%5C%2841%29%5C%2841%29%7C%5C%2844%29%7Cln%5C%2840%29Power%5Be%2C%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D+%2B+Divide%5B10%2C900%5D%5C%2840%29Power%5Be%2C-%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D-Power%5Be%2C%5C%2840%29Power%5B2%2C-24%5D%5C%2841%29%5D%5C%2841%29%5C%2841%29%7C%5C%2841%29
% Another piece of information that tightens our error analysis is knowing how big
% our output value is. For example, if we knew that $\tilde{r} = 1000$, then we
% can apply Theorem~\ref{thm:paired-a-posteriori} and obtain the following tighter
% \textit{a posterori} bound: TODO.
In Section~\ref{sec:encoding}, we present a more generalized and sophisticated
version of the bounds analysis given in the example above.

% TODO: justin says "go slower here, connect to numfuzz type, space out, break
% up paragraphs. this part is important to help readers understand how our
% system works"

We now consider how to compositionally introduce a precise bounds analysis to
the type system. A naive approach to incorporating a bounds analysis in the type
system would be to annotate each $\textbf{num}_{\bnd{i}}$ with a subscript
$\bnd{b}$. However, many programs we wish to type call the same
function many times. 
For example, if we have a function such as the identity function over numbers
$id : \textbf{num}_{\bnd{b}} \multimap \textbf{num}_{\bnd{b}}$ called at two
different call sites with bounds $b_0$ and $b_1$, the tightest sound bound for
$b$ would be an upper bound on $b_0$ and $b_1$.

To ensure that our bounds analysis is scalable and that functions types can be
reused, we support \textit{bound polymorphism}, which allows functions to be
specialized with different concrete bounds $\bnd{b}$ at each call site. We write
types $\tau$ polymorphic in bound variable $\bnd{\epsilon}$ as $\bnd{\forall
\epsilon.} \tau$.
By adding \textit{bound polymorphism}, we allow functions to be
typed like so: $id : \bnd{\forall \epsilon.} ~ \textbf{num}_{\bnd{\epsilon}}
\multimap \textbf{num}_{\bnd{\epsilon}}$. We can introduce and eliminate
polymorphism via $\forall$ introduction and elimination rules.
% provided in Figure~\ref{fig:typing_rules}.
To reduce annotation overhead, we provide a type inference algorithm that can
automatically infer bound polymorphism abstraction and instantiation sites. We
prove our type inference algorithm sound in Section~\ref{sec:inference}. 

% TODO: awk phrasing I think, can be tighter. forwards reference to section
% where we do this once this is written
A natural question to ask is whether our approach of first using a paired
representation, then running a bounds analysis to recover the error on the
original unpaired representation hurts the precision of our analysis on
programs. Interestingly, we do not lose any precision. We provide several
examples showing that our bounds analysis is capable of reasoning that programs
containing only-growing functions on non-negative numbers can only produce
non-negative numbers. Therefore, on the programs supported by both Negative Fuzz and
Numerical Fuzz, the maximum round-off error obtained by Negative Fuzz is simply
the inferred grade of the monad and is no looser than the bound obtained by
Numerical Fuzz. Negative Fuzz is therefore a strict improvement on Numerical
Fuzz: our approach can analyze more programs with possibly tighter bounds on
previously supported programs.

\subsection{Language Feature: Precise Structuring and Sequencing of Arithmetic} 
\label{sec:factor}
Suppose, by way of example, that we wish to compute the sum
$w~\tilde{+}~x~\tilde{+}~y~\tilde{+}~z$, where $\tilde{+}$ represents addition
with round-off error. It is well known that round-off error grows linearly in
the height of a summation tree. So, a user might naturally want to sequence the
summation as a perfect binary tree: $(w~\tilde{+}~x)~\tilde{+}~(y~\tilde{+}~z)$.
In prior type-based work \cite{numfuzz}, the monadic computation sequencing rule
forces the error analysis to always compute the worst-case round-off error
associated with the pathological degenerate tree:
$((w~\tilde{+}~x)~\tilde{+}~y)~\tilde{+}~z$.
In other words, the error bound obtained by \cite{numfuzz} grows linearly in
size of the number of \textit{nodes}.

Our extension enables the user to associate the computation tree arbitrarily,
and to obtain tighter error bounds that grow linearly in size of the \textit{height} of
the tree.
We accomplish this by including a special primitive $\textbf{factor}$ in our
term language inspired by linear logic. Stucturing the example arithmetic into a
program below, we can see that using factor results in a 50\% smaller error
bound for a program representing the same association and order of operations.

\begin{figure}[ht] \label{fig:factor-cmp-overview}
\centering

\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
// (w + x) + (y + z) : M[3*u] num 
let-bind a = addfp <w, x> in
let-bind b = addfp <y, z> in
let-bind c = addfp <a, b> in
addfp c 
\end{lstlisting}
\caption{Without factor: error bound is $3u$.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
// (w + x) + (y + z) : M[2*u] num 
let-bind a = 
  factor <addfp <w, x>, addfp <y, z>> in
  addfp a
\end{lstlisting}
\caption{With factor: error bound is $2u$.}
\end{subfigure}

\caption{Side-by-side comparison with and without the \textbf{factor} primitive.}
\label{fig:factor-side-by-side}
\end{figure}

More generally, for perfect binary trees of addition, we have a $log_2 (n)$
factor improvement on our error bounds. We defer a detailed explaination of
\textbf{factor} and how it allows for structuring and sequencing arithmetic
towards a more precise error analysis in Section~\ref{sec:structure}. We also
showcase the improved precision offered by programs annotated with the
\textbf{factor} primitive in Section~\ref{sec:eval}.
