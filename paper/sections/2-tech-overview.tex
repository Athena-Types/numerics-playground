\section{Technical Overview} \label{sec:overview}
In this paper, we are primarily motivated to extend Numerical Fuzz to support
more floating-point operations and richer, more precise ways to reason about
addition and subtraction. We call our extension Negative Fuzz because it
supports negative numbers and subtraction. We first provide some background and
our motivating running example. Then, we overview our extensions.

% In this section, we briefly introduce floating-point round-off error and
% outline the main technical features of our system. 
\subsection{Background: floating-point and error metrics}
The binary floating-point number system is a family of formats with the
following setup:
\begin{equation}
  x = (-1)^s \cdot m \cdot 2^{e - p + 1}
\end{equation}
where $s$ controls the sign, $m$ is the mantissa, $e$ is the exponent, and $p$
is the precision. A specific floating-point number system fixes a particular
precision $p$ and represents a subset of the reals, for example
$\textsc{binary32}$ and $\textsc{binary64}$ which fix $p = 32$ and $p = 64$
respectively.
Arithmetic operations with round-off error can be represented in a
floating-point system as the exact operation followed by rounding to a float by
a rounding function (written $\rho$).
The error introduced by the rounding function $\rho$ is known as
\textit{round-off error}.
The choice of rounding function will vary depending on the rounding mode (e.g.
round-to-nearest, round-towards-infinity, etc.) and the precision used.

For this paper, we assume the absence of overflow and underflow. Then, the
standard floating-point rounding error model assumes $\rho$ will observe
round-off error bounded like so: 
\begin{equation}
  \rho(x) = x (1 + \delta) \quad \quad |\delta| \leq u
\end{equation}
where $u$ is the \textit{unit round-off} value representing the maximum error
possible in the last bit for the floating point format (e.g. $2^{-24}$ and
$2^{-53}$ for 32 and 64-bit floating-point numbers respectively).

We are interested in bounding round-off error. 
To bound error, one must first have a suitable definition for measuring error.
Following Numerical Fuzz, we rely on Olver's \cite{olver} \textit{relative
precision} metric for measuring round-off error:
$d_{\mathbb{R}}(r, \tilde{r}) = | ln(\frac{r}{\tilde{r}})|$.

% Our approach relies on triangle inequality for the soundness of the error grades
% for our charateristic monadic bind rule.
% As relative precision forms a true metric and satisfies triangle equality, we
% are able to relative precision to compositionally reason about repeated
% applications of the monadic bind rule.
% Importantly, triangle inequality does not hold for \textit{relative error}.

\subsection{Running example: pairwise summation}
We consider the well-known pairwise summation (sometimes referred to as cascade
summation) algorithm. 
As a concrete running example, we consider adding up the floating-point numbers
$w, x, y, z \in [-1, 1]$ via pairwise summation: 
$\rho(\rho((w + x)) + \rho((y + z)))$.

\subsection{Supporting Subtraction and Negative Numbers}
Numerical Fuzz can handle inputs in the range $[0, 1]$ but cannot handle our
example which includes negative numbers. 
This is because subtraction in Numerical Fuzz and addition over the unrestricted
reals is infinitely sensitive. 
% Stated in terms of numerical analysis, subtraction between two possibly close
% numbers is ill-conditioned. 
The core problem is that relatively small perturbations in the
input of subtraction can lead to hard-to-bound relatively large perturbations in
the output. 
As an illustrative toy example, suppose we have two large numbers
like 1,000,001 and 1,000,000 that we wish to subtract. 
If we perturb the input by a small amount, we can observe quite large
\textit{relative} changes in the output. 
In other words, subtracting two large nearby numbers with small round-off error
in its inputs could obtain a small number with arbitrarily high relative
round-off error.
This feature of subtraction is sometimes referred to as \textit{catastrophic
cancellation} and forms one of the primary technical obstacles to verifying our
running example. 

% \begin{enumerate}
%   \item A way to ``simulate" round-off error on $r$ as error on the pair $(a,
%     b)$. We do this by taking the rounding function $\rho(r)$ and defining a
%     corresponding rounding function $\rho_{\mathbb{P}}(r, a, b) = (\rho(r),
%     \tilde{a}, \tilde{b})$ such that $\rho(r) = \tilde{a} - \tilde{b}$.
%   \item A way to measure error on the paired components. We define distance
%     between pairs $(r_0, a_0, b_0)$ and $(r_1, a_1, b_1)$ as the maximum of the
%     pairwise distance: $max(d(a_0, a_1), d(b_0, b_1))$. 
%   \item A way to ``translate" the distances between pairs $(r, a, b)$ and
%     $(\tilde{r}, \tilde{a}, \tilde{b})$ to be a suitable bound on the
%     floating-point error on $r$. 
%     We do this by integrating an interval-style bounds analysis into the type
%     system. 
% \end{enumerate}
Note that supporting addition with negative numbers is effectively the same
problem as supporting subtraction.
To understand the problem posed by an infinitely-sensitive operation, we try to
write our running example using $\textbf{rnd}$ to represent our floating point
rounding function $\rho$, \textbf{add} to represent exact addition ($+$) and
\textbf{let-bind} to sequence floating-point computation:
\begin{equation}
\begin{aligned}[c]
\textbf{let-bind} \ q \ = \ \textbf{rnd} \ (\textbf{add} \ \langle w, x \rangle) \ \textbf{in} \  \\
\textbf{let-bind} \ p \ = \ \textbf{rnd} \ (\textbf{add} \ \langle y, z \rangle) \ \textbf{in} \  \\
\textbf{rnd} \ (\textbf{add} \  \langle q, p \rangle  )
\end{aligned}
\end{equation}
under the restriction that $w, x, y, z \in [-1, 1]$. 
% TODO: more intuition
We will show how the program cannot be typed in Numerical Fuzz.
We will first assign a type in Negative Fuzz to a simplified toy example. Then,
we will demonstrate how to use the type to obtain a sound error bound.

\paragraph{Standard representation.}
Numerical Fuzz's type system relies on bounding function sensitivity. 
We first attempt to assign a type to a hypothetical addition primitive in
Numerical Fuzz over the unrestricted reals:
$\textbf{add} : !_{\infty} (\textbf{num} \times \textbf{num}) \multimap \textbf{num}$.
Then, the type of our running example is 
$M_{\infty}~\textbf{num}$
in the context 
$w : \textbf{num}, x : \textbf{num}, y : \textbf{num}, z : \textbf{num}$.
\footnote{For presentational purposes, we elide the sensitivity co-effect
annotation in the typing context. The program is infinitely-sensitive in
variables $w,x,y,z$. }
Our type comes from two facts.
Firstly, due to $\textbf{rnd}$, we know that there can be up to $u$ error
introduced in the inputs $p$ and $q$ to the last $add$ call.
Secondly, since the $add$ function may amplify error in its input by up to
$\infty$, the round-off error of the whole program is $\infty \cdot u = \infty$
where $u$ is our unit-roundoff constant.
Therefore, we know that our example program is both infinitely-sensitive in its
free variables and has infinitely-bounded error.
This is not a useful error bound.

\paragraph{Paired representation.}


As addition and subtraction over the unrestricted reals is infinitely-sensitive,
we first deal with the problem of infinite sensitivity.

We develop a paired representation where unrestricted addition and subtraction
have finitely-bounded sensitivity. The trick is to associate for each
real $r \in \mathbb{R}$ a triple $(r, a, b) \in \mathbb{P} = \mathbb{R} \times
\mathbb{R}^+ \times \mathbb{R}^+ = \mathit{num}$ such that $r = a - b$.
The paired components $a$ and $b$ are only used for error analysis and serve a
function similar to ghost variables in other program analyses.
Our error function $d_{\mathbb{P}}((r, a, b), (\tilde{r}, \tilde{a}, \tilde{b}))$
over $\mathbb{P}$ is defined as
$d_{\mathbb{P}}((r, a, b), (\tilde{r}, \tilde{a}, \tilde{b})) \triangleq max(d_{\mathbb{R}}(a, \tilde{a}), d_{\mathbb{R}}(b, \tilde{b}))$.
% \begin{definition}[Distance over pairs] \label{def:distance-p}
% \end{definition}
Importantly, we can encode our operations, including addition and subtraction,
such that both components of the pair are always-growing. This avoids the
problem of catastrophic cancellation in the paired representation.
When our semantics adds $r_0 = (a_0, b_0)$ and $r_1 = (a_1, b_1)$, we produce
$r_0 + r_1 = (a_0 + a_1, b_0 + b_1)$. When we subtract two numbers $r_0 - r_1 =
(a_0 + b_1, b_0 + a_1)$, our paired type representation only needs to add
components; subtraction is represented implicitly. 
By ensuring that each operation in the paired representation is always-growing,
we can encode addition and subtraction in a finitely-sensitive manner. 
%
% We can ``simulate" error on the paired representation by defining a
% paired version of our rounding function $\rho$:
% \begin{definition}[Rounding over pairs] \label{def:rounding-p}
% $\rho_{\mathbb{P}}((r, a, b)) = 
% \begin{cases}
%   (0, a, b)  & r = 0\\
% (\rho_{\mathbb{R}}(r), a \cdot \frac{\rho_{\mathbb{R}}(r)}{r}  b \cdot
%   \frac{\rho_{\mathbb{R}}(r)}{r}), & r \not=0 \\
% \end{cases}
% $
% \end{definition}
% which satisfies the constraint that $r = a - b$ at all times.
% This constraint is useful for letting us relate error over the unpaired
% components to error on the paired component.

Under the paired representation, we can implement and type addition and
subtraction in terms of finitely-sensitive operations over
the paired components:
\begin{equation}
  \begin{aligned}[c]
    add((r, a, b), (r', a', b')) &\mapsto (r + r', a + a', b + b') \\
    sub((r, a, b), (r', a', b')) &\mapsto (r - r', a + b', a' + b) \\
    % mul((r, a, b), (r', a', b')) 
    %   &\mapsto 
    % (r * r', a * a' + b * b', a * b' + a' * b)
  \end{aligned}
\end{equation}
In this manner, we can bound the sensitivity of addition and subtraction and soundly
assign a type. 
$\textbf{add}$ and $\textbf{mul}$ 
have the same type as in Numerical Fuzz. Subtraction has type as
addition: $\textbf{sub} : \textbf{num} \times \textbf{num} \multimap
\textbf{num}$. 
Returning to our running example, we can assign the type 
$\textbf{num} \times \textbf{num} \multimap M_u \textbf{num}$ to the program 
$\lambda x . \ \textbf{let} \ z \ = \ \textbf{sub} \ x \ \textbf{in} \ \textbf{rnd} \ z$
where $u$ is the unit round-off constant.

\paragraph{Using the type to assign an error bound.}
Now we wish to translate our type over the paired model into a error bound over
the unpaired model.
Our type system will assign the type $M_{3 \cdot u} \ \textbf{num}_{\bnd{((-4,
4), (0, 4), (0, 4))}}$ to the program.
In order to obtain a useful error bound, we need two pieces of information
derived from the program's type: (1) the result of an interval-style analysis on
the exact, unrounded versions of $r, a, b$, and (2) the relative precision bound
between the exact and approximate components between $d_{\mathbb{R}}(a,
\tilde{a})$ and $d_{\mathbb{R}}(b, \tilde{b})$, which comes from our $3u$ monad
grade.
%
% $3u$ monad grade provides (1) and the subscript provides (2) by ensuring the
% program result will be inside of the set .

% TODO: how exactly do we type add?
To obtain (1), we run an interval-style analysis on the exact versions of $r, a,
b$. We observe that in our running example, we have that the paired
representation of $w, x, y, z \in [-1, 1] \times [0, 1] \times [0, 1]$. Our
bounds analysis will take this input and determine that the ideal computation
has type $\textbf{num}_{\bnd{((-4, 4), (0, 4), (0, 4))}}$ ensures that the
program result will be within $[-4, 4] \times [0, 4] \times [0,
4]$ with the invariant for every triple $(r, a, b)$ the equation $r = a - b$
  holds.
% If we have bounds on $a$ and $b$, we can take advantage of our invariant.
% Simplifying for presentational purposes, we perform an interval-style analysis.
To obtain (2), we can deduce from the $3u$ grade of our monad to conclude that
for all inputs $w, x, y, z$ to our input program, for an output value $(r, a,
b), (\tilde{r}, \tilde{a}, \tilde{b})$, that $max(d(a, \tilde{a}), d(b,
\tilde{b})) \leq 3 \cdot u$. 
Combining (1) and (2), we can apply our main absolute error theorem,
Theorem~\ref{thm:paired-a-priori-abs}, for 64-bit floats and obtain the absolute
error bound: $r - \tilde{r} = 2.6645352591003757 \times 10^{-15}$. 
%

% TODO: justin says "go slower here, connect to numfuzz type, space out, break
% up paragraphs. this part is important to help readers understand how our
% system works"
\paragraph{Bound polymorphism.}
It is challenging to assign a single, most precise type to $\textbf{add}$.
The difficulty arises because $\textbf{add}$ is called multiple times in
different contexts.
If we look at the first two add calls, we would try to assign the type: 
$\textbf{add} : \textbf{num}_{\bnd{((-1, 1), (0, 1), (0, 1))}} \times
\textbf{num}_{\bnd{((-1, 1), (0, 1), (0, 1))}} \multimap
\textbf{num}_{\bnd{((-2, 2), (0, 2), (0, 2))}}$.
This type would make it impossible to type the second add call.
On the other hand, if we look at the last add call, we would try to assign the
type: 
$\textbf{add} : \textbf{num}_{\bnd{((-2, 2), (0, 2), (0, 2))}} \times
\textbf{num}_{\bnd{((-2, 2), (0, 2), (0, 2))}} \multimap
\textbf{num}_{\bnd{((-4, 4), (0, 4), (0, 4))}}$
However, this would make our analysis imprecise in our first two calls to
$\textbf{add}$.

To ensure that our bounds analysis is scalable and that functions types can be
reused, we support \textit{bound polymorphism}, which allows functions to be
specialized with different concrete bounds $\bnd{b}$ at each call site. We write
types $\tau$ polymorphic in bound variable $\bnd{\epsilon}$ as $\bnd{\forall
\epsilon.} \tau$.
By adding \textit{bound polymorphism}, we allow functions to be
typed like so: $id : \bnd{\forall \epsilon.} ~ \textbf{num}_{\bnd{\epsilon}}
\multimap \textbf{num}_{\bnd{\epsilon}}$. We can introduce and eliminate
polymorphism via $\forall$ introduction and elimination rules.
% provided in Figure~\ref{fig:typing_rules}.
We also introduce \textit{bound operations}, which are meta-level operations
that let us type:
$$
\textbf{add} : \bnd{\forall \epsilon_1, \epsilon_2}, \mathbf{num}_{\bnd{\epsilon_1}} \times
\mathbf{num}_{\bnd{\epsilon_2}} \multimap \mathbf{num}_{\bnd{\epsilon_1 + \epsilon_2}}
$$
To reduce annotation overhead, we provide a type inference algorithm that can
automatically infer bound polymorphism abstraction and instantiation sites. We
prove our type inference algorithm sound in Section~\ref{sec:inference} and
provide an evaluation in Section~\ref{sec:eval}.

\subsection{Language Feature: More Precise Treatment of Addition and Subtraction} 
\label{sec:factor}
Returning to our running example, we would expect Numerical Fuzz to infer that
the error on our paired summation example has $2u$ error.
However, Numerical Fuzz will infer that the error on our paired summation
example has $3u$ error.
The issue is that the monadic computation sequencing rule forces the error
analysis to always compute the worst-case round-off error associated with the
most error-prone addition summation $\rho(\rho(\rho(w~+~x)~+~y)~+~z)$, which
yields an undesirable, overly-conservative error analysis.

More generally, we would expect Numerical Fuzz to infer that a binary tree of
height $h$ where each leaf contains a number that the sum of numbers over the
tree computed by recursively adding the children of each parent has
floating-point error growing in $O(h)$.
For example, consider the pairwise summation algorithm which will recursively divide a
list of numbers in half and compute the sum of the halves. This produces a
computation tree of height $\left \lceil{log_2(n)}\right \rceil$. We would
expect Numerical Fuzz's inferred error to grow in $O(log_2(n))$.
However, this is not the case; Numerical Fuzz instead infers error growing in
size $O(n)$.

Our extension enables the user to obtain more precise error bounds that
grow linearly in size of the \textit{height} of the computation tree, allowing
us to obtain a the expected $O(log_2(n))$ bound for pairwise summation.
We accomplish this by developing a special primitive $\textbf{factor}$ in our
term language. Rewriting our running example with our special primitive
below, we can see that using \textbf{factor} results in a significantly smaller
error grade for a program representing the same association and order of
operations. A smaller monadic error grade results in a smaller floating-point
error bound, reducing the error of our running example from $2.66 \times
10^{-15}$ to $1.77 \times 10^{-15}$.

\begin{figure}[ht] \label{fig:factor-cmp-overview}
\centering

\begin{subfigure}[t]{0.48\textwidth}
  \vspace{0.6em}
  \begin{equation*}
    \begin{aligned}[c]
      \textbf{let-bind} \ q \ = \ \textbf{rnd} \ (\textbf{add} \ \langle w, x \rangle) \ \textbf{in} \  \\
      \textbf{let-bind} \ p \ = \ \textbf{rnd} \ (\textbf{add} \ \langle y, z \rangle) \ \textbf{in} \  \\
      \textbf{rnd} \ (\textbf{add} \  \langle q, p \rangle  )
    \end{aligned}
  \end{equation*}
\caption{Without factor: the type assigned is $M_{3u} \ \textbf{num}$.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
  \begin{equation*}
    \begin{aligned}[c]
      \textbf{let-bind} \ a \ = \\ \textbf{factor} \
        \langle 
        \textbf{rnd} \ (\textbf{add} \langle w, x \rangle),
        \textbf{rnd} \ (\textbf{add} \langle y, z \rangle)
        \rangle \ \textbf{in} \\
        \textbf{rnd} \ (\textbf{add} \ a)
    \end{aligned}
  \end{equation*}
\caption{With factor: the type assigned is $M_{2u} \ \textbf{num}$.}
\end{subfigure}

\caption{Side-by-side comparison of addition with and without the
\textbf{factor} primitive. We omit the bounds subscripts for presentational
purposes.}
\label{fig:factor-side-by-side}
\end{figure}

More generally, for perfect binary trees of addition, we have a $log_2 (n)$
factor improvement on our error bounds when compared to Numerical Fuzz. We defer
a detailed explanation of \textbf{factor} and how it allows for structuring and
sequencing arithmetic towards a more precise error analysis in
Section~\ref{sec:structure}. We also showcase the improved precision offered by
programs annotated with the \textbf{factor} primitive in Section~\ref{sec:eval}.

% we would expect Numerical Fuzz to infer that the error on our paired summation
% example grows in $O(log_2(n))$
% It is well known that round-off error grows linearly in the height of a
% summation tree. 
% This would match our intuition 
%
% pairwise summation will recursively a list of numbers in half and compute the
% sum of the halves.
% This produces a computation tree of height $\left \lceil{log_2(n)}\right
% \rceil$ with error growing in .
% By contrast, naive iterative summation takes a list of numbers and sequentialy
% adds from left to right over the list. This produces a computation tree of
% height $O(n)$, with error also growing in $O(n)$.
% linearly in size of the number of \textit{nodes}, which is undesirable.
