\section{Technical Overview} \label{sec:overview}
In this paper, we are primarily motivated to extend Numerical Fuzz to support
more floating-point operations and richer ways of structuring and sequencing
arithmetic. We provide some background and provide our motivating running
example below. Then, we overview our extensions.

% In this section, we briefly introduce floating-point round-off error and
% outline the main technical features of our system. 
\subsection{Background: floating-point and error metrics}
The binary floating-point number system is a family of formats with the
following setup:
\begin{equation}
  x = (-1)^s \cdot m \cdot 2^{e - p + 1}
\end{equation}
where $s$ controls the sign, $m$ is the mantissa, $e$ is the exponent, and $p$
is the precision. A specific floating-point number system fixes a particular
precision $p$ and represents a subset of the reals, for example
$\textsc{binary32}$ and $\textsc{binary64}$ which fix $p = 32$ and $p = 64$
respectively.
Operations over the reals can be represented in a floating-point system as the
exact operation followed by rounding by a function $\rho$ to a float. The error
introduced by the rounding function $\rho$ is known as \textit{round-off error}.

For this paper, we assume the absence of overflow and underflow. Then, the
standard rounding error model assumes $\rho$ will observe round-off error in at
most the last bit:
\begin{equation}
  \rho(x) = x (1 + \delta) \quad \quad |\delta| \leq u
\end{equation}
where $u$ is the \textit{unit round-off} value representing the maximum error
possible in the last bit for the floating point format (e.g. $2^{-24}$ and
$2^{-53}$ for 32 and 64-bit floating-point numbers respectively).

We are interested in bounding round-off error. 
To bound error, one must first have a suitable definition for measuring error.
Following Numerical Fuzz, we rely on Olver's \cite{olver} \textit{relative
precision} metric for measuring round-off error. 

\begin{equation}
  d_{\mathbb{R}}(r, \tilde{r}) = | ln(\frac{r}{\tilde{r}})|
\end{equation}

Our approach relies on triangle inequality for the soundness of the error grades
for our charateristic monadic bind rule.
As relative precision forms a true metric and satisfies triangle equality, we
are able to relative precision to compositionally reason about repeated
applications of the monadic bind rule.
Importantly, triangle inequality does not hold for \textit{relative error}.

\subsection{Running example: pairwise summation}
We consider the well-known pairwise summation (sometimes referred to as cascade
summation) algorithm. 
To sum up a list of numbers, pairwise summation will recursively divide the list
in half and compute the sum of the halves.
In other words, pairwise summation takes a flattened list of numbers to add and
associates them into a balanced binary tree of height 
$\left \lceil{log_2(n)}\right \rceil$. Error grows in $O(log(n))$.
By contrast, naive iterative summation takes a flattened list of numbers and
left-folds addition over the list. Error grows in $O(n)$.

As a concrete running example, we consider adding up the floating-point numbers
$w, x, y, z \in [-1, 1]$ via pairwise summation: 

\begin{equation}
  \rho(\rho((w + x)) + \rho((y + z)))
\end{equation}

Numerical Fuzz cannot handle negative numbers, which leads us to our first new
major language feature:


\subsection{Language Feature: Supporting Subtraction and Negative Numbers}
% TODO: mention that this is forwards error above maybe? in contrast with bean?
% maybe explain fowards vs backwards
We wish to support a forwards error analysis on programs containing subtraction
or negative numbers. 
Unforutantely, Numerical Fuzz cannot assign a type to infinitely sensitive
functions and subtraction over the reals is infinitely sensitive. 
Stated in terms of numerical analysis, subtraction between two possibly close
numbers is ill-conditioned. 
The core problem is that relatively small peturbations in the
input of subtraction can lead to hard-to-bound relatively large perturbations in
the output. As an illustrative toy example, suppose we have two large numbers
like 1,000,001 and 1,000,000 that we wish to subtract. If we perturb the input
by a small amount, for example, by 1 per-cent, we can observe quite large
\textit{relative} changes in the output. 
In other words, subtracting two large nearby numbers could obtain a small number
with arbitrarily high relative error. This feature of subtraction is sometimes
referred to as \textit{catestrophic cancelation} and forms one of the primary
technical obstacles to verifying our running example. 

% \begin{enumerate}
%   \item A way to ``simulate" round-off error on $r$ as error on the pair $(a,
%     b)$. We do this by taking the rounding function $\rho(r)$ and defining a
%     corresponding rounding function $\rho_{\mathbb{P}}(r, a, b) = (\rho(r),
%     \tilde{a}, \tilde{b})$ such that $\rho(r) = \tilde{a} - \tilde{b}$.
%   \item A way to measure error on the paired components. We define distance
%     between pairs $(r_0, a_0, b_0)$ and $(r_1, a_1, b_1)$ as the maximum of the
%     pairwise distance: $max(d(a_0, a_1), d(b_0, b_1))$. 
%   \item A way to ``translate" the distances between pairs $(r, a, b)$ and
%     $(\tilde{r}, \tilde{a}, \tilde{b})$ to be a suitable bound on the
%     floating-point error on $r$. 
%     We do this by integrating an interval-style bounds analysis into the type
%     system. 
% \end{enumerate}

Considering addition with negative numbers and subtraction are effective the
same problem, so we first try to support the paper's running example:
% TODO: make program formatting consistent
\begin{equation}
\begin{aligned}[c]
\textbf{let-bind} \ q \ = \ \textbf{rnd} \ (\textbf{add} \ \langle w, x \rangle) \ \textbf{in} \  \\
\textbf{let-bind} \ p \ = \ \textbf{rnd} \ (\textbf{add} \ \langle y, z \rangle) \ \textbf{in} \  \\
\textbf{rnd} \ (\textbf{add} \  \langle q, p \rangle  )
\end{aligned}
\end{equation}
under the restriction that $x \in [-1, 1]$ and $y \in [-1, 1]$. 
% TODO: more intuition
We will show how the program cannot be typed in Numerical Fuzz.
We will first assign a type in Negative Fuzz to a simplified toy example. Then,
we will demonstrate how to obtain a useful error bound from the program type
from the example.

\paragraph{Standard representation.}
Our type system relies on bounding function senstivity. 
We first attempt to assign a type to a hypothetical addition primitive in
Numerical Fuzz over the reals:
$\textbf{add} : !_{\infty} (\textbf{num} \times \textbf{num}) \multimap \textbf{num}$.
Then, the type of our running example is 
$M_{\infty}~\textbf{num}$
in the context 
$w : \textbf{num}, x : \textbf{num}, y : \textbf{num}, z : \textbf{num}$.
\footnote{For presentational purposes, we elide the sensitivity co-effect
annotation in the typing context. The program is infinitely-sensitive in
variables $w,x,y,z$. }
Our type comes from two facts.
Firstly, due to $\textbf{rnd}$, we know that there can be up to $u$ error
introduced in the inputs $p$ and $q$ to the last $add$ call.
Secondly, since the $add$ function may amplify error in its input by up to
$\infty$, the round-off error of the whole program is $\infty \cdot u = \infty$
where $u$ is our unit-roundoff constant.
Therefore, we know that our example program is both infinitely-sensitive in its
free variables and has infintely-bounded error.
This is not a useful error bound.

\paragraph{Paired representation.}
As addition and subtraction over the unrestricted reals is infinitely-sensitive,
we first deal with the problem of infinite sensitivity.
We develop a paired representation where unrestricted addition and subtraction
have finitely-bounded sensitivty. The trick is to associate for each
real $r \in \mathbb{R}$ a triple $(r, a, b) \in \mathbb{P} = \mathbb{R} \times
\mathbb{R}^+ \times \mathbb{R}^+ = \mathit{num}$ such that $r = a - b$.
The paired components $a$ and $b$ are only used for error analysis and serve a
function similar to ghost variables in other program analyses.
Our error function $d_{\mathbb{P}}((r, a, b)(\tilde{r}, \tilde{a}, \tilde{b}))$
over $\mathbb{P}$ is defined below:
\begin{definition}[Distance over pairs] \label{def:distance-p}
$d_{\mathbb{P}}((r, a, b)(\tilde{r}, \tilde{a}, \tilde{b})) = max(d_{\mathbb{R}}(a, \tilde{a}), d_{\mathbb{R}}(b, \tilde{b}))$.
\end{definition}
Importantly, we can encode our operations, including addition and subtraction,
such that both components of the pair are always-growing. This avoids the
problem of catestrophic cancellation in the paired representation.
When our semantics adds $r_0 = (a_0, b_0)$ and $r_1 = (a_1, b_1)$, we produce
$r_0 + r_1 = (a_0 + a_1, b_0 + b_1)$. When we subtract two numbers $r_0 - r_1 =
(a_0 + b_1, b_0 + a_1)$, our paired type represenation only needs to add
components; subtraction is represented implictly. 
By ensuring that each operation in the paired representation is always-growing,
we can encode addition and subtraction in a finitely-sensitive manner. 

We can ``simulate" error on the paired representation by defining a
paired version of our rounding function $\rho$:
\begin{definition}[Rounding over pairs] \label{def:rounding-p}
$\rho_{\mathbb{P}}((r, a, b)) = 
\begin{cases}
  (0, a, b)  & r = 0\\
(\rho_{\mathbb{R}}(r), a \cdot \frac{\rho_{\mathbb{R}}(r)}{r}  b \cdot
  \frac{\rho_{\mathbb{R}}(r)}{r}), & r \not=0 \\
\end{cases}
$
\end{definition}
which satisfies the constraint that $r = a - b$ at all times.
This constraint is useful for letting us relate error over the unpaired
components to error on the paired component.

Under the paired representation, we can implement and faitfully type addition,
subtraction, and multiplication in terms of finitely-sensitive operations over
the paired components:
\begin{equation}
  \begin{aligned}[c]
    add((r, a, b), (r', a', b')) &\mapsto (r + r', a + a', b + b') \\
    sub((r, a, b), (r', a', b')) &\mapsto (r - r', a + b', a' + b) \\
    mul((r, a, b), (r', a', b')) 
      &\mapsto 
    (r * r', a * a' + b * b', a * b' + a' * b)
  \end{aligned}
\end{equation}
In this manner, we can bound the sensitivity of each operation and soundly
assign a type. 
$\textbf{add}$ and $\textbf{mul}$ 
have the same type as in Numerical Fuzz. Subtraction has type as
addition: $\textbf{sub} : \textbf{num} \times \textbf{num} \multimap
\textbf{num}$. 
Returning to our running example, we can assign the type 
$\textbf{num} \times \textbf{num} \multimap M_u \textbf{num}$ to the program 
$\lambda x . \ \textbf{let} \ z \ = \ \textbf{sub} \ x \ \textbf{in} \ \textbf{rnd} \ z$
where $u$ is the unit round-off constant.

\paragraph{Bound polymorphism.}
% TODO: how exactly do we type add?
Now that we have a way to bound error on the paired components, we need to
translate these bounds into something useful over the unpaired component.
% Unfortuantely, the error bounds obtained through the paired representation only
% bound the maximum error on the \textit{paired} components. 
% To be useful, we wish to relate error on the paired components $a$, $b$ with the
% error on the \textit{unpaired} component $r$.
We can obtain bounds on $r$ by recalling the invariant that for a triple $(r, a,
b)$ that $r = a - b$.
If we have bounds on $a$ and $b$, we can take advantage of our invariant.
Simplifying for presentational purposes, we perform an interval-style analysis.
In our running example, we have that the paired represetnation of 
$x, y \in [-1, 1] \times [0, 1] \times [0, 1]$.

We'd like to have the bounds analysis in type inference algorithm to infer the
type 
$M_{3 \cdot u} \ \textbf{num}_{\bnd{((-4, 4), (0, 4), (0, 4))}}$ 
to the program, where the subscript indicates that the result of the program
will be inside of the set 
$[-4, 4] \times [0, 4] \times [0, 4]$.
Further, we can deduce from the inferred grade of our monad that for all inputs
$x, y$ to our input program, for an output value $(r, a, b), (\tilde{r},
\tilde{a}, \tilde{b})$, that $max(d(a, \tilde{a}) d(b, \tilde{b})) \leq 3 \cdot u$.
With this, we can apply our main absolute error theorem,
Theorem~\ref{thm:paired-a-priori}, for 64-bit floats and obtain: $r - \tilde{r}
= 1.7763568394002505 \times 10^{-15}$. 
%

% TODO: justin says "go slower here, connect to numfuzz type, space out, break
% up paragraphs. this part is important to help readers understand how our
% system works"
However, there is some difficulty in assigning a single, most precise type to
$\textbf{add}$.
The difficulty arises because $\textbf{add}$ is called multiple times in
different contexts.
If we look at the first two add calls, we would try to assign the type: 
$$\textbf{add} : \textbf{num}_{\bnd{((-1, 1), (0, 1), (0, 1))}} \times
\textbf{num}_{\bnd{((-1, 1), (0, 1), (0, 1))}} \multimap
\textbf{num}_{\bnd{((-2, 2), (0, 2), (0, 2))}}$$
However, this would make it impossible to type the second add call.
On the other hand, if we look at the last add call, we would try to assign the
type: 
$$\textbf{add} : \textbf{num}_{\bnd{((-2, 2), (0, 2), (0, 2))}} \times
\textbf{num}_{\bnd{((-2, 2), (0, 2), (0, 2))}} \multimap
\textbf{num}_{\bnd{((-4, 4), (0, 4), (0, 4))}}$$
However, this would make our analysis imprecise in our first two calls to
$\textbf{add}$.

To ensure that our bounds analysis is scalable and that functions types can be
reused, we support \textit{bound polymorphism}, which allows functions to be
specialized with different concrete bounds $\bnd{b}$ at each call site. We write
types $\tau$ polymorphic in bound variable $\bnd{\epsilon}$ as $\bnd{\forall
\epsilon.} \tau$.
By adding \textit{bound polymorphism}, we allow functions to be
typed like so: $id : \bnd{\forall \epsilon.} ~ \textbf{num}_{\bnd{\epsilon}}
\multimap \textbf{num}_{\bnd{\epsilon}}$. We can introduce and eliminate
polymorphism via $\forall$ introduction and elimination rules.
% provided in Figure~\ref{fig:typing_rules}.
We also introduce \textit{bound operations}, which are meta-level operations
that let us type:
$$
\textbf{add} : \bnd{\forall \epsilon_1, \epsilon_2}, \mathbf{num}_{\bnd{\epsilon_1}} \times
\mathbf{num}_{\bnd{\epsilon_2}} \multimap \mathbf{num}_{\bnd{\epsilon_1 + \epsilon_2}}
$$
To reduce annotation overhead, we provide a type inference algorithm that can
automatically infer bound polymorphism abstraction and instantiation sites. We
prove our type inference algorithm sound in Section~\ref{sec:inference}. 

% TODO: awk phrasing I think, can be tighter. forwards reference to section
% where we do this once this is written
One might wonder if using a paired representation, then running a bounds
analysis to recover the error on the original unpaired representation hurts the
precision of our analysis on programs. Interestingly, we do not lose any
precision. We provide several examples showing that our bounds analysis is
capable of reasoning that programs containing only-growing functions on
non-negative numbers can only produce non-negative numbers. Therefore, on the
programs supported by both Negative Fuzz and Numerical Fuzz, the maximum
round-off error obtained by Negative Fuzz is simply the inferred grade of the
monad and is no looser than the bound obtained by Numerical Fuzz. Negative Fuzz
is therefore a strict improvement on Numerical Fuzz: our approach can analyze
more programs with possibly tighter bounds on previously supported programs.

\subsection{Language Feature: Precise Structuring and Sequencing of Arithmetic} 
\label{sec:factor}
Returning to our running example, it is well known that round-off error grows
linearly in the height of a summation tree. 
We'd expect Numerical Fuzz to infer that the error on our paired summation
example grows in $log_2(n)$.
In prior type-based work \cite{numfuzz}, the monadic computation sequencing rule
forces the error analysis to always compute the worst-case round-off error
associated with the pathological degenerate tree:
$\rho(\rho(\rho(w~+~x)~+~y)~+~z)$.
In other words, the error bound obtained by \cite{numfuzz} grows linearly in
size of the number of \textit{nodes}, which is undesirable.

Our extension enables the user to associate the computation tree arbitrarily,
and to obtain tighter error bounds that grow linearly in size of the \textit{height} of
the tree.
We accomplish this by including a special primitive $\textbf{factor}$ in our
term language inspired by linear logic. Stucturing the example arithmetic into a
program below, we can see that using factor results in a 50\% smaller error
bound for a program representing the same association and order of operations.

\begin{figure}[ht] \label{fig:factor-cmp-overview}
\centering

\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
// (w + x) + (y + z) : M[3*u] num 
let-bind a = rnd (add <w, x>) in
let-bind b = rnd (add <y, z>) in
rnd (addfp <a, b>)
\end{lstlisting}
\caption{Without factor: error bound is $3u$.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
// (w + x) + (y + z) : M[2*u] num 
let-bind a =
  factor 
    <rnd (add <w, x>), rnd (add <y, z>)> 
  in rnd (add a)
\end{lstlisting}
\caption{With factor: error bound is $2u$.}
\end{subfigure}

\caption{Side-by-side comparison with and without the \textbf{factor} primitive.}
\label{fig:factor-side-by-side}
\end{figure}

More generally, for perfect binary trees of addition, we have a $log_2 (n)$
factor improvement on our error bounds when compared to Numerical Fuzz. We defer
a detailed explaination of \textbf{factor} and how it allows for structuring and
sequencing arithmetic towards a more precise error analysis in
Section~\ref{sec:structure}. We also showcase the improved precision offered by
programs annotated with the \textbf{factor} primitive in Section~\ref{sec:eval}.
